# Networking

Although a large part of this lab focuses on systemd socket activation, we will also cover custom Podman networks and inter-container connectivity. The lab builds upon the foundational knowledge from previous labs, particularly the PostgreSQL setup from the Volumes Lab.

In this lab you will:

1. Retrofit the test-postgresql quadlet with systemd socket activation
2. Create an unprivileged pgadmin service and demonstrate cross-over dependency triggering
3. Create network quadlets to connect multiple containers and compare connectivity mechanisms

## 0. Snapshot

Before starting this lab, create a VM snapshot to allow easy rollback in case of issues.

```bash
vagrant snapshot save networking-start
```

## 1. Retrofit PostgreSQL with Socket Activation

The previous Volumes Lab had you create a `test-postgresql` quadlet that started a PostgreSQL container with a published port using the `PublishPort=5432:5432` directive. The directive uses Podman's built-in port forwarding to expose the container's PostgreSQL service on the host's port 5432. It works great, however another approach provides for systemd socket activation enabling cross-over dependency triggering and better performance.

>**SECURITY NOTE**: By default, `PublishPort=5432:5432` binds to **all interfaces** (0.0.0.0:5432), making the service accessible from any network interface on the host. For security, you can bind to specific interfaces:
>- `PublishPort=127.0.0.1:5432:5432` - localhost only (most secure)  
>- `PublishPort=192.168.1.100:5432:5432` - specific IP only
>- `PublishPort=eth0:5432:5432` - specific interface only
>
>This interface binding control that `PublishPort=` has is also available in systemd socket activation, where the socket unit determines the listening interface.

In this lab section, you will modify that quadlet to use systemd socket activation with `systemd-socket-proxyd` instead of Podman's proxy infrastructure with its `PublishPort` directive. This change improves resource efficiency and triggers PostgreSQL container startup only with incoming connections. This involves creating a `.socket` unit that listens on the desired port and a corresponding `.service` unit that starts the container when a connection is made.

### 1.1 Motivation

Now that's tightly packed so let's break it down. In systemd, user-scoped services running under unprivileged user accounts cannot use dependency directives like `After=` or `Requires=` to depend on user-scoped services running under other unprivileged user accounts. By "cross-over" we mean across systemd scope boundaries, essentially across different unprivileged user accounts. The impact of these limitations is that other containers depending on the `test-postgresql` quadlet cannot express a dependency on it when running under a different unprivileged user account.

>**NOTE**: User-scoped services can however depend on system-wide "like" (user scoped pseudo) targets like `networking-online.target` or `default.target` in `WantedBy=` directives. 

User scope cross-over dependencies between unprivileged users are not supported, but we can work around this limitation using systemd socket activation while also benefiting from on-demand activation. This is a powerful primitive that allows services to start on-demand when a connection is made to a listening socket. By using socket activation, other containers can depend on an active socket instead of a service unit, allowing for cross-over dependencies to actually work.

### 1.2 Background

Systemd socket activation allows services to start on-demand, reducing resource usage when the service is idle. A socket unit listens for incoming connections and activates the service unit when a connection is received, the `accept()` call. The mechanism is particularly useful for containers, as it allows them to remain stopped until  they're needed.

It works by having the socket unit listen on a specified port (e.g., 5432 for PostgreSQL). When a client connects, systemd starts the associated service unit, which in turn starts the container then passesPerfect! I'll add a clarification about PublishPort interface binding options right after the first mention. Let me add this important security information:

4. networking.md+7-0 the file descriptors of the socket through the service to the container so it can handle the incoming connection. The socket hand off mechanism is efficient with near native performance. However the socket handoff requires support for systemd's socket passing mechanism. Although many services support socket activation natively, many do not. For those that do not, like PostgreSQL, we can use `systemd-socket-proxyd` as a proxy to forward connections from the host to the container's internal network namespace.

Handing off the listening socket to the container is achieved using `systemd-socket-proxyd`, as intermediary, which forwards connections from the host to the container's internal network namespace. The extra proxying effort adds minimal latency while enabling socket activation for services that do not natively support it.

#### Socket Descriptor Passing Support

Podman added socket file descriptor passing support, see <https://podman.io/blogs/2022/09/30/podman-4.0.html#socket-activation>, allowing containers to receive incoming connections directly through the host's socket file descriptors. There's a long chain of forking involved to get the socket file descriptors to the container's service which must support systemd's socket descriptor passing. This support just added it to Podman and OCI containers which separately added support too. It does not include the network service wrapped in the container. There are two problems with this approach:

1. Some key services do not natively support systemd socket descriptor passing, including PostgreSQL.
2. Container image customizations are required to handle the socket passing mechanism: just see what it takes to make an echo service in an `echo.container` work with socket passing in the [Podman documentation](https://podman.io/blogs/2022/09/30/podman-4.0.html#socket-activation).

This feels way too involved for most use cases. Instead, Brothaman uses `systemd-socket-proxyd` as a workaround for all network services, especially ones that **DO NOT** natively support socket descriptor passing. The proxy runs inside the container's network namespace and forwards connections from the host's service socket to the container's service port bound usually to the loopback. The configuration is straightforward, more secure, and works with any container image without modification.

Once set up, the socket activation mechanism works as follows:

1. The socket unit is started and listens on the host's port (e.g., 0.0.0.0:5432 for PostgreSQL)
2. When a client connects (accept()), systemd activates the service unit passing it the socket file descriptors
3. The service unit starts the container if it's not already running
4. The service unit starts `systemd-socket-proxyd` inside the container's network namespace using `bro-helper`, forwarding connection traffic to the container's internal service port (e.g., 127.0.0.1:5432 for PostgreSQL)
5. The container's network service (in this case PostgreSQL) handles the incoming connection

Socket passing concerns are all handled by the `systemd-socket-proxyd` proxy, allowing the container's service to operate normally without any special socket descriptor handling considerations. There is a minimal performance cost for the proxy as with the PublishPort feature of Podman, but we're more than willing to bear it for the on-demand activation, simplicity of use, and compatibility with any container image that it provides.

Brothaman's `bro-activate` command creates activator socket and service pairs as add-on infrastructure to enable any container quadlet to use socket activation with minimal effort. So say for a test-postgresql.container quadlet, it can create a `test-postgresql-activator.socket` and `test-postgresql-activator.service` pair that listens on port 5432 and starts the `test-postgresql.container` quadlet when a connection is made.

Furthermore, it is fully compatible with cross-over dependencies between unprivileged user accounts regardless of the container and whether or not its service supports socket descriptor passing or not. For example, I can have two separate `bro-user` created `gitea` and `postgres` unprivileged user accounts, with a gitea container running gitea yet hitting the host port of the postgresql service running under the postgres unprivileged user account. Here the postgresql server be triggered to start on-demand via socket activation and both the gitea container and postgresql server can run under separate unprivileged user accounts. Heck I can even socket activate the gitea container's web port so it too fires up on-demand when a web request comes in!

>**CONTRIBUTE**: Anyone interested in adding a `bro-activate-direct` command to use Podman's native socket descriptor passing support directly without the `systemd-socket-proxyd` service proxying traffic is welcome to contribute it! A good benefit of this is that it avoids having an extra service and an extra hop which proxies traffic. Without this direct option we chose convenience over performance.

#### TCP/UDP Sockets vs. Unix Domain Sockets

Systemd socket activation can use TCP or UDP sockets as well as Unix Domain Sockets (UDS). Both activation mechanisms listen for connections: one works on remotely accessible network ports, whereas Unix Domain Sockets (UDS) are file-based pipes used for inter-process communication on the same host. UDS can also be used for cross-over dependencies between different user accounts, but those accounts must be on the same host. Also direct UDS file access with the right permissions is required: very doable using a common group.

TCP/UDP sockets, on the other hand, used in systemd socket activation listen on network ports which can be accessed both locally and remotely to activate services on-demand. No file access permissions are need for it to work. This is particularly useful for other remote services to depend on TCP activated services over the network. Unlike UDS, TCP/UDP sockets do not require direct file access permissions. Remote access makes them more flexible for cross-over dependencies yet keep in mind this occurs at the price of greater risk since services are now exposed over the network. Proper firewalling and security measures should be in place to protect these services from unauthorized access.

#### Hint: Proxying from any interface to any other interface

`systemd-socket-proxyd` can proxy from any host specific interface (not just `0.0.0.0`) to any container interface, not just its loopback, `127.0.0.1`. This includes Podman created networks. So, it could proxy from a container to a dedicated inter-container network bridge. It connects containers to networks in general, while also activating those same containers on-demand. The same mechanism enables connecting containers to inter-container network quadlets (network definitions), not just a particular host interfaces.

### 1.3 Create the PostgreSQL Activator Service Unit

Before creating the socket unit, we need to create the corresponding service unit that will be activated when a connection is made to the socket.

When this service unit is activated it activates the `test-postgresql.service` quadlet if it is not already running. Once the container is running, the service unit retrieves the container's PID using `podman inspect` and then starts `systemd-socket-proxyd` inside the container's network namespace using `bro-helper`.

In a `lingeruser` shell, a new file named `postgresql-activator.service` is created in the systemd user directory with the following content:

```bash
systemctl --user disable test-postgresql-activator.service
mkdir -p ~/.config/systemd/user/
cat <<EOF > ~/.config/systemd/user/test-postgresql-activator.service
[Unit]
Description=Test PostgreSQL Activator Service
Requires=test-postgresql-activator.socket test-postgresql.service
After=test-postgresql-activator.socket test-postgresql.service

[Service]
Type=simple

# Create the environment file for the proxy helper with the target container PID
ExecStartPre=/usr/bin/env sh -c "/usr/bin/podman inspect -f 'TARGET_PID={{.State.Pid}}' test-postgresql > %t/test-postgresql-proxyd.env"

# Environment variable file for the proxy helper does not have to exist with the dash
EnvironmentFile=-%t/test-postgresql-proxyd.env

# Wait until Postgres is accepting connections inside its netns
ExecStartPre=/usr/bin/env sh -c "i=0; while [ \$i -lt 20 ]; do \
  /usr/bin/podman unshare nsenter -t \"\$TARGET_PID\" -n \
    pg_isready -h 127.0.0.1 -p 5432 && exit 0; \
  i=\$((i+1)); sleep 1; \
done; exit 1"

# Key is the use of exec to replace the shell with podman process
# Use podman unshare to enter user namespace, then bro-helper for network namespace
ExecStart=/usr/bin/env sh -c 'exec podman unshare /usr/local/bin/bro-helper --pid "\${TARGET_PID}" -- /lib/systemd/systemd-socket-proxyd 127.0.0.1:5432'

NoNewPrivileges=no
PrivateTmp=yes
ProtectHome=yes
ProtectSystem=full

Restart=on-failure
RestartSec=5s
TimeoutStopSec=120

[Install]
WantedBy=default.target
EOF

systemctl --user daemon-reload
systemctl --user enable test-postgresql-activator.service
```

Let's break down each of the significant directives in this service unit:

- `Requires=` and `After=`: Ensures that the socket unit and the PostgreSQL container service are started before this service can be activated.
- `ExecStartPre=`: Several pre-start commands are used to gather the target container's PID and verify that PostgreSQL is ready to accept connections which makes the system much more robust.
- `ExecStart=`: The main command starts `systemd-socket-proxyd` inside the container's network namespace using `bro-helper`.
- `NoNewPrivileges`, `PrivateTmp`, `ProtectHome`, and `ProtectSystem`: Security directives to limit the service's privileges and access.
- `Restart=` and `RestartSec=`: Configures the service to restart on failure.

### 1.4 Create the Socket Unit

The `postgresql-activator.service` above is activated by the `postgresql-activator.socket` unit. This socket unit listens on port 5432 and triggers the service unit when a connection is made. In a `lingeruser` shell create a new file named `postgresql-activator.socket` in the systemd user directory with the following content:

```bash
systemctl --user stop test-postgresql.service
mkdir -p ~/.config/systemd/user/
cat <<EOF > ~/.config/systemd/user/test-postgresql-activator.socket
[Unit]
Description=PostgreSQL Activator Socket

[Socket]
ListenStream=5432
NoDelay=true
ReusePort=true
Backlog=128

[Install]
WantedBy=default.target
EOF

systemctl --user daemon-reload
systemctl --user enable test-postgresql-activator.socket
systemctl --user restart test-postgresql-activator.socket
```

The socket is now set up to listen on port 5432. When a connection is made, it will trigger the associated service unit with the same name. The socket unit configuration includes directives:

- `ListenStream=5432`: Listens on TCP port 5432 for PostgreSQL (all interfaces)
- `NoDelay=true`: Disables Nagle's algorithm for lower latency
- `ReusePort=true`: Allows multiple sockets to bind to the same port
- `Backlog=128`: Sets the maximum number of pending connections

>**INTERFACE BINDING**: Like `PublishPort=`, systemd socket units bind to all interfaces by default. For security, you can specify:
>- `ListenStream=127.0.0.1:5432` - localhost only
>- `ListenStream=192.168.1.100:5432` - specific IP
>- `ListenStream=[::1]:5432` - IPv6 localhost
>
>Multiple `ListenStream=` directives can bind to different interfaces simultaneously.

In the beginning before anything occurs, the only enabled and started unit is the socket unit (postgresql-activator.socket).

So what happens when a connection is made to the socket: i.e. with the psql client?

```bash
export PGPASSWORD=password
psql -h localhost -U postgres
```

When a connection is made to port 5432, i.e. with psql, the socket unit activates its (same name) service unit: `postgresql-activator.service`. Activating `postgresql-activator.service` in turn activates `test-postgresql.service`. Once the container is up, as detected by looping checks in an `ExecStartPre=` directive's shell command, the container's PID is retrieved using `podman inspect`. Finally, `systemd-socket-proxyd` is started inside the PostgreSQL container's network namespace using `bro-helper`. This setup allows incoming connections on port 5432 to be proxied directly (in one hop) to the PostgreSQL service running inside the container.

It's essentially a substitute for Podman's built-in port forwarding mechanism, but with the added benefits of on-demand socket activation which allows us to implement cross-over dependency triggering.

### 1.5 Update the test-postgresql quadlet

Now we do not need Podman's built-in port forwarding anymore since systemd socket activation is handling incoming connections. Below we remove the `PublishPort=` directive from the `test-postgresql.container` quadlet. Also note we're not starting the service as usual after updating the quadlet and reloading. It will now be started by the activator service and its socket. Update the `test-postgresql.container` quadlet as follows: 

```bash
if systemctl --user is-active test-postgresql.service; then
  systemctl --user stop test-postgresql.service
fi
mkdir -p ~/.config/containers/systemd/
cat > ~/.config/containers/systemd/test-postgresql.container <<'EOF'
[Unit]
Description=PostgreSQL Container
PropagatesStopTo=postgres-data-volume.service
BindsTo=postgres-data-volume.service
After=postgres-data-volume.service

[Container]
Image=postgres:16
ContainerName=test-postgresql
Environment=POSTGRES_PASSWORD=password
Volume=/home/lingeruser/postgres:/var/lib/postgresql/data
Network=none

[Service]
Restart=always
TimeoutStartSec=300
StartLimitBurst=5
RestartSec=10s

[Install]
WantedBy=default.target
EOF

systemctl --user daemon-reload
```

### 1.4 Validate Socket Activation

Now that everything is set up, we can validate that the socket activation is working as expected. Remember we stopped the `test-postgresql.service` above, so it should not be running yet. Check the status of the socket and service units:

```bash
systemctl --user is-active test-postgresql.service
systemctl --user is-active test-postgresql-activator.socket
systemctl --user is-active test-postgresql-activator.service
```

Basically the `test-postgresql.service` should be **in**active, the `test-postgresql-activator.socket` should be active, and the `test-postgresql-activator.service` should be **in**active. Now try connecting to PostgreSQL using the `psql` client:

```bash
export PGPASSWORD=password
psql -h localhost -U postgres
# Try running a simple SQL command, e.g.: SELECT version();
```

The first connection attempt may take a few seconds as the container starts up and is fully available online. Once connected, you should be able to run SQL commands as usual. After disconnecting, check the status of the units again:

```bash
systemctl --user is-active test-postgresql.service
systemctl --user is-active test-postgresql-activator.socket
systemctl --user is-active test-postgresql-activator.service
```

All three units should now be active, indicating that the PostgreSQL container is running and ready to accept connections.

## 2.0 Define a network quadlet

Again as the `lingeruser` run the following commands to create a network quadlet named `test.network` that defines a Podman network:

```bash
mkdir -p ~/.config/containers/systemd/
cat > ~/.config/containers/systemd/test.network <<'EOF'
[Unit]
Description=Test Network Quadlet
After=network-online.target

[Network]
Subnet=192.168.100.0/24
Gateway=192.168.100.1
DNS=192.168.100.1

[Install]
WantedBy=default.target
EOF
systemctl --user daemon-reload
systemctl --user start test-network.service
systemctl --user status test-network.service -l --no-pager
podman network list
podman network inspect systemd-test
if ping -W 0.5 -c 1 192.168.100.1 > /dev/null; then
  echo "Ping to gateway 192.168.100.1 successful"
else
  echo "Ping to gateway 192.168.100.1 failed"
fi
```

The quadlet creates a private bridge network with the specified subnet, gateway, and DNS settings in its own network namespace owned by the unprivileged user. Commands like the `ping` above and `brctl show` on the host's network namespace will **NOT** display the bridge. You can see for yourself by installing `bridge-utils` as the `vagrant` user if needed and try for yourself:

```bash
sudo apt-get install -y bridge-utils
sudo brctl show
```

We can use the following command to switch to the unprivileged user's network namespace and see the bridge interface created by Podman for this network:

```bash
podman unshare --rootless-netns /usr/sbin/brctl show
```

The rootless-netns option allows us to enter the unprivileged user's network namespace where the Podman networks are created.

But wait, you see nothing right? Well this was to show you that Podman does not activate the bridge until a container is connected to it. This could produce some gotchyas situations so we wanted to point this out. Try again after connecting a container to the network in the next section.

## 3.0 Connect containers to the network

This is great but we need to connect the container onto this network. Modify the `test-postgresql.container` quadlet adding `Network=test.network` to connect it to the `test` network as follows:

```bash
if systemctl --user is-active test-postgresql.service; then
  systemctl --user stop test-postgresql.service
fi
mkdir -p ~/.config/containers/systemd/
cat > ~/.config/containers/systemd/test-postgresql.container <<'EOF'
[Unit]
Description=PostgreSQL Container
PropagatesStopTo=postgres-data-volume.service
BindsTo=postgres-data-volume.service
After=postgres-data-volume.service

[Container]
Image=postgres:16
ContainerName=test-postgresql
Environment=POSTGRES_PASSWORD=password
Volume=/home/lingeruser/postgres:/var/lib/postgresql/data
Network=test.network

[Service]
Restart=always
TimeoutStartSec=300
StartLimitBurst=5
RestartSec=10s

[Install]
WantedBy=default.target
EOF
systemctl --user daemon-reload
systemctl --user start test-postgresql.service
systemctl --user status test-postgresql.service -l --no-pager
```

### 3.1 Verify bridge activation

Let's try that bridge lookup and ping the gateway again to verify the bridge is now active in the unprivileged user's (rootless) network namespace:

```bash
podman unshare --rootless-netns ip addr show
podman unshare --rootless-netns /usr/sbin/brctl show
# Uses the ping program inside the host but executes in the rootless netns
if podman unshare --rootless-netns ping -W 0.5 -c 1 192.168.100.1 > /dev/null; then
  echo "Ping to gateway 192.168.100.1 successful"
else
  echo "Ping to gateway 192.168.100.1 failed"
fi
```

You'll see the `podman1` bridge interface and the ping to the gateway is successful from within the rootless network namespace. The `test-postgresql` container has its own namespace but the veth device connects them together. The veth device type is a virtual Ethernet pair that connects two network namespaces. One end of the veth pair is placed inside the container's network namespace, while the other end is attached to the bridge in the rootless network namespace. This setup allows network traffic to flow between the container and the bridge, enabling communication with other containers connected to the same bridge network as well as external networks through NAT. Let's demonstrate this clearly in the next section.

### 3.2 Verify network namespaces and their isolated views

Let's print out the namespace inodes for the host, container, and rootless network namespace to see just how they are all different:

```bash
# Container has its own network namespace (anonymous, not named)
# Compare namespace inodes to verify isolation
echo "Host netns inode: $(stat -c %i /proc/self/ns/net)"
echo "Container netns inode: $(podman exec test-postgresql stat -c %i /proc/self/ns/net)"
echo "Rootless netns inode: $(podman unshare --rootless-netns stat -c %i /proc/self/ns/net)"
```

Notice that all network namespace inodes are different. Now let's look at the isolation this achieved. First let's look at the host's network namespace view of interfaces and bridges:

```bash
ip link show
/usr/sbin/brctl show
```

You should see the `lo` loopback and your main network interface (e.g., `eth0` or `ens3`), but you will **NOT** see the Podman created bridge or veth endpoints. This is because the Podman networks are created in the unprivileged user's rootless network namespace, not the host's network namespace.

Now let's look at the rootless (unprivileged user's) network namespace view of interfaces and bridges:

```bash
# Rootless namespace can see the bridge and veth endpoints
podman unshare --rootless-netns ip link show
podman unshare --rootless-netns /usr/sbin/brctl show
```

Here we see the `podman1` bridge interface and the veth endpoint connected to it, including the one connected to the `test-postgresql` container. This demonstrates the isolation provided by separate network namespaces while still allowing connectivity through the veth pair that acts as a tunnel through the namespaces.

Now let's look at the `test-postgresql` container's network namespace view of interfaces:

```bash
# Container namespace can see its own veth endpoint only
podman exec test-postgresql bash -c '
  NONINTERACTIVE=1 apt-get update &&
  NONINTERACTIVE=1 apt-get install -y bridge-utils'
podman exec test-postgresql ip link show
podman exec test-postgresql /usr/sbin/brctl show
```

Unlike the rootless network namespace, the container's network namespace cannot see the bridge or other veth endpoints. It can only see its own veth endpoint (usually named `veth0@ifX` where X is an number unless renamed). This demonstrates the isolation (different network device views) provided by separate network namespaces while still allowing connectivity through the veth pair that acts as a tunnel through the namespaces. Pretty cool right? This bridge network can now be used to connect multiple containers together on the same network each in their own namespace.

### 3.3 Verify container connectivity

Let's peak inside the container to see its assigned IP address on the network:

```bash
podman exec -it test-postgresql bash -c '
  ( NONINTERACTIVE=1 apt-get update &&
  NONINTERACTIVE=1 apt-get install -y iproute2 inetutils-ping net-tools ) >/dev/null &&
  ip a s'
IP=$(podman exec -it test-postgresql hostname -i)
trim_string() {
    : "${1#"${1%%[![:space:]]*}"}"
    : "${_%"${_##*[![:space:]]}"}"
    printf '%s\n' "$_"
}
IP=$(trim_string "$IP")
echo "Container IP address is: $IP"
if podman exec -it test-postgresql bash -c 'ping -W 1 -c 1 192.168.100.1 >/dev/null'; then
  printf "Ping from container %s to gateway 192.168.100.1 successful\n" "$IP"
else
  printf "Ping from container %s to gateway 192.168.100.1 failed\n" "$IP"
fi
```

There you go! The container has an IP address assigned from the network bridge. Now we were able to ping the gateway from inside the container. In fact, maybe you notice before, we are also routing to the Internet and resolving DNS queries since apt pulled down our packages. This shows that the container is successfully connected to the custom Podman network defined by the `test.network` quadlet. If you want to disable Internet access you can modify the network quadlet by adding a `Internal=true` directive and restarting the `test-network.service`. The `Internal=true` directive in a Podman network quadlet creates an internal-only network that blocks external internet access while still allowing:

* Container-to-container communication within the same network
* Container-to-host communication (limited through the rootless namespace)

This is useful for isolating containers from the outside world while still allowing them to communicate with each other and the host system.

### 3.4 IPAM Settings

Podman networks use an IPAM (IP Address Management) mechanism, not traditional DHCP. Here's how it works:

* Podman uses a static IPAM allocation system
* IP addresses are assigned sequentially from the subnet range
* No DHCP daemon running - it's handled by Podman's network stack
* IP assignments are persistent and deterministic

How IP Assignment Works:

* Network Creation: Subnet defined (e.g., 192.168.100.0/24)
* Container Connection: Podman assigns next available IP from range
* Static Assignment: IP is statically configured in container's netns
* Persistence: Same container gets same IP on restart (usually)

IPAM Configuration Options:

```ini
# Network quadlet with IPAM control
[Network]
Subnet=192.168.100.0/24
Gateway=192.168.100.1
IPRange=192.168.100.100-192.168.100.200  # Limit assignable range
```

You can also assign specific IPs to existing containers without restarting them:

```bash
# Disconnect from current network first (if already connected)
podman network disconnect systemd-test test-postgresql || true

# Reconnect with a specific IP - hot assignment
podman network connect --ip 192.168.100.157 systemd-test test-postgresql

# Verify the new IP assignment
podman exec test-postgresql hostname -i
```

Let's verify the setting by inspecting the network:

```bash
# Extract just the subnet and gateway
podman network inspect systemd-test | jq -r '.[0].containers[] | select(.name=="test-postgresql") | .interfaces.eth0.subnets[0].ipnet'

# Extract container's IP address using jq
podman inspect test-postgresql | jq -r '.[0].NetworkSettings.Networks."systemd-test".IPAddress'
```

### 3.5 Ping an external IP from inside the container

Let's play with a ping quirk before moving on. Try pinging `8.8.8.8` from inside the container:

```bash
if podman exec -it test-postgresql bash -c 'ping -W 1 -c 1 8.8.8.8 >/dev/null'; then
  echo "Ping from container to external IP 8.8.8.8 successful"
else
  echo "Ping from container to external IP 8.8.8.8 failed"
fi
```

ICMP ping typically doesn't work from inside rootless Podman containers for several reasons related to unprivileged containers and network namespaces. By default, unprivileged containers lack the necessary capabilities to create raw sockets required for ICMP operations. Additionally, network namespaces isolate the container's network stack from the host, preventing direct access to certain network functionalities. These factors combined result in the inability to successfully ping external IPs like `8.8.8.8`. Let's change this by changing the `net.ipv4.ping_group_range` (for UIDs) setting on the host to include the unprivileged user range. As the `vagrant` user run:

```bash
cat /proc/sys/net/ipv4/ping_group_range
sudo sysctl -w net.ipv4.ping_group_range="0 2000000"
cat /proc/sys/net/ipv4/ping_group_range
```

Now `sudo su - lingeruser` and try the ping again:

```bash
if podman exec -it test-postgresql bash -c 'ping -W 1 -c 1 8.8.8.8 >/dev/null'; then
  echo "Ping from container to external IP 8.8.8.8 successful"
else
  echo "Ping from container to external IP 8.8.8.8 failed"
fi
```

Works!

### 3.4 Connecting and disconnecting containers from the network

You can connect and disconnect containers from the network using Podman commands. For example, to disconnect the `test-postgresql` container from the `test` network, run:

```bash
podman exec -it test-postgresql ip a s
podman network disconnect systemd-test test-postgresql
if podman exec -it test-postgresql bash -c 'ping -W 1 -c 1 8.8.8.8 >/dev/null'; then
  echo "Ping from container to external IP 8.8.8.8 successful"
else
  echo "Ping from container to external IP 8.8.8.8 failed"
fi
podman exec -it test-postgresql ip a s
```

As you can see the container can no longer ping external IPs since it's disconnected from the network. The veth interface, the tap from the container into the bridge is gone. To reconnect the container to the network, run:

```bash
podman network connect systemd-test test-postgresql
if podman exec -it test-postgresql bash -c 'ping -W 1 -c 1 8.8.8.8 >/dev/null'; then
  echo "Ping from container to external IP 8.8.8.8 successful"
else
  echo "Ping from container to external IP 8.8.8.8 failed"
fi
podman exec -it test-postgresql ip a s
```

The container can now ping external IPs again since it's reconnected to the network. The veth interface is back, reconnecting the container to the bridge network.

### 3.5 Network quadlet conventions review

Let's break down network quadlets conventions:

The `test.network` quadlet generates a `test-network.service` unit. It uses the convention of appending `-network` to the name before the dot in the quadlet name followed by `.service` for the service unit name.

When listing the network in `podman network list` its network name is displayed as `systemd-test`. It uses the convention of taking the quadlet name before the dot and prefixing it with `systemd-`. If we want to change the listed name we can add a `NetworkName=` directive to the `[Network]` section.

### 3.6 Network quadlets and unprivileged cross-over

Unfortunately, there's no way for containers in separate unprivileged user accounts to share the same Podman network quadlet since each unprivileged user has their own separate rootless network namespace. This limitation is a actually good for security.

Inter-unprivileged container communication across different user accounts would require elevated privileges to bridge the separate network namespaces, which goes against the principle of least privilege and could introduce security vulnerabilities. Each unprivileged user account is isolated from others to prevent unauthorized access and maintain system integrity.

The only option for cross-over communication between containers owned by different users is the use of external port mappings via `PublishPort=` and the socket activation pattern demonstrated in section 1. These approaches allow controlled access, on a port by port basis, to services across user boundaries without compromising the isolation provided by separate network namespaces. This is particularly useful for scenarios where multiple services need to interact with each other while maintaining isolation between user accounts.

Separate unprivileged user account isolation is important for several reasons:

1. **Security**: It helps to prevent unauthorized access to resources and services. By isolating user accounts, we reduce the risk of one user compromising another user's containers or data.

2. **Resource Management**: Isolating user accounts allows for better resource management and allocation. Each user can have their own set of resources without interfering with others.

3. **Stability**: If one user's container crashes or experiences issues, it won't affect the containers or services running under other user accounts.

4. **Compliance**: In multi-tenant environments, isolation is often a requirement for compliance with security policies and regulations.

While it may seem limiting, these isolation measures are crucial for maintaining a secure and stable environment when running containers under unprivileged user accounts.

### 3.7 ANALYSIS: When to use cross-over dependencies vs. network quadlets

So, when should cross-over dependencies between unprivileged user accounts be used vs. using a network quadlet to connect multiple containers within a single unprivileged user account? This is a great question and the answer is not black or white: it depends on specific use cases, the security considerations they impose, and resource constraints. To speak the same language, let's define some terms first.

#### Our own terminology definitions:

>**NOT EXHAUSTIVE**

* **Directly Exposed \[containers|services]**: Containers and/or their services that expose service ports to other containers and to external clients outside of a common security scope, allowing communication and interaction. These containers are typically designed to be accessed by external consumers of the service. Like a web application server in a container exposing port 443 serving an application for the outside world to use.

* **Indirectly Exposed \[containers|services]**: Containers and/or their services that do not expose service ports directly, but instead rely on other containers or services to communicate with the outside world. These containers are typically designed to be accessed only by other containers within the same security scope. For example, a database server in a container that only exposes its port to other containers within the same unprivileged user account.

The mentality here is, everything in a system no matter how much is still exposed to some degree. The question is how much exposure and to whom? Directly exposed containers/services have a larger attack surface since they are accessible from outside the security scope, whereas indirectly exposed containers/services have a smaller attack surface since they are only accessible within the security scope. There's a lot more ways available to an attacker from the outside than from the inside where container to container communication is more controlled and limited.

#### Existing terminology adapted to our domain:

>**NOT EXHAUSTIVE**

* **Cross-over dependency**: A container in one unprivileged user account depending on a service port exposed by a container in another unprivileged user account. This is typically done using Podman's `PublishPort=` feature or systemd socket activation to forward traffic from one container to another across user boundaries.

* **Dependency graph**: In our context, a set of containers that depend on each other to provide a complete application or service. For example, a web application container that depends on a database container and a cache container.

* **Blast radius**: The extent of impact or damage that can occur if a container is compromised. A smaller blast radius means that the compromise is contained within a limited scope, reducing the potential harm to the overall system. Sometimes it helps thinking about this in terms of contagion risk and in terms of concentric circles of impact.

* **Attack surface**: The total number of points (or vectors) where an unauthorized user can try to enter or extract data from a system. A larger attack surface means more potential vulnerabilities.

* **Pivoting**: The technique used by attackers to move from one compromised system or container to another within a network. This often involves exploiting vulnerabilities in interconnected systems to gain further access.

* **Least privilege principle**: A security concept that advocates for granting only the minimum necessary permissions to users or services to perform their tasks, reducing the risk of unauthorized access.

#### Contagion Risk

From a security perspective containers depending on each other (an app to its database and cache server for example) represent a single unique attack surface together. They are inseparable. This is because a request entering a container from the outside can still exploit vulnerabilities to pivot into potentially all its dependent containers. These containers form a dependency graph where threat vectors can flow through the entire graph.

These dependencies and the contagion risks they give rise to are due to one container requiring access to another container's service ports through network communication: i.e. the web application container needs to connect to the database container over the database's service port. They're all exposed, one or more are directly exposed while others are indirectly exposed. The entire dependency graph is exposed to the degree of its unified attack surface.

#### Every container into its own unprivileged user account

Putting each container into its own unprivileged user account and using cross-over port dependency to inter-connect them verses using container wide network connectivity via network quadlets does not reduce the attack surface. It may, in fact, increase complexity, the attack surface, and reduce performance. Let's analyze this further but pragmatically.

No matter how we skin this cat, threat vectors still flow through the dependency graph executing code across dependent containers. Whether the containers are in the same unprivileged user account connected via a network quadlet or partitioned into many separate unprivileged user accounts connected via cross-over dependencies, the threat vectors still flow through the entire dependency graph in the same way.

Now however you have increased complexity with cross-over dependencies. Each container now has to publish its service ports to the outside world (the host) and each dependent container has to connect to those published ports across unprivileged user account boundaries. This increases the attack surface since more ports are published to the outside world and increases complexity since each container has to manage its own published ports and connections. No bueno!

In both PublishPort= and socket activation cross-over dependency patterns, traffic is proxied for each connection across unprivileged user account boundaries. This adds latency and overhead compared to direct container to container communication over a network quadlet within the same unprivileged user account.

So there are many reasons why putting every container into its own unprivileged user account with cross-over dependencies is not ideal. It makes sense to put an application container and containers in its dependency graph together into the same unprivileged user account to limit the attack surface to that single unprivileged user.

#### Every container in one unprivileged user account connected via network quadlet

Presume all containers in a dependency graph are put into the same unprivileged user account and connected using a network quadlet. At least one container in the graph publish ports to the outside world. This reduces complexity and the attack surface since fewer ports are published to the outside world. Threat vectors can still flow through the entire set of dependent containers the same way as before in one unprivileged account on a network quadlet. However, the blast radius is reduced to just that unprivileged user account.

#### Formal Rule

>**\[RULE]**: ***A dependency graph of containers with at least one container exposing ports to the outside should all go into a single unprivileged user account using private network connectivity with network quadlet for inter-container communication.***

The complexity, security and performance penalties are further exasperated when containers communicate with each other using many protocols and service ports tying them together internally and forming their dependency graph. Compromising one container still has the potential compromises the entire group, but the blast radius is limited to the unprivileged user account.

#### Resource Trade-offs

There is yet another trade-off between security and resource efficiency, alongside the ease of maintenance. In enterprise environments security takes utmost precedence over efficiency and resources are abundant. In these scenarios, isolating each application container in its own unprivileged user account with dedicated database containers makes sense to minimize the blast radius and attack surface. Yeah you have application dedicated databases running everywhere but who cares you have the resources to spare and an army of developers and DBA's to tweak and manage them. So package each application container in an unprivileged user account with its own dedicated database container in the same unprivileged user account and connect them over a dedicated network quadlet.

However, in a small business or in a home lab, resources and manpower are limited. You don't want a postgresql instance per application service requiring a separate dedicated database to monitor, tweak, and manage in general. When resources and manpower are limited efficiency and easy of maintenance becomes a priority.

For example, in my home lab, I manage a single shared (multi-tenant) postgresql database server. Multiple application containers access it through their specific database users who own the application's schema. I tweak and maintain one database server instead of many. This configuration, although more risky, caters to my overwhelming need to reduce management overheads while making the most of my resources.

#### Reverse Proxies and Selective Access Patterns

This picture we've been discussing so far starts to change when reverse proxies and selective access patterns are introduced into the mix. Reverse proxies can act as intermediaries between clients and backend services, allowing for more granular control over access and communication. For example, a reverse proxy can be configured to route requests to specific backend services based on URL paths or headers, effectively isolating services from direct exposure. This can reduce the attack surface by limiting the number of exposed ports and services.

Additionally, selective access patterns can be implemented to restrict which services can communicate with each other. For instance, using firewall rules or network policies, we can define which containers are allowed to connect to specific services, further reducing the risk of lateral movement in case of a compromise. This approach allows for a more modular and secure architecture, where services can be isolated based on their roles and access requirements, rather than being grouped together solely based on their dependency relationships.

Nice theoretical discussion right? Let's move on to a concrete incarnation where the theory is demonstrated. We have the usual web application server, a database server, and a cache server triad connected together using a network quadlet in a single unprivileged user account. The web application server exposes its service port to the outside world. The database and cache servers are indirectly exposed only to the web application server over the private network quadlet.

Now instead of using the default port to port mapping (with PublishPort= or the systemd socket activator) for the web application server to expose its service port to the outside world we can selectively expose it on the host's loopback. This way only local clients on the host can access the web application server. From there, we can selectively expose it through a reverse proxy container (like Nginx, Caddy, or Traefik) running in a separate unprivileged user account. The reverse proxy container publishes and listens on standard ports (80 and 443) on all interfaces of the host and routes incoming requests to the web application server based on predefined rules. It might even wedge a WAF in front of it. This way, only the reverse proxy is directly exposed to the outside world, while the web application server remains hidden behind it. Compromise of the reverse proxy does not directly compromise the web application server since they are in separate unprivileged user accounts.

These tools also allow for complex dependency graphs to be broken apart into smaller isolated graph segments each protected to reduce the effective attack surface and blast radius. The way to do this is to leverage the reverse proxy and selective access patterns to create isolated communication channels between segments. Each segment can be placed into its own unprivileged user account with its own network quadlet. Communication between segments can be controlled and monitored through the reverse proxy and selective access patterns. This segmentation further reduces the blast radius in case of a compromise, as attackers would have to breach multiple layers of isolation to move laterally through the system.

These techniques further reduce the attack surface and blast radius by isolating services and controlling access through well-defined channels. They also allow for more flexible and scalable architectures, where services or groups of services can be added or modified without affecting the overall security posture especially when clustered and load balanced.

## 4.0 Create a pgAdmin quadlet with cross-over dependency

## 5.0 Use bro-activator to connect and activate containers over the network

## 6.0 Rollback

## Lessons Learned
