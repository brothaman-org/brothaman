# Networking

Although a large part of this lab focuses on systemd socket activation, we will also cover custom Podman networks and inter-container connectivity. The lab builds upon the foundational knowledge from previous labs, particularly the PostgreSQL setup from the Volumes Lab.

In this lab you will:

1. Retrofit the test-postgresql quadlet with systemd socket activation
2. Define a network quadlet for a private network
3. Connect containers to the private network
4. Discuss when to use cross-over ports vs. network quadlets for container communication

## 0. Snapshot

Before starting this lab, create a VM snapshot to allow easy rollback in case of issues.

```bash
vagrant snapshot save networking-start
```

## 1. Retrofit PostgreSQL with Socket Activation

The previous Volumes Lab had you create a `test-postgresql` quadlet that started a PostgreSQL container with a published port using the `PublishPort=5432:5432` directive. The directive uses Podman's built-in port forwarding mechanism to expose the container's PostgreSQL service on the host's port 5432. It works great, however another approach provides for systemd socket activation enabling cross-over dependencies and on-demand triggering.

>**SECURITY NOTE**: By default, `PublishPort=5432:5432` binds to **all interfaces** (0.0.0.0:5432), making the service accessible from any network interface on the host. For security, you can bind to specific IP addresses:
>* `PublishPort=127.0.0.1:5432:5432` - localhost only (most secure)
>* `PublishPort=192.168.1.100:5432:5432` - specific IP address only
>* `PublishPort=[::1]:5432:5432` - IPv6 localhost only
>
>**Note**: Podman does **not** support binding to interface names (like `eth0`). You must use specific IP addresses. To find interface IP addresses, use `ip addr show` or `hostname -I`.
>
>This interface binding control that `PublishPort=` has is also available in systemd socket activation, where the socket unit determines the listening interface and port. Try to restrict as much as possible instead of binding to every interface.

In this lab section, you will modify that quadlet to use systemd socket activation with `systemd-socket-proxyd` instead of Podman's proxy infrastructure with its `PublishPort` directive. This change could improve resource utilization using on-demand triggering, starting the PostgreSQL container only with incoming connections. This involves creating a `.socket` unit that listens on the desired port and a corresponding `.service` unit that starts the container when a connection is made.

### 1.1 Motivation

Now that's tightly packed so let's break it down. In systemd, user-scoped services running under unprivileged user accounts cannot use dependency directives like `After=` or `Requires=` to depend on user-scoped services running under other unprivileged user accounts. By "cross-over" we mean across systemd scope boundaries which are security boundaries, essentially across different unprivileged user accounts. The impact of these limitations is that other containers depending on the `test-postgresql` quadlet cannot express a dependency on it when running under a different unprivileged user account.

>**NOTE**: User-scoped services can however depend on system-wide "like" (user scoped pseudo) targets like `networking-online.target` or `default.target` in `WantedBy=` directives. 

User scope cross-over dependencies between unprivileged users are not supported, but we can work around this limitation using systemd socket activation while also benefiting from on-demand activation. This is a powerful primitive that allows services to start on-demand when a connection is made to a listening socket. There are also ways to shut down services that are not being used and have been idle for some time. By using socket activation, other containers can depend on an active socket instead of a service unit, allowing for cross-over dependencies to actually work.

### 1.2 Background

Systemd socket activation allows services to start on-demand, even shutting them down after being idle to reduce resource usage. A socket unit listens for incoming connections and activates the service unit when a connection is received, the `accept()` call. The mechanism is particularly useful for containers, as it allows them to remain stopped until they're needed.

>**NOTE**: On-demand triggering may introduce slight latency on the first connection as the container starts up. However, subsequent connections will be fast as the container remains running until explicitly stopped. Measures can be taken to shutdown the container after a period of inactivity if desired too. This makes sense in the case of large numbers of infrequently used services. In general though, the kernel is very efficient at managing idle resources across processes and those running in containers, so the benefits may be marginal. KVM VMs benefit more from being stopped when idle, because the Linux kernel cannot reclaim the guest's memory while it's running.

It works by having the socket unit listen on a specified port (e.g., 5432 for PostgreSQL). When a client connects, systemd starts the associated service unit, which in turn starts the container then passes the file descriptors of the socket through the service to the container so it can handle the incoming connection. The socket hand off mechanism is efficient with near native performance. However the socket handoff requires support for systemd's socket passing mechanism. Although many services support socket activation natively, many do not. For those that do not, like PostgreSQL, we can use `systemd-socket-proxyd` as a proxy to forward connections from the host to the container's internal network namespace.

Handing off the listening socket to the container is achieved using `systemd-socket-proxyd`, as intermediary, which forwards connections from the host to the container's internal network namespace. The extra proxying effort adds minimal latency while enabling socket activation for services that do not natively support it.

#### Socket Descriptor Passing Support

Podman added socket file descriptor passing support, see <https://podman.io/blogs/2022/09/30/podman-4.0.html#socket-activation>, allowing containers to receive incoming connections directly through the host's socket file descriptors. There's a long chain of forking involved to get the socket file descriptors to the container's service which must support systemd's socket descriptor passing. This support just added it to Podman and OCI containers which separately added support too. It does not include the network service wrapped in the container. There are two problems with this approach:

1. Some key services do not natively support systemd socket descriptor passing, including PostgreSQL.
2. Container image customizations are required to handle the socket passing mechanism: just see what it takes to make an echo service in an `echo.container` work with socket passing in the [Podman documentation](https://podman.io/blogs/2022/09/30/podman-4.0.html#socket-activation).

This feels way too involved for most use cases. Instead, Brothaman uses `systemd-socket-proxyd` as a workaround for all network services, especially ones that **DO NOT** natively support socket descriptor passing. The proxy runs inside the container's network namespace and forwards connections from the host's service socket to the container's service port bound usually to the loopback. The configuration is straightforward, more secure, and works with any container image without modification.

Once set up, the socket activation mechanism works as follows:

1. The socket unit is started and listens on the host's port (e.g., 0.0.0.0:5432 for PostgreSQL)
2. When a client connects (accept()), systemd activates the service unit passing it the socket file descriptors
3. The service unit starts the container if it's not already running
4. The service unit starts `systemd-socket-proxyd` inside the container's network namespace using `bro-helper`, forwarding connection traffic to the container's internal service port (e.g., 127.0.0.1:5432 for PostgreSQL)
5. The container's network service (in this case PostgreSQL) handles the incoming connection

Socket passing concerns are all handled by the `systemd-socket-proxyd` proxy, allowing the container's service to operate normally without any special socket descriptor handling considerations. There is a minimal performance cost for the proxy as with the PublishPort feature of Podman, but we're more than willing to bear it for the on-demand activation, simplicity of use, and compatibility with any container image that it provides.

Brothaman's `bro-activate` command creates activator socket and service pairs as add-on infrastructure to enable any container quadlet to use socket activation with minimal effort. So say for a test-postgresql.container quadlet, it can create a `test-postgresql-activator.socket` and `test-postgresql-activator.service` pair that listens on port 5432 and starts the `test-postgresql.container` quadlet when a connection is made.

Furthermore, it is fully compatible with cross-over dependencies between unprivileged user accounts regardless of the container and whether or not its service supports socket descriptor passing or not. For example, I can have two separate `bro-user` created `gitea` and `postgres` unprivileged user accounts, with a gitea container running gitea yet hitting the host port of the postgresql service running under the postgres unprivileged user account. Here the postgresql server be triggered to start on-demand via socket activation and both the gitea container and postgresql server can run under separate unprivileged user accounts. Heck I can even socket activate the gitea container's web port so it too fires up on-demand when a web request comes in!

>**CONTRIBUTE**: Anyone interested in adding a `bro-activate-direct` command to use Podman's native socket descriptor passing support directly without the `systemd-socket-proxyd` service proxying traffic is welcome to contribute it! A good benefit of this is that it avoids having an extra service and an extra hop which proxies traffic. Without this direct option we chose convenience over performance.

#### TCP/UDP Sockets vs. Unix Domain Sockets

Systemd socket activation can use TCP or UDP sockets as well as Unix Domain Sockets (UDS). Both activation mechanisms listen for connections: one works on remotely accessible network ports, whereas Unix Domain Sockets (UDS) are file-based pipes used for inter-process communication on the same host. UDS can also be used for cross-over dependencies between different user accounts, but those accounts must be on the same host. Also direct UDS file access with the right permissions is required: very doable using a common group.

TCP/UDP sockets, on the other hand, used in systemd socket activation listen on network ports which can be accessed both locally and remotely to activate services on-demand. No file access permissions are need for it to work. This is particularly useful for other remote services to depend on TCP activated services over the network. Unlike UDS, TCP/UDP sockets do not require direct file access permissions. Remote access makes them more flexible for cross-over dependencies yet keep in mind this occurs at the price of greater risk since services are now exposed over the network. Proper firewalling and security measures should be in place to protect these services from unauthorized access.

#### Hint: Proxying from any interface to any other interface

`systemd-socket-proxyd` can proxy from any host specific interface (not just `0.0.0.0`) to any container interface, not just its loopback, `127.0.0.1`. This includes Podman created networks. So, it could proxy from a container to a dedicated inter-container network bridge. It connects containers to networks in general, while also activating those same containers on-demand. The same mechanism enables connecting containers to inter-container network quadlets (network definitions), not just a particular host interfaces.

### 1.3 Create the PostgreSQL Activator Service Unit

Before creating the socket unit, we need to create the corresponding service unit that will be activated when a connection is made to the socket.

When this service unit is activated it activates the `test-postgresql.service` quadlet if it is not already running. Once the container is running, the service unit retrieves the container's PID using `podman inspect` and then starts `systemd-socket-proxyd` inside the container's network namespace using `bro-helper`.

In a `lingeruser` shell, a new file named `postgresql-activator.service` is created in the systemd user directory with the following content:

```bash
systemctl --user disable test-postgresql-activator.service
mkdir -p ~/.config/systemd/user/
cat <<EOF > ~/.config/systemd/user/test-postgresql-activator.service
[Unit]
Description=Test PostgreSQL Activator Service
Requires=test-postgresql-activator.socket test-postgresql.service
After=test-postgresql-activator.socket test-postgresql.service

[Service]
Type=simple

# Create the environment file for the proxy helper with the target container PID
ExecStartPre=/usr/bin/env sh -c "/usr/bin/podman inspect -f 'TARGET_PID={{.State.Pid}}' test-postgresql > %t/test-postgresql-proxyd.env"

# Environment variable file for the proxy helper does not have to exist with the dash
EnvironmentFile=-%t/test-postgresql-proxyd.env

# Wait until Postgres is accepting connections inside its netns
ExecStartPre=/usr/bin/env sh -c "i=0; while [ \$i -lt 20 ]; do \
  /usr/bin/podman unshare nsenter -t \"\$TARGET_PID\" -n \
    pg_isready -h 127.0.0.1 -p 5432 && exit 0; \
  i=\$((i+1)); sleep 1; \
done; exit 1"

# Key is the use of exec to replace the shell with podman process
# Use podman unshare to enter user namespace, then bro-helper for network namespace
ExecStart=/usr/bin/env sh -c 'exec podman unshare /usr/local/bin/bro-helper --pid "\${TARGET_PID}" -- /lib/systemd/systemd-socket-proxyd 127.0.0.1:5432'

NoNewPrivileges=no
PrivateTmp=yes
ProtectHome=yes
ProtectSystem=full

Restart=on-failure
RestartSec=5s
TimeoutStopSec=120

[Install]
WantedBy=default.target
EOF

systemctl --user daemon-reload
systemctl --user enable test-postgresql-activator.service
```

Let's break down each of the significant directives in this service unit:

* `Requires=` and `After=`: Ensures that the socket unit and the PostgreSQL container service are started before this service can be activated.
* `ExecStartPre=`: Several pre-start commands are used to gather the target container's PID and verify that PostgreSQL is ready to accept connections which makes the system much more robust.
* `ExecStart=`: The main command starts `systemd-socket-proxyd` inside the container's network namespace using `bro-helper`.
* `NoNewPrivileges`, `PrivateTmp`, `ProtectHome`, and `ProtectSystem`: Security directives to limit the service's privileges and access.
* `Restart=` and `RestartSec=`: Configures the service to restart on failure.

### 1.4 Create the Socket Unit

The `postgresql-activator.service` above is activated by the `postgresql-activator.socket` unit. This socket unit listens on port 5432 and triggers the service unit when a connection is made. In a `lingeruser` shell create a new file named `postgresql-activator.socket` in the systemd user directory with the following content:

```bash
systemctl --user stop test-postgresql.service
mkdir -p ~/.config/systemd/user/
cat <<EOF > ~/.config/systemd/user/test-postgresql-activator.socket
[Unit]
Description=PostgreSQL Activator Socket

[Socket]
ListenStream=5432
NoDelay=true
ReusePort=true
Backlog=128

[Install]
WantedBy=default.target
EOF

systemctl --user daemon-reload
systemctl --user enable test-postgresql-activator.socket
systemctl --user restart test-postgresql-activator.socket
```

The socket is now set up to listen on port 5432. When a connection is made, it will trigger the associated service unit with the same name. The socket unit configuration includes directives:

* `ListenStream=5432`: Listens on TCP port 5432 for PostgreSQL (all interfaces)
* `NoDelay=true`: Disables Nagle's algorithm for lower latency
* `ReusePort=true`: Allows multiple sockets to bind to the same port
* `Backlog=128`: Sets the maximum number of pending connections

>**INTERFACE BINDING**: Like `PublishPort=`, systemd socket units bind to all interfaces by default. For security, you can specify:
>* `ListenStream=127.0.0.1:5432` - localhost only
>* `ListenStream=192.168.1.100:5432` - specific IP
>* `ListenStream=[::1]:5432` - IPv6 localhost
>
>Multiple `ListenStream=` directives can bind to different interfaces simultaneously.

In the beginning before anything occurs, the only enabled and started unit is the socket unit (postgresql-activator.socket).

So what happens when a connection is made to the socket: i.e. with the psql client?

```bash
export PGPASSWORD=password
psql -h localhost -U postgres
```

When a connection is made to port 5432, i.e. with psql, the socket unit activates its (same name) service unit: `postgresql-activator.service`. Activating `postgresql-activator.service` in turn activates `test-postgresql.service`. Once the container is up, as detected by looping checks in an `ExecStartPre=` directive's shell command, the container's PID is retrieved using `podman inspect`. Finally, `systemd-socket-proxyd` is started inside the PostgreSQL container's network namespace using `bro-helper`. This setup allows incoming connections on port 5432 to be proxied directly (in one hop) to the PostgreSQL service running inside the container.

It's essentially a substitute for Podman's built-in port forwarding mechanism, but with the added benefits of on-demand socket activation which allows us to implement cross-over dependency triggering.

### 1.5 Update the test-postgresql quadlet

Up until now since the volume lab we have been using Podman's built-in port forwarding with the `PublishPort=` directive in the `test-postgresql.container` quadlet. We no longer need that now since systemd socket activation is handling incoming connections. Below we remove the `PublishPort=` directive from the `test-postgresql.container` quadlet.

Also note we're not starting the service directly as usual after updating the quadlet and reloading. It will now be started by the activator service and its socket when anyone tries to connect.

Update the `test-postgresql.container` quadlet as follows and give it a try:

```bash
if systemctl --user is-active test-postgresql.service; then
  systemctl --user stop test-postgresql.service
fi
mkdir -p ~/.config/containers/systemd/
cat > ~/.config/containers/systemd/test-postgresql.container <<'EOF'
[Unit]
Description=PostgreSQL Container
PropagatesStopTo=postgres-data-volume.service
BindsTo=postgres-data-volume.service
After=postgres-data-volume.service

[Container]
Image=postgres:16
ContainerName=test-postgresql
Environment=POSTGRES_PASSWORD=password
Volume=/home/lingeruser/postgres:/var/lib/postgresql/data
Network=none

[Service]
Restart=always
TimeoutStartSec=300
StartLimitBurst=5
RestartSec=10s

[Install]
WantedBy=default.target
EOF

systemctl --user daemon-reload
```

Now that everything is set up, we can validate that the socket activation is working as expected. Remember we stopped the `test-postgresql.service` above, so it should not be running yet. Check the status of the socket and service units:

```bash
systemctl --user is-active test-postgresql.service
systemctl --user is-active test-postgresql-activator.socket
systemctl --user is-active test-postgresql-activator.service
```

Basically the `test-postgresql.service` should be **in**active, the `test-postgresql-activator.socket` should be active, and the `test-postgresql-activator.service` should be **in**active. Now try connecting to PostgreSQL using the `psql` client:

```bash
export PGPASSWORD=password
psql -h localhost -U postgres
# Try running a simple SQL command, e.g.: SELECT version();
```

The first connection attempt may take a few seconds as the container starts up and is fully available online. Once connected, you should be able to run SQL commands as usual. After disconnecting, check the status of the units again:

```bash
systemctl --user is-active test-postgresql.service
systemctl --user is-active test-postgresql-activator.socket
systemctl --user is-active test-postgresql-activator.service
```

All three units should now be active, indicating that the PostgreSQL container is running and ready to accept connections.

Notice in the container quadlet that `Network=none` is used to disable Podman's default network setup since socket proxying handles all network connectivity for us. Incidentally, Podman's default proxy forwarding mechanism with the `PublishPort=` directive would also work with `Network=none`. The `Network=` directive has no impact on port publishing or socket activation mechanisms directly.

### 1.6 Explanation of Network= directive

You might be asking, then what the heck is the point of the Network= directive?

The directive will become much more apparent as we go through this lab and the next using it. But in short the `Network=` directive in a Podman-Quadlet `.container` file is used to specify how the container's networking should be set up for inter-container communication and communication with the outside world. Here's a summary of what it's for and how it works:

* It tells Podman (via the Quadlet generator) **which network mode or network(s)** the container should attach to. Yes there are modes.
* It can either select a built-in networking mode (like `none`, `host`, or modes provided by user-mode stacks such as `pasta:` or `slirp4netns:`) **or** specify one or more custom networks (defined as `.network` units) that the container joins. It allows us to create private inter-container networks within an unprivileged user account.
* It influences how the container sees the outside world and the network: whether it has its own network namespace, shares the host network, has isolated connectivity, or is connected to a custom defined bridge/network.

#### How it is used (value options)

* `Network=none` → The container is given no network interface (or a very limited one); internet access is disabled. We often use this for highly isolated containers. With networking set to none, you still have the loopback interface in the container. Because of that you can run things like postgresql with Network=none and still publish its port with PublishPort=5432:5432.
* `Network=host` → The container shares the host's network namespace; it sees the host's network interfaces directly. Useful for performance or when the container needs direct access to host facilities.
* `Network=pasta:[OPTIONS]` → Use the “pasta” networking backend (common in rootless Podman) with specified options for how connectivity, host-gateway mapping, loopback mapping etc. should work.
* `Network=slirp4netns:[OPTIONS]` → Use the slirp4netns backend for rootless containers with its own configuration. Pasta is preferred nowadays but slirp4netns is still supported. Whatever slirp4netns can do Pasta can as well more efficiently.
* `Network=<custom-network>.network` → Join a user-defined Podman network that was created with a `.network` Quadlet unit, allowing multiple containers on the same isolated network.

Multiple `Network=` lines may be used to attach a container to **more than one network** (depending on setup) for advanced scenarios.

#### Why it matters

* It defines **how the container communicates**: will it see the host directly, will it be isolated, will it join a specific subnet, will it be able to reach other containers.
* It affects things like port forwarding, host-loopback access, container-to-container access, and which backend (slirp, pasta, network bridge) is used — each backend has different capabilities/limitations (e.g., rootless mode support, IP mapping, performance).
* Selecting the correct `Network=` value is essential when you want to do things like “container talks to host” or “container talks to another container on same host” under rootless conditions.

#### Things to watch out for

* Some modes (especially rootless ones like `pasta:` and `slirp4netns:`) have **extra options** required if you want features like host-loopback access or preserving source IPs. Example: `--map-gw`, `--map-host-loopback`, etc.
* If you choose `Network=none`, port publishing only work out of the box for internal services that bind to the container's loopback interface.
* When you specify a custom network (`.network` unit), you must ensure it is created/defined ahead of time and that the container file references it by name.
* Rootless networking comes with extra constraints (e.g., interfaces, namespace privileges) and the choice of backend in `Network=` affects what is possible.

### 1.7 Use bro-activator to connect and activate containers over the network

<!-- TODO: write the bro-activate specifications and implement the script -->
WIP


## 2.0 Define a network quadlet

Again as the `lingeruser` run the following commands to create a network quadlet named `test.network` that generates a Podman network service:

```bash
mkdir -p ~/.config/containers/systemd/
cat > ~/.config/containers/systemd/test.network <<'EOF'
[Unit]
Description=Test Network Quadlet
After=network-online.target

[Network]
Subnet=192.168.100.0/24
Gateway=192.168.100.1
DNS=192.168.100.1

[Install]
WantedBy=default.target
EOF
systemctl --user daemon-reload
systemctl --user start test-network.service
systemctl --user status test-network.service -l --no-pager
podman network list
podman network inspect systemd-test
if ping -W 0.5 -c 1 192.168.100.1 > /dev/null; then
  echo "Ping to gateway 192.168.100.1 successful"
else
  echo "Ping to gateway 192.168.100.1 failed"
fi
```

The quadlet creates a private bridge network with the specified subnet, gateway, and DNS settings in its own network namespace owned by the unprivileged user. Commands like the `ping` above and `brctl show` on the host's network namespace will **NOT** display the bridge. You can see for yourself by installing `bridge-utils` as the `vagrant` user if needed and try for yourself:

```bash
sudo apt-get install -y bridge-utils
sudo brctl show
```

We can use the following command to switch to the unprivileged user's network namespace and see the bridge interface created by Podman for this network:

```bash
podman unshare --rootless-netns /usr/sbin/brctl show
```

The rootless-netns option allows us to enter the unprivileged user's network namespace where the Podman networks are created.

But wait, you see nothing right? Well this was to show you that Podman does not activate the bridge until a container is connected to it. This could produce some gotchyas situations so we wanted to point this out. Try again after connecting a container to the network in the next major section.

## 3.0 Connect containers to the network

This is great but we need to connect the container onto this network. Modify the `test-postgresql.container` quadlet adding `Network=test.network` to connect it to the `test` network as follows:

```bash
if systemctl --user is-active test-postgresql.service; then
  systemctl --user stop test-postgresql.service
fi
mkdir -p ~/.config/containers/systemd/
cat > ~/.config/containers/systemd/test-postgresql.container <<'EOF'
[Unit]
Description=PostgreSQL Container
PropagatesStopTo=postgres-data-volume.service
BindsTo=postgres-data-volume.service
After=postgres-data-volume.service

[Container]
Image=postgres:16
ContainerName=test-postgresql
Environment=POSTGRES_PASSWORD=password
Volume=/home/lingeruser/postgres:/var/lib/postgresql/data
Network=test.network

[Service]
Restart=always
TimeoutStartSec=300
StartLimitBurst=5
RestartSec=10s

[Install]
WantedBy=default.target
EOF
systemctl --user daemon-reload
systemctl --user start test-postgresql.service
systemctl --user status test-postgresql.service -l --no-pager
```

### 3.1 Verify bridge activation

Let's try that bridge lookup and ping the gateway again to verify the bridge is now active in the unprivileged user's (rootless) network namespace:

```bash
podman unshare --rootless-netns ip addr show
podman unshare --rootless-netns /usr/sbin/brctl show
# Uses the ping program inside the host but executes in the rootless netns
if podman unshare --rootless-netns ping -W 0.5 -c 1 192.168.100.1 > /dev/null; then
  echo "Ping to gateway 192.168.100.1 successful"
else
  echo "Ping to gateway 192.168.100.1 failed"
fi
```

You'll see the `podman1` bridge interface and the ping to the gateway is successful from within the rootless network namespace. The `test-postgresql` container has its own namespace but the veth device connects them together. The veth device type is a virtual Ethernet pair that connects two network namespaces. One end of the veth pair is placed inside the container's network namespace, while the other end is attached to the bridge in the rootless network namespace. This setup allows network traffic to flow between the container and the bridge, enabling communication with other containers connected to the same bridge network as well as external networks through NAT. Let's demonstrate this clearly in the next section.

### 3.2 Verify network namespaces and their isolated views

Let's print out the namespace inodes for the host, container, and rootless network namespace to see just how they are all different:

```bash
# Container has its own network namespace (anonymous, not named)
# Compare namespace inodes to verify isolation
echo "Host netns inode: $(stat -c %i /proc/self/ns/net)"
echo "Container netns inode: $(podman exec test-postgresql stat -c %i /proc/self/ns/net)"
echo "Rootless netns inode: $(podman unshare --rootless-netns stat -c %i /proc/self/ns/net)"
```

Notice that all network namespace inodes are different. Now let's look at the isolation this achieved. First let's look at the host's network namespace view of interfaces and bridges:

```bash
ip link show
/usr/sbin/brctl show
```

You should see the `lo` loopback and your main network interface (e.g., `eth0` or `ens3`), but you will **NOT** see the Podman created bridge or veth endpoints. This is because the Podman networks are created in the unprivileged user's rootless network namespace, not the host's network namespace.

Now let's look at the rootless (unprivileged user's) network namespace view of interfaces and bridges:

```bash
# Rootless namespace can see the bridge and veth endpoints
podman unshare --rootless-netns ip link show
podman unshare --rootless-netns /usr/sbin/brctl show
```

Here we see the `podman1` bridge interface and the veth endpoint connected to it, including the one connected to the `test-postgresql` container. This demonstrates the isolation provided by separate network namespaces while still allowing connectivity through the veth pair that acts as a tunnel through the namespaces.

Now let's look at the `test-postgresql` container's network namespace view of interfaces:

```bash
# Container namespace can see its own veth endpoint only
podman exec test-postgresql bash -c '
  NONINTERACTIVE=1 apt-get update &&
  NONINTERACTIVE=1 apt-get install -y bridge-utils'
podman exec test-postgresql ip link show
podman exec test-postgresql /usr/sbin/brctl show
```

Unlike the rootless network namespace, the container's network namespace cannot see the bridge or other veth endpoints. It can only see its own veth endpoint (usually named `veth0@ifX` where X is an number unless renamed). This demonstrates the isolation (different network device views) provided by separate network namespaces while still allowing connectivity through the veth pair that acts as a tunnel through the namespaces. Pretty cool right? This bridge network can now be used to connect multiple containers together on the same network each in their own namespace.

### 3.3 Verify container connectivity

Let's peak inside the container to see its assigned IP address on the network:

```bash
podman exec -it test-postgresql bash -c '
  ( NONINTERACTIVE=1 apt-get update &&
  NONINTERACTIVE=1 apt-get install -y iproute2 inetutils-ping net-tools ) >/dev/null &&
  ip a s'
IP=$(podman exec -it test-postgresql hostname -i)
trim_string() {
    : "${1#"${1%%[![:space:]]*}"}"
    : "${_%"${_##*[![:space:]]}"}"
    printf '%s\n' "$_"
}
IP=$(trim_string "$IP")
echo "Container IP address is: $IP"
if podman exec -it test-postgresql bash -c 'ping -W 1 -c 1 192.168.100.1 >/dev/null'; then
  printf "Ping from container %s to gateway 192.168.100.1 successful\n" "$IP"
else
  printf "Ping from container %s to gateway 192.168.100.1 failed\n" "$IP"
fi
```

There you go! The container has an IP address assigned from the network bridge. Now we were able to ping the gateway from inside the container. In fact, maybe you notice before, we are also routing to the Internet and resolving DNS queries since apt pulled down our packages. This shows that the container is successfully connected to the custom Podman network defined by the `test.network` quadlet. If you want to disable Internet access you can modify the network quadlet by adding a `Internal=true` directive and restarting the `test-network.service`. The `Internal=true` directive in a Podman network quadlet creates an internal-only network that blocks external internet access while still allowing:

* Container-to-container communication within the same network
* Container-to-host communication (limited through the rootless namespace)

This is useful for isolating containers from the outside world while still allowing them to communicate with each other and the host system.

### 3.4 IPAM Settings

Podman networks use an IPAM (IP Address Management) mechanism, not traditional DHCP. Here's how it works:

* Podman uses a static IPAM allocation system
* IP addresses are assigned sequentially from the subnet range
* No DHCP daemon running - it's handled by Podman's network stack
* IP assignments are persistent and deterministic

How IP Assignment Works:

* Network Creation: Subnet defined (e.g., 192.168.100.0/24)
* Container Connection: Podman assigns next available IP from range
* Static Assignment: IP is statically configured in container's netns
* Persistence: Same container gets same IP on restart (usually)

IPAM Configuration Options:

```ini
# Network quadlet with IPAM control
[Network]
Subnet=192.168.100.0/24
Gateway=192.168.100.1
IPRange=192.168.100.100-192.168.100.200  # Limit assignable range
```

You can also assign specific IPs to existing containers without restarting them:

```bash
# Disconnect from current network first (if already connected)
podman network disconnect systemd-test test-postgresql || true

# Reconnect with a specific IP - hot assignment
podman network connect --ip 192.168.100.157 systemd-test test-postgresql

# Verify the new IP assignment
podman exec test-postgresql hostname -i
```

Let's verify the setting by inspecting the network:

```bash
# Extract just the subnet and gateway
podman network inspect systemd-test | jq -r '.[0].containers[] | select(.name=="test-postgresql") | .interfaces.eth0.subnets[0].ipnet'

# Extract container's IP address using jq
podman inspect test-postgresql | jq -r '.[0].NetworkSettings.Networks."systemd-test".IPAddress'
```

### 3.5 Ping an external IP from inside the container

Let's play with a ping quirk before moving on. Try pinging `8.8.8.8` from inside the container:

```bash
if podman exec -it test-postgresql bash -c 'ping -W 1 -c 1 8.8.8.8 >/dev/null'; then
  echo "Ping from container to external IP 8.8.8.8 successful"
else
  echo "Ping from container to external IP 8.8.8.8 failed"
fi
```

ICMP ping typically doesn't work from inside rootless Podman containers for several reasons related to unprivileged containers and network namespaces. By default, unprivileged containers lack the necessary capabilities to create raw sockets required for ICMP operations. Additionally, network namespaces isolate the container's network stack from the host, preventing direct access to certain network functionalities. These factors combined result in the inability to successfully ping external IPs like `8.8.8.8`. Let's change this by changing the `net.ipv4.ping_group_range` (for UIDs) setting on the host to include the unprivileged user range. As the `vagrant` user run:

```bash
cat /proc/sys/net/ipv4/ping_group_range
sudo sysctl -w net.ipv4.ping_group_range="0 2000000"
cat /proc/sys/net/ipv4/ping_group_range
```

Now `sudo su - lingeruser` and try the ping again:

```bash
if podman exec -it test-postgresql bash -c 'ping -W 1 -c 1 8.8.8.8 >/dev/null'; then
  echo "Ping from container to external IP 8.8.8.8 successful"
else
  echo "Ping from container to external IP 8.8.8.8 failed"
fi
```

Works!

### 3.4 Connecting and disconnecting containers from the network

You can connect and disconnect containers from the network using Podman commands. For example, to disconnect the `test-postgresql` container from the `test` network, run:

```bash
podman exec -it test-postgresql ip a s
podman network disconnect systemd-test test-postgresql
if podman exec -it test-postgresql bash -c 'ping -W 1 -c 1 8.8.8.8 >/dev/null'; then
  echo "Ping from container to external IP 8.8.8.8 successful"
else
  echo "Ping from container to external IP 8.8.8.8 failed"
fi
podman exec -it test-postgresql ip a s
```

As you can see the container can no longer ping external IPs since it's disconnected from the network. The veth interface, the tap from the container into the bridge is gone. To reconnect the container to the network, run:

```bash
podman network connect systemd-test test-postgresql
if podman exec -it test-postgresql bash -c 'ping -W 1 -c 1 8.8.8.8 >/dev/null'; then
  echo "Ping from container to external IP 8.8.8.8 successful"
else
  echo "Ping from container to external IP 8.8.8.8 failed"
fi
podman exec -it test-postgresql ip a s
```

The container can now ping external IPs again since it's reconnected to the network. The veth interface is back, reconnecting the container to the bridge network.

### 3.5 Network quadlet conventions review

Let's break down network quadlets conventions:

The `test.network` quadlet generates a `test-network.service` unit. It uses the convention of appending `-network` to the name before the dot in the quadlet name followed by `.service` for the service unit name.

When listing the network in `podman network list` its network name is displayed as `systemd-test`. It uses the convention of taking the quadlet name before the dot and prefixing it with `systemd-`. If we want to change the listed name we can add a `NetworkName=` directive to the `[Network]` section.

### 3.6 Network quadlets and unprivileged cross-over

By default, containers in separate unprivileged user accounts cannot access Podman network quadlet in other unprivileged user accounts due to them having separate rootless network namespace. One could create veth pairs to attach containers in one user's network namespace to a network in another user's namespace, but this would requires elevated privileges and its not recommended. Doing so goes against the principle of least privilege. The limitation is actually a good security measure. Each unprivileged user account is isolated from others to prevent unauthorized access and maintain system integrity.

The best option for ***cross-over*** communication between containers owned by different unprivileged users is by way of port mappings via `PublishPort=` or the socket activation pattern demonstrated in section 1.0. If containers expose service ports to host interfaces, other unprivileged user accounts can connect to those services over the host's network interfaces. This method still maintains isolation between user accounts while allowing necessary communication between services across user boundaries.

**Example**: If unprivileged user account 'A' runs container 'a' with `PublishPort=127.0.0.1:8080:80`, then container 'b' running under unprivileged user account 'B' can access container 'a's service by connecting to `127.0.0.1:8080` on the host's loopback interface when configured to do so with user-mode networking (Network=slirp4netns or Network=pasta). FYI, Network=none cannot be set. The loopback in the container is not the loopback in the host: it is the container's own loopback interface. The traffic flows:

```text
Container 'b' → Host loopback (127.0.0.1:8080) → port mapping → Container 'a' (port 80)
```

This works because:

* Both containers can access the host's network interfaces from within their separate network namespaces (with proper Network= directive settings)
* The host's loopback interface (127.0.0.1) is accessible from any container's network namespace so long as the user-mode networking backend is configured to allow it
* Podman's port mapping bridges the gap between the host interface and the target container's service port
* No elevated privileges or network namespace bridging required

The same principle works with all public interfaces included (e.g., `PublishPort=0.0.0.0:8080:80`) for remote access, though localhost binding is more secure and always desired for inter-container communication on the same host.

Separate unprivileged user account isolation is important for several reasons:

1. **Security**: It helps to prevent unauthorized access to resources and services. By isolating user accounts, we reduce the risk of one user compromising another user's containers or data.

2. **Resource Management**: Isolating user accounts allows for better resource management and allocation. Each user can have their own set of resources without interfering with others.

3. **Stability**: If one user's container crashes or experiences issues, it won't affect the containers or services running under other user accounts.

4. **Compliance**: In multi-tenant environments, isolation is often a requirement for compliance with security policies and regulations.

While it may seem limiting, these isolation measures are crucial for maintaining a secure and stable environment when running containers under unprivileged user accounts.

## 4.0 When to use cross-over dependencies vs. network quadlets

***So, when should cross-over dependencies between unprivileged user accounts be used vs. using a network quadlet to connect multiple containers within a single unprivileged user account?*** This is a great question. Effectively it is the same as asking, "When should I bundle containers into the same unprivileged user account vs. when should I separate them into different unprivileged user accounts?"

Surprisingly, the answer is not black or white: it depends on specific use cases, the security posture imposed, and on resource and performance constraints. To speak the same language, let's define some terms first. This is an involved conversation.

### 4.1 Our own terminology definitions

>**NOT EXHAUSTIVE**

* **Directly Exposed \[containers|services]**: Are containers or services exposing ports to the outside of a common security scope. The common security scope boils down to a network namespace where containers are concerned. What is considered the outside includes the global host's global network namespace containing its loopback interface. Directly exposed containers are typically intended to be accessed by external consumers (usually end users) of the service: like a web application server in a container exposing port 8443 to serve an application to the outside world to use.

* **Indirectly Exposed \[containers|services]**: Are containers or services not exposing service ports directly, but instead are reachable by other containers (perhaps directly or indirectly) exposed to the outside world: i.e. a database server in a container that only exposes its database service port to a web application container within the same unprivileged user account and users access the application from the network.

The mentality here is, every part of a system, no matter how much, is still exposed to some extent if it is connected or in other words dependency relationships exist. The question is how it is exposed? How easily can attackers compromise it?

Directly exposed containers and their services are at greater risk. A slew of network stack based vectors increase their attack surface. Indirectly exposed containers and their services are at lower risk with a smaller attack surface. They can still be compromised since they are reachable through other containers, but attachers have to first compromise the directly exposed containers before they can even attempt to pivot into the indirectly exposed containers which have more constrained interactions with directly exposed containers.

### 4.2 Existing terminology adapted to our domain

>**NOT EXHAUSTIVE**

* **Cross-over dependency**: A container in one unprivileged user account depending on a service port exposed by a container in another unprivileged user account. This is typically done using Podman's `PublishPort=` feature or systemd socket activation to forward traffic from one container to another across user boundaries.

* **Dependency graph**: In our context, a set of containers that depend on each other to provide a complete application or service. For example, a web application container that depends on a database container and a cache container.

* **Blast radius**: The extent of impact or damage that can occur if a container is compromised. A smaller blast radius means that the compromise is contained within a limited scope, reducing the potential harm to the overall system. Sometimes it helps thinking about this in terms of contagion risk and in terms of concentric circles of impact.

* **Attack surface**: The total number of points (or vectors) where an unauthorized user can try to enter or extract data from a system. A larger attack surface means more potential vulnerabilities.

* **Pivoting**: The technique used by attackers to move from one compromised system or container to another within a network. This often involves exploiting vulnerabilities in interconnected systems to gain further access.

* **Least privilege principle**: A security concept that advocates for granting only the minimum necessary permissions to users or services to perform their tasks, reducing the risk of unauthorized access.

### 4.3 Contagion Risk

From a security perspective containers depending on each other (an app to its database and cache server for example) represent a single combined attack surface together. This is because a request entering a container from the outside can still exploit vulnerabilities to pivot into potentially all its dependent containers. These containers form a dependency graph where attackers can penetrate and compromise each node in the entire dependency graph.

These dependencies and the contagion risks they give rise to are due to one container requiring access to another container's service ports through network communication: i.e. the web application container needs to connect to the database container over the database's service port. They're all exposed, some are directly exposed while others are indirectly exposed. All containers in the dependency graph are exposed to a degree. We already pointed this out earlier.

That said though, the deeper into the dependency graph, the more difficult it gets for attackers to pivot. Contagion risk decreases the deeper into the dependency graph you go. A web application container directly exposed to the outside world has a higher chance of compromise than its dependent database container which is only indirectly exposed to the outside world through the web application container. An attacker would have to first compromise the web application container before they can even attempt to pivot into the database container. Also, the database container typically has a more constrained interaction pattern with the web application container (e.g., only specific queries allowed) which further reduces the attack surface and contagion risk. This reduces the opportunities an attacker has to pivot into the database container.

#### Every container into its own unprivileged user account

Let's play devil's advocate and consider the case where every container in a dependency graph is placed into its own unprivileged user account and connected using cross-over dependencies.

Putting each container into its own unprivileged user account appears to reduce the blast radius and it often does, however attackers can potentially still pivot through the dependency graph spreading through unprivileged user accounts. We have to say this no matter how slim the chances are. With that said though, compromising an application running in an unprivileged container is one thing, and compromising the unprivileged execution environment to issue commands and access other containers in the unprivileged account is another. It all depends on the application, and what it has been allowed to do. If the application can fork and execute commands, then its easy to compromise the unprivileged container environment after compromising the application. There are many ways to harden unprivileged containers to reduce this risk, but it is still there. So, no matter how we skin this cat, the possibility of compromise through the dependency graph is still there. But remember contagion risk usually decreases the deeper into the dependency graph you go.

Let's consider inter-container connectivity: namely, providing specific TCP/UDP port access verses full network connectivity between containers. Putting each container into its own unprivileged user account requires exposing ports for containers to interact over. Constraining access to specific ports for container communication is generally a better strategy than giving full connectivity between containers. However, the network quadlet is private without exposure to outside security contexts. Exposing many ports across unprivileged user accounts into outside security contexts increases complexity and the attack surface. There are real tradeoffs to consider.

#### All containers in one unprivileged user account connected via a network quadlet

Putting all containers on the same network quadlet in a single unprivileged user account could reduce complexity, the attack surface, and improve performance but now every container is included in the blast radius when the unprivileged account is compromised.

The middle road is best. That is putting containers in a dependency graph into the same unprivileged user account and using a private network quadlet to connect them. At least one container in the graph publishes ports to the outside world. This reduces complexity and the attack surface since fewer ports are exposed to the outside world. Execution still flows through the entire set of dependent containers the same way as before in an unprivileged account using a private network quadlet. The blast radius is still limited to the unprivileged user account and there's no proxying of traffic between containers adding latency and performance penalties.

#### Formal Rule

>**\[RULE]**: ***A dependency graph of containers with at least one container exposing ports to the outside should all go into a single unprivileged user account using private network connectivity with a network quadlet for all dependent inter-container communication.***

#### Resource Trade-offs

There is yet another trade-off between security and resource efficiency, and the ease of maintenance. In enterprise environments security often takes precedence over resources since resources are usually abundant. In these scenarios, isolating an application container in its own unprivileged user account with a dedicated database container only for the application's data makes sense to minimize the blast radius and attack surface. You have dedicated application databases running everywhere, but who cares: enterprises have the developers and DBA's to tweak and manage them. For maximum security in enterprise environments, **ALWAYS** place applications and their dedicated databases into the same unprivileged user account and connect them over a dedicated network quadlet: **NEVER USE SHARED RESOURCES**. You can expand this mantra beyond databases to any kind of resource.

Conversely, in a small business or in a home lab, resources and manpower are limited. Manpower is usually the primary constraint. You don't want a postgresql instance per application service requiring a separate dedicated database service to have to monitor, tweak, and manage. When resources and manpower are limited, efficiency and easy of maintenance becomes a greater priority and you start to compromise.

In a home lab, a single shared (multi-tenant) postgresql database server may make sense. Databases are multi-tenant systems after all. Multiple applications can use the same database instance with each application having its own database user and schema on the same shared instance. Only one database server consumes resources, which is a significant advantage. Tuning and monitoring it can be done in a centralized manner which is even more advantageous. This configuration, although less secure, caters to the overwhelming need to reduce management overheads while making the most of limited resources.

>**FUTURE**: I see AI handling maintenance for us eventually so that problem will go away. Also hardware will become so cheap that resource constraints will also go away. In the end, if you can, use dedicated resources for maximum security. Until then, we have to make do with what we have.

#### Reverse Proxies and Selective Access Patterns

Combining the power of reverse proxies and selective access patterns can further reduce the attack surface and blast radius beyond what we've discussed so far. It's a powerful combination that can significantly enhance the security posture of containerized applications.

Reverse proxies can do a lot of things as intermediaries but from a security perspective they can act as policy enforcement points. The reverse proxy acts as an authorization gateway between clients and backend services, allowing for more granular control over access and communication. For example, a reverse proxy can be configured to route requests to specific backend services based on identities, and authorization rules or reject them all together. This can reduce the attack surface by limiting the number of exposed ports and services.

Selective access patterns can be implemented to restrict which services can communicate with each other. For instance, using firewall rules or network policies, we can define which containers are allowed to connect to specific services, further reducing the risk of lateral movement in case of a compromise. This approach allows for a more modular and secure architecture, where services can be isolated based on their roles and access requirements, rather than being grouped together solely based on their dependency relationships.

Nice theoretical discussion right? Let's move on to concrete problems demonstrating the theories and rules in action, then actually implement real solutions in the next major section.

#### Concrete Example

We have the usual web application server, a database server, and a cache server triad connected together using a network quadlet in a single unprivileged user account. The web application server exposes its service port to the host on its loopback interface. The database and cache servers are only indirectly exposed through the web application server. All are connected over a private network quadlet in the unprivileged user account.

Only local clients on the host can access the web application server. From there, we can selectively expose it to the network through a reverse proxy container (like Nginx, Caddy, or Traefik) running in a separate unprivileged user account. Remember all unprivileged user accounts have access to the host's loopback if you configure them to. The reverse proxy container can be port mapped to listen on 8443 for HTTPS traffic to route traffic back and forth from the web application server since it can connect to the web application server's port over the host's loopback interface. A firewall rule on the host forwards incoming traffic on port 443 to port 8443 for the reverse proxy since the reverse proxy operating unprivileged cannot bind to ports below 1024.

The reverse proxy can do so much more. It can even wedge a WAF in front of the application. This way, only the reverse proxy is itself directly exposed to the world outside of the host, while multiple web application servers remain hidden behind it. Compromise of the reverse proxy does not necessarily compromise the web application servers since they are in separate unprivileged user accounts.

These techniques further reduce the attack surface and blast radius by isolating services and controlling access through well-defined channels and access control points. They also allow for more flexible and scalable architectures, where services or groups of services can be added or modified without affecting the overall security posture especially when clustered and load balanced.

In the next section we consider a very dangerous application, pgAdmin, and a full blown example of how to secure it properly.

## 5.0 Rollback

You can roll back to the previous VM snapshot before we made all these changes with the following command on your host:

```bash
vagrant snapshot restore networking-start
```

## Lessons Learned

In this lab we learned how to create and manage custom Podman networks using systemd quadlets. We created a private bridge network with specific subnet, gateway, and DNS settings. We connected containers to this network and verified connectivity between containers and the outside world. We explored network namespaces and their isolated views of network interfaces. We discussed IPAM settings and how Podman assigns IP addresses to containers. Finally, we delved into security considerations when using cross-over dependencies vs. network quadlets for container communication across unprivileged user accounts.
