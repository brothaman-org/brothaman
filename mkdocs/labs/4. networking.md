# Networking

Although a large part of this lab focuses on systemd socket activation, we will also cover custom Podman networks and inter-container connectivity. The lab builds upon the foundational knowledge from previous labs, particularly the PostgreSQL setup from the Volumes Lab.

In this lab you will:

1. Retrofit the test-postgresql quadlet with systemd socket activation
2. Create an unprivileged pgadmin service and demonstrate cross-over dependency triggering
3. Create network quadlets to connect multiple containers and compare connectivity mechanisms

## 0. Snapshot

Before starting this lab, create a VM snapshot to allow easy rollback in case of issues.

```bash
vagrant snapshot save networking-start
```

## 1. Retrofit PostgreSQL with Socket Activation

The previous Volumes Lab had you create a `test-postgresql` quadlet that started a PostgreSQL container with a published port using the `PublishPort=5432:5432` directive. The directive uses Podman's built-in port forwarding to expose the container's PostgreSQL service on the host's port 5432. It works great, however another approach provides for systemd socket activation enabling cross-over dependency triggering and better performance.

In this lab section, you will modify that quadlet to use systemd socket activation with `systemd-socket-proxyd` instead of Podman's proxy infrastructure with its `PublishPort` directive. This change improves resource efficiency and triggers PostgreSQL container startup only with incoming connections. This involves creating a `.socket` unit that listens on the desired port and a corresponding `.service` unit that starts the container when a connection is made.

### 1.1 Motivation

Now that's tightly packed so let's break it down. In systemd, user-scoped services running under unprivileged user accounts cannot use dependency directives like `After=` or `Requires=` to depend on user-scoped services running under other unprivileged user accounts. By "cross-over" we mean across systemd scope boundaries, essentially across different unprivileged user accounts. The impact of these limitations is that other containers depending on the `test-postgresql` quadlet cannot express a dependency on it when running under a different unprivileged user account.

>**NOTE**: User-scoped services can however depend on system-wide "like" (user scoped pseudo) targets like `networking-online.target` or `default.target` in `WantedBy=` directives. 

User scope cross-over dependencies between unprivileged users are not supported, but we can work around this limitation using systemd socket activation while also benefiting from on-demand activation. This is a powerful primitive that allows services to start on-demand when a connection is made to a listening socket. By using socket activation, other containers can depend on an active socket instead of a service unit, allowing for cross-over dependencies to actually work.

### 1.2 Background

Systemd socket activation allows services to start on-demand, reducing resource usage when the service is idle. A socket unit listens for incoming connections and activates the service unit when a connection is received, the `accept()` call. The mechanism is particularly useful for containers, as it allows them to remain stopped until  they're needed.

It works by having the socket unit listen on a specified port (e.g., 5432 for PostgreSQL). When a client connects, systemd starts the associated service unit, which in turn starts the container then passes the file descriptors of the socket through the service to the container so it can handle the incoming connection. The socket hand off mechanism is efficient with near native performance. However the socket handoff requires support for systemd's socket passing mechanism. Although many services support socket activation natively, many do not. For those that do not, like PostgreSQL, we can use `systemd-socket-proxyd` as a proxy to forward connections from the host to the container's internal network namespace.

Handing off the listening socket to the container is achieved using `systemd-socket-proxyd`, as intermediary, which forwards connections from the host to the container's internal network namespace. The extra proxying effort adds minimal latency while enabling socket activation for services that do not natively support it.

#### Socket Descriptor Passing Support

Podman added socket file descriptor passing support, see [here](https://podman.io/blogs/2022/09/30/podman-4.0.html#socket-activation), allowing containers to receive incoming connections directly through the host's socket  file descriptors. There's a long chain of forking involved to get the socket file descriptors to the container's service which must support systemd's socket descriptor passing. This support just added it to Podman and OCI containers which separately added support too. It does not include the network service wrapped in the container. There are two problems with this approach:

1. Many services do not natively support systemd socket descriptor passing, including PostgreSQL.
2. Container image customizations are required to handle the socket passing mechanism: just see what it takes to make an echo service in an `echo.container` work with socket passing in the [Podman documentation](https://podman.io/blogs/2022/09/30/podman-4.0.html#socket-activation).

This feels way too involved for most use cases. Instead, Brothaman uses `systemd-socket-proxyd` as a workaround for all network services, especially ones that do not natively support socket descriptor passing. The proxy runs inside the container's network namespace and forwards connections from the host's socket to the container's internal service port.

Once set up, the socket activation mechanism works as follows:

1. The socket unit listens on the host's port (e.g., 5432 for PostgreSQL).
2. When a client connects, systemd activates the service unit passing it the file descriptors
3. The service unit starts the container if it's not already running.
4. The service unit starts `systemd-socket-proxyd` inside the container's
    network namespace using `bro-helper`, forwarding connections to the container's internal service port (e.g., 5432 for PostgreSQL).
5. The container's service handles the incoming connection.

Socket passing concerns are all handled by the `systemd-socket-proxyd` proxy, allowing the container's service to operate normally without any special socket descriptor handling considerations. There is a minimal performance cost for the proxy as with the PublishPort feature of Podman, but we're more than willing to bear it for the on-demand activation, simplicity and container image compatibility it provides.

Brothaman's `bro-socket-proxyd` command creates activator socket and service pairs as addon infrastructure to enable any container quadlet to use socket activation with minimal effort. Furthermore it is compatible with cross-over dependencies between unprivileged user accounts regardless of the container and whether or not its service supports socket descriptor passing or not.

>**CONTRIBUTE**: Anyone interested in adding a `bro-socket-direct` command to use Podman's native socket descriptor passing support directly without the proxy is welcome to contribute it!

#### TCP/UDP Sockets vs. Unix Domain Sockets

Systemd socket activation can use TCP or UDP sockets as well as Unix Domain Sockets (UDS). Both activation mechanisms listen for connections: one works on remotely accessible network ports, whereas Unix Domain Sockets (UDS) are file-based sockets used for inter-process communication on the same host. UDS can also be used for cross-over dependencies between different user accounts, but they're limited to the same host, while also requiring direct UDS file access with the right permissions.

TCP/UDP sockets, on the other hand, used in systemd socket activation listen on network ports which can be accessed both locally and remotely to activate services on-demand. No file access permissions are need for it to work. This is particularly useful for other remote services to depend on TCP activated services over the network. Unlike UDS, TCP/UDP sockets do not require direct file access permissions. Remote access makes them more flexible for cross-over dependencies yet keep in mind this occurs at the price of greater risk since services are now exposed over the network. Proper firewalling and security measures should be in place to protect these services from unauthorized access.

#### Hint: Proxying from any interface to any other interface

`systemd-socket-proxyd` can proxy from any host specific interface (not just `0.0.0.0`) to any container interface, not just its loopback, `127.0.0.1`. So, it could proxy from a container ONLY dedicated inter-container network to the container. It connects containers to networks while also activating containers on-demand. The same mechanism enables connecting containers to inter-container network quadlets, not just the host's external interfaces.

### 1.3 Create the PostgreSQL Activator Service Unit

Before creating the socket unit, we need to create the corresponding service unit that will be activated when a connection is made to the socket.

When this service unit is activated it activates the `test-postgresql.service` quadlet if it is not already running. Once the container is running, the service unit retrieves the container's PID using `podman inspect` and then starts `systemd-socket-proxyd` inside the container's network namespace using `bro-helper`.

In a `lingeruser` shell, a new file named `postgresql-activator.service` is created in the systemd user directory with the following content:

```bash
mkdir -p ~/.config/systemd/user/
cat <<EOF > ~/.config/systemd/user/postgresql-activator.service
[Unit]
Description=PostgreSQL Activator Service
Requires=postgresql-activator.socket test-postgresql.service
After=postgresql-activator.socket test-postgresql.service

[Service]
Type=simple

# Create the environment file for the proxy helper with the target container PID
ExecStartPre=/usr/bin/env sh -c "/usr/bin/podman inspect -f 'TARGET_PID={{.State.Pid}}' test-postgresql > %t/postgresql-proxyd.env"

# Environment variable file for the proxy helper does not have to exist with the dash
EnvironmentFile=-%t/postgresql-proxyd.env

# Demonstrates proper in namespace execution of bro-helper by gathering network info
ExecStartPre=/usr/bin/env sh -c "/usr/local/bin/bro-helper --pid \${TARGET_PID} -- ip -o addr"
ExecStartPre=/usr/bin/env sh -c "/usr/local/bin/bro-helper --pid \${TARGET_PID} -- netstat -tlpn"

# Wait until Postgres is accepting connections inside its netns
ExecStartPre=/usr/bin/env sh -c "i=0; while [ \$i -lt 20 ]; do \
  /usr/bin/podman unshare nsenter -t \"\$TARGET_PID\" -n \
    pg_isready -h 127.0.0.1 -p 5432 && exit 0; \
  i=\$((i+1)); sleep 1; \
done; exit 1"

# Key is the use of exec to replace the shell with the proxyd process
# Hands off the listening socket to systemd-socket-proxyd as desired
ExecStart=/usr/bin/env sh -c 'exec /usr/local/bin/bro-helper --pid "\${TARGET_PID}" -- /lib/systemd/systemd-socket-proxyd 127.0.0.1:5432'

NoNewPrivileges=no
PrivateTmp=yes
ProtectHome=yes
ProtectSystem=full

Restart=on-failure
RestartSec=5s
TimeoutStopSec=120

[Install]
WantedBy=default.target
EOF

systemctl --user daemon-reload
systemctl --user enable postgresql-activator.service
```

Let's break down each of the significant directives in this service unit:

- `Requires=` and `After=`: Ensures that the socket unit and the PostgreSQL container service are started before this service can be activated.
- `ExecStartPre=`: Several pre-start commands are used to gather the target container's PID and verify that PostgreSQL is ready to accept connections which makes the system much more robust.
- `ExecStart=`: The main command starts `systemd-socket-proxyd` inside the container's network namespace using `bro-helper`.
- `NoNewPrivileges`, `PrivateTmp`, `ProtectHome`, and `ProtectSystem`: Security directives to limit the service's privileges and access.
- `Restart=` and `RestartSec=`: Configures the service to restart on failure.

So what happens when a connection is made to the socket: i.e. with the psql client?

```bash
psql -h localhost -U postgres
```

In the beginning before anything occurs, the only enabled and started unit is the socket unit (postgresql-activator.socket).

When a connection is made to port 5432, i.e. with psql, the socket unit activates its (same name) service unit: `postgresql-activator.service`. Activating `postgresql-activator.service` in turn activates `test-postgresql.service`. Once the container is up, as detected by looping checks in an `ExecStartPre=` directive's shell command, the container's PID is retrieved using `podman inspect`. Finally, `systemd-socket-proxyd` is started inside the PostgreSQL container's network namespace using `bro-helper`. This setup allows incoming connections on port 5432 to be proxied directly (in one hop) to the PostgreSQL service running inside the container.

It's essentially a substitute for Podman's built-in port forwarding mechanism, but with the added benefits of on-demand socket activation which allows us to implement cross-over dependency triggering.

### 1.4 Create the Socket Unit

The `postgresql-activator.service` above is activated by the `postgresql-activator.socket` unit. This socket unit listens on port 5432 and triggers the service unit when a connection is made. In a `lingeruser` shell create a new file named `postgresql-activator.socket` in the systemd user directory with the following content:

```bash
systemctl --user stop test-postgresql.service
mkdir -p ~/.config/systemd/user/
cat <<EOF > ~/.config/systemd/user/postgresql-activator.socket
[Unit]
Description=PostgreSQL Activator Socket

[Socket]
ListenStream=5432
NoDelay=true
ReusePort=true
Backlog=128

[Install]
WantedBy=default.target
EOF

systemctl --user daemon-reload
systemctl --user enable --now postgresql-activator.socket
```

The socket is now set up to listen on port 5432. When a connection is made, it will trigger the associated service unit with the same name. The socket unit configuration includes directives:

- `ListenStream=5432`: Listens on TCP port 5432 for PostgreSQL
- `NoDelay=true`: Disables Nagle's algorithm for lower latency
- `ReusePort=true`: Allows multiple sockets to bind to the same port
- `Backlog=128`: Sets the maximum number of pending connections

### 1.5 Update the test-postgresql quadlet

Now we do not need Podman's built-in port forwarding anymore since systemd socket activation is handling incoming connections. Below we remove the `PublishPort=` directive from the `test-postgresql.container` quadlet. Also note we're not starting the service as usual after updating the quadlet and reloading. It will now be started by the activator service and its socket. Update the `test-postgresql.container` quadlet as follows: 

```bash
if systemctl --user is-active test-postgresql.service; then
  systemctl --user stop test-postgresql.service
fi
mkdir -p ~/.config/containers/systemd/
cat > ~/.config/containers/systemd/test-postgresql.container <<'EOF'
[Unit]
Description=PostgreSQL Container
PropagatesStopTo=postgres-data-volume.service
BindsTo=postgres-data-volume.service
After=postgres-data-volume.service

[Container]
Image=postgres:16
ContainerName=test-postgresql
Environment=POSTGRES_PASSWORD=password
Volume=/home/lingeruser/postgres:/var/lib/postgresql/data
Network=none
UserNS=keep-id

[Service]
Restart=always
TimeoutStartSec=300
StartLimitBurst=5
RestartSec=10s

[Install]
WantedBy=default.target
EOF

systemctl --user daemon-reload
```

### 1.4 Validate Socket Activation

Now that everything is set up, we can validate that the socket activation is working as expected. Remember we stopped the `test-postgresql.service` above, so it should not be running yet. Check the status of the socket and service units:

```bash
systemctl --user is-active test-postgresql.service
systemctl --user is-active postgresql-activator.socket
systemctl --user is-active postgresql-activator.service
```

Basically the `test-postgresql.service` should be **in**active, the `postgresql-activator.socket` should be active, and the `postgresql-activator.service` should be **in**active. Now try connecting to PostgreSQL using the `psql` client:

```bash
psql -h localhost -U postgres
# Try running a simple SQL command, e.g.: SELECT version();
```

The first connection attempt may take a few seconds as the container starts up and is fully available online. Once connected, you should be able to run SQL commands as usual. After disconnecting, check the status of the units again:

```bash
systemctl --user is-active test-postgresql.service
systemctl --user is-active postgresql-activator.socket
systemctl --user is-active postgresql-activator.service
```

All three units should now be active, indicating that the PostgreSQL container is running and ready to accept connections.

## 2.0 Define a network quadlet

```bash
mkdir -p ~/.config/containers/systemd/
cat > ~/.config/containers/systemd/testnet.network <<'EOF'
[Unit]
Description=TestNet Quadlet
After=network-online.target

[Network]
NetworkName=testnet
Subnet=192.168.100.0/24
Gateway=192.168.100.1
DNS=192.168.100.1

[Install]
WantedBy=default.target
EOF
systemctl --user daemon-reload
systemctl --user start testnet-network.service
systemctl --user status testnet-network.service
postman network list
```

## 3.0 Connect containers to the network

## 4.0 Create a pgAdmin quadlet with cross-over dependency

## 5.0 Use bro-socket-proxyd to connect containers to the network

## 6.0 Rollback

## Lessons Learned
