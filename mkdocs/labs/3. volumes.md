# Volumes

In this lab you will orchestrate storage using Podman volume quadlets: first wiring bind mounts into container units so nginx can serve the synced MkDocs site, then defining dedicated volume units whose lifecycle is decoupled from their consuming containers. Along the way you'll manage PostgreSQL data on ZFS, practice snapshotting and rollbacks, and see how volume descriptors integrate cleanly with container services and why a separate volume quadlet is beneficial in many scenarios.

1. Serve Brothaman's docs with `Volume=` directives in test-quadlet.container's `[Container]` section
2. Creating a volume quadlet
3. Switching to the zfs-helper for snapshot management
4. Connecting the volume to the PostgreSQL container quadlet
5. Testing data persistence and snapshot rollbacks

## 0. Snapshot

Since this test runs inside a Vagrant VM, capture a baseline snapshot before making any changes. From your host machine (in the same directory as your Vagrantfile):

```shell
if ! vagrant snapshot list 2>&1 | grep before-volumes >/dev/null; then
  vagrant snapshot save before-volumes;
else
  echo before-volumes snapshot already exists;
fi
```

This allows you to roll the VM back to its exact pre-test state later.

## 1. Serve Brothaman's docs with `Volume=` directives in test-quadlet.container's `[Container]` section

In the last lab we created a simple nginx based container quadlet. By default without any configuration it serves a default test page. Let's go one step further and have it serve the brothaman website.

If you look into the Vagrantfile an rsync command is used to copy the local `docs` directory into the VM at `/brothaman` with your lingeruser's uid:

```ruby
  config.vm.synced_folder "docs", "/brothaman",
    type: "rsync",
    create: true,
    owner: 1001,
    group: 1001,
    rsync__chown: true,
    rsync__auto: false,
    rsync__args: [
      "--verbose",
      "--archive",
    ]
```

To have nginx serve this content, a virtual server configuration needs to be passed telling it how and from where to serve the content. The following nginx server configuration tells nginx to use /var/www/brothaman as its server root:

```ini
mkdir ~/nginx-conf.d/
cat > ~/nginx-conf.d/brothaman.org.conf <<'EOF'
server {
    listen 80;
    server_name brothaman.org www.brothaman.org;

    # put your site files here (we just made it above)
    root /var/www/brothaman;
    index index.html index.htm;

    # log to container stdout/stderr (best for containers)
    access_log /dev/stdout;
    error_log  /dev/stderr warn;

    location / {
        try_files $uri $uri/ =404;
    }
}
EOF
```

The container author configured nginx to look for server configurations in `/etc/nginx/conf.d/`. Our configuration needs to reside there to get picked up at startup. Instead of copying the file into the container image, we can use a bind mount to map our configuration directory onto the container at `/etc/nginx/conf.d/` the quadlet descriptor, `Volume=/home/lingeruser/nginx-conf.d:/etc/nginx/conf.d`. Another `Volume=` mapping, `Volume=/brothaman:/var/www/brothaman`, also directly within the test-quadlet's `[Container]` section. Together nginx can then serve this directory as using the brothaman.org virtual server:

```bash
mkdir -p ~/.config/containers/systemd/
cat > ~/.config/containers/systemd/test-quadlet.container <<'EOF'
[Unit]
Description=Quadlet Test Container

[Container]
Image=nginx:alpine
Exec=nginx -g 'daemon off;'
ContainerName=test-quadlet
PublishPort=8080:80
Volume=/brothaman:/var/www/brothaman
Volume=/home/lingeruser/nginx-conf.d:/etc/nginx/conf.d
UserNS=keep-id

[Service]
Restart=always
TimeoutStartSec=300
StartLimitIntervalSec=60s
StartLimitBurst=5
RestartSec=10s

[Install]
WantedBy=default.target
EOF
```

Before starting the container notice no separate volume quadlet was needed: meaning we did not create a `dot.volume` file. Both directives were put into the container quadlet's `[Container]` section directly. There's only one service, the test-quadlet container service, and it can now serve the mkdocs content. Reload systemd and start the container:

```bash
systemctl --user daemon-reload
systemctl --user restart test-quadlet.service
curl "http://$(ip route | grep 'kernel scope link' | awk '{print $9}'):8080"
```

You should see the bro site contents. You can point your browser to the VM's IP address at port 8080 to see it as well. Don't worry you don't have to set up DNS for brothaman.org, just accessing the IP:8080 will work fine. With one virtual server configuration nginx just uses it as the default site.

## 2. Creating a volume quadlet

The example above is so simple, no separate volume quadlet was needed. However, for more complex scenarios where you:

* share the volume with multiple containers, or
* you want to manage the lifecycle of the volume separately from the container, or
* you want to use advanced volume features like ZFS snapshots

It's beneficial to create a dedicated volume quadlet. In this lab section, the concept will be put to the test so we can see how it works and helps.

### 2.1 Create a ZFS backed volume quadlet for PostgreSQL

Let's consider a database container like PostgreSQL which needs persistent storage for its data. Before the container starts, we want to take a snapshot of the data. In case anything goes wrong during the container's operation, we can roll back to the previous snapshot. In such cases, a dedicated volume quadlet is the way to go to isolate the functionality.

Let's use a ZFS-backed volume to take snapshots of the data volume between container restarts. We start by creating a ZFS dataset for PostgreSQL data storage and set it to mount at `/home/lingeruser/postgres`. Then we create a volume quadlet to manage this dataset.

Make sure you're logged as the `vagrant` user to escalate privileges to root with `sudo`. Below a ZFS dataset, `testing/postgres`, is created then its mountpoint set to `/home/lingeruser/postgres`: it mounts automatically using the ZFS mount service. The mountpoint's user and group ownership permissions are also set to be owned by lingeruser so lineruser's unprivileged containers can access it.

```bash
sudo zfs create testing/postgres
sudo zfs set mountpoint=/home/lingeruser/postgres testing/postgres
sudo chown lingeruser:lingeruser /home/lingeruser/postgres
sudo -iu lingeruser ls -ld postgres
df -h /home/lingeruser/postgres/
```

The dataset is now ready to be used as a persistent volume for our PostgreSQL container. Now switch back to the `lingeruser` again with `sudo su - lingeruser`:

```bash
mkdir -p ~/.config/containers/systemd/
cat > ~/.config/containers/systemd/postgres-data.volume <<'EOF'
[Unit]
Description=PostgreSQL Volume

[Volume]
VolumeName=postgres-data

[Service]
ExecStartPre=/usr/sbin/zfs snapshot testing/postgres@pre
EOF

systemctl --user daemon-reload
systemctl --user start postgres-data-volume.service
```

>**NOTICE**: The `postgres-data.volume` quadlet is started and stopped using a service name that combines the volume name, the `-volume` suffix, and the `.service` extension. The service name construction pattern is automatically handled by podman when you create a volume quadlet.

### 2.2 Troubleshooting Problems

How did it go for you? Using `journalctl --user -xe -u postgres-data-volume.service`, here are the failure errors in the service journal log:

```shell
Oct 26 14:13:16 debian12.localdomain systemd[904]: Starting postgres-data-volume.service - PostgreSQL Volume...
Oct 26 14:13:16 debian12.localdomain postgres-data-volume[45001]: cannot create snapshots : permission denied
Oct 26 14:13:16 debian12.localdomain systemd[904]: postgres-data-volume.service: Control process exited, code=exited, status=1/FAILURE
Oct 26 14:13:16 debian12.localdomain systemd[904]: postgres-data-volume.service: Failed with result 'exit-code'.
Oct 26 14:13:16 debian12.localdomain systemd[904]: Failed to start postgres-data-volume.service - PostgreSQL Volume.
```

The permission error is expected. The `lingeruser` does not have permission to create snapshots. Try this as `vagrant` to escalate to sudo and delegate snapshot permission to `lingeruser` on the dataset:

```shell
sudo zfs allow lingeruser snapshot testing/postgres
```

Now sudo back to `lingeruser` and start the volume service again:

```shell
systemctl --user start postgres-data-volume.service
systemctl --user status postgres-data-volume.service
zfs list -t snapshot testing/postgres
```

You should see the volume service started successfully this time and a snapshot was created. Amazing! Now try stopping the volume service and starting it again now with:

```shell
systemctl --user status postgres-data-volume.service
systemctl --user start postgres-data-volume.service
```

This fails too because the snapshot with the same name already exists. Feel free to check the error message with `journalctl --user -xe -u postgres-data-volume.service`. Let's create snapshots with unique names using timestamps prefixed to the base name of the snapshot:

```bash
mkdir -p ~/.config/containers/systemd/
cat > ~/.config/containers/systemd/postgres-data.volume <<'EOF'
[Unit]
Description=PostgreSQL Volume

[Volume]
VolumeName=postgres-data

[Service]
ExecStartPre=/usr/sbin/zfs snapshot testing/postgres@pre_$(date +%Y%m%d%H%M%S)
EOF

```

This too fails and the output looks a little cryptic in the logs:

```log
Oct 28 08:28:45 debian12.localdomain systemd[904]: Starting postgres-data-volume.service - PostgreSQL Volume...
Oct 28 08:28:45 debian12.localdomain postgres-data-volume[78548]: usage:
Oct 28 08:28:45 debian12.localdomain postgres-data-volume[78548]:         snapshot [-r] [-o property=value] ... <filesystem|volume>@<snap> ...
Oct 28 08:28:45 debian12.localdomain postgres-data-volume[78548]: For the property list, run: zfs set|get
Oct 28 08:28:45 debian12.localdomain postgres-data-volume[78548]: For the delegated permission list, run: zfs allow|unallow
Oct 28 08:28:45 debian12.localdomain systemd[904]: postgres-data-volume.service: Control process exited, code=exited, status=2/INVALIDARGUMENT
Oct 28 08:28:45 debian12.localdomain systemd[904]: postgres-data-volume.service: Failed with result 'exit-code'.
```

What's going on here? The clue is in the `usage` complaint by the zfs CLI program. It's not able to parse the command line. The problem is that the `$(date +%Y%m%d%H%M%S)` command substitution is not being interpreted by systemd as expected. To fix this, we need to wrap the command substitution in a shell execution context. Update the `ExecStartPre` line as follows and try again:

```bash
mkdir -p ~/.config/containers/systemd/
cat > ~/.config/containers/systemd/postgres-data.volume <<'EOF'
[Unit]
Description=PostgreSQL Volume

[Volume]
VolumeName=postgres-data

[Service]
ExecStartPre=/bin/sh -c '/usr/sbin/zfs snapshot testing/postgres@pre_"$(date +%Y%m%d%H%M%S)"'
EOF

systemctl --user daemon-reload
systemctl --user start postgres-data-volume.service
```

This too fails. Take a look at why in the journal, but there's a bunch of junk generated instead of the date string we wanted to append. That happens because systemd is performing specifier expansion. The '%' and '$' characters have special meanings in systemd unit files. They need to be escaped to be interpreted literally. Update the `ExecStartPre` line again as follows:

```bash
mkdir -p ~/.config/containers/systemd/
cat > ~/.config/containers/systemd/postgres-data.volume <<'EOF'
[Unit]
Description=PostgreSQL Volume

[Volume]
VolumeName=postgres-data

[Service]
ExecStartPre=/bin/sh -c '/usr/sbin/zfs snapshot testing/postgres@pre_"$$(date +%%Y%%m%%d%%H%%M%%S)"'
EOF

systemctl --user daemon-reload
systemctl --user start postgres-data-volume.service
zfs list -t snapshot testing/postgres
```

Now this works! Each time the volume service is started a new snapshot with a unique timestamped name is created. Great! Keep trying it and test by starting and restarting the service. Here let me help you:

```bash
for restart in $(seq 1 3); do
  systemctl --user restart postgres-data-volume.service
  sleep 2
done
zfs list -t snapshot testing/postgres
```

### 2.3 Let's delete the extra snapshots

That's looking good. Multiple snapshots were created with unique names. Perfect! But now we got that `pre` without the prefix let's try deleting that still logged in as the `lingeruser`:

```bash
zfs destroy testing/postgres@pre
```

In fact we cannot delete any snapshot. You should see they all fail due to a lack of permissions. That's because the `lingeruser` does not have permission to destroy snapshots. Let's switch back to `vagrant` and delegate the destroy permission on the dataset to `lingeruser`:

```bash
sudo zfs allow lingeruser destroy,snapshot testing/postgres
```

Now if you then try the destroy on the snapshot again it still does not work. What's going on here? The reason is ZFS supports the delegation of snapshot creation, but not the destruction of snapshots. Even the dataset owner can not destroy snapshots. But try destroy the pre snapshot with the `vagrant` user as root:

```bash
vagrant@debian12:~$ sudo zfs destroy testing/postgres@pre
vagrant@debian12:~$ sudo zfs list -t snapshot testing/postgres
NAME                                  USED  AVAIL     REFER  MOUNTPOINT
testing/postgres@pre-20251028085229     0B      -     40.0M  -
testing/postgres@pre-20251028085533     0B      -     40.0M  -
testing/postgres@pre-20251028085535     0B      -     40.0M  -
testing/postgres@pre-20251028085537     0B      -     40.0M  -
vagrant@debian12:~$
```

## 3. Switching to the zfs-helper for snapshot management

So to manage snapshot deletion, we need to be perform the destroy operation with elevated privileges with the vagrant sudo enabled user. This is a limitation of OpenZFS's delegation model. The whole situation should be pretty irritating to you. Especially if you want to automate snapshot management in unprivileged containers and have to login and out of one user to sudo from another all the time right?

>**WARNING**: What many lazy admins (who you should never hire) often do in this case is make their unprivileged users sudoers without password prompt for zfs commands. Even adding having a sudo password should be forbidden on production systems. Unprivileged users for application sandboxing purposes should **NEVER** have the ability to escalate privileges with or without passwords. WTF is the point anyway right?

Then what if you want out of this login, log out hell, and want some consistency. First off you should be automating the process with DevOps tools, but let me not go there right now. It's still pretty irritating even if you automate things. Plus doing so you still need two different users, one with sudo, and the target non sudoers user contexts to properly manage snapshots. Ugh.

### 3.1 Setup lingeruser to use zfs-helper

Although there are several ways to skin this cat, the best is to use a helper service that runs with elevated privileges and performs zfs management tasks on behalf of unprivileged user-scoped services. Such a service needs a solid policy framework to validate and authorize requests from unprivileged users. Its authorization controls should limit what operations can be performed on which datasets by which users. Luckily we did just that for brothaman and it is called [`zfs-helper`](https://github.com/brothaman-org/zfs-helper). The stock package is already installed in the VM. All we need to do is configure it to allow our user's `postgres-data-volume.service` to manage snapshots on the `testing/postgres` dataset. See the edits added to the zfs-helper configuration files at `/etc/zfs-helper/policy.d/`:

>Perform these operations as the `vagrant` user.

```bash
sudo usermod -a -G zfshelper lingeruser
sudo loginctl kill-user lingeruser
sudo loginctl user-status lingeruser --no-pager
sudo loginctl enable-linger lingeruser
sudo loginctl user-status lingeruser --no-pager
sudo -iu lingeruser id

pushd . || true
sudo mkdir -p /etc/zfs-helper/policy.d/lingeruser
cd /etc/zfs-helper/policy.d/lingeruser
cat <<'EOF' | sudo tee -a units.list
postgres-data-volume.service
EOF

cat <<'EOF' | sudo tee -a mount.list unmount.list snapshot.list rollback.list create.list destroy.list
lingeruser testing/postgres
EOF
popd

```

Now let's use the service to manage snapshots for us. First, update the volume quadlet to use zfs-helper for snapshot creation instead of calling zfs directly. We could do this before directly since we were granted delegation, but let's do it now again to be consistent:

Issue these commands as the `lingeruser`:

```bash
mkdir -p ~/.config/containers/systemd/
cat > ~/.config/containers/systemd/postgres-data.volume <<'EOF'
[Unit]
Description=PostgreSQL Volume

[Volume]
VolumeName=postgres-data

[Service]
ExecStartPre=/bin/sh -c '/usr/bin/zfs-helperctl snapshot testing/postgres@pre_"$$(date +%%Y%%m%%d%%H%%M%%S)"'
EOF

systemctl --user daemon-reload
systemctl --user restart postgres-data-volume.service
zfs list -t snapshot testing/postgres
```

### 3.2 Delete pre-start snapshots above a limit

Now let's enhance the volume quadlet further to delete old snapshots above a certain limit each time the volume service starts. This way we can keep the number of snapshots manageable without manual intervention. Update the volume quadlet as follows to add a snapshot cleanup step before creating a new snapshot:

```bash
mkdir -p ~/.config/containers/systemd/
cat > ~/.config/containers/systemd/postgres-data.volume <<'EOF'
[Unit]
Description=PostgreSQL Volume
PartOf=test-postgresql.service

[Volume]
VolumeName=postgres-data

[Service]
Environment=KEEP_SNAPSHOTS=5
ExecStartPre=/bin/sh -c '/usr/bin/zfs-helperctl snapshot testing/postgres@pre_"$$(date +%%Y%%m%%d%%H%%M%%S)"'
ExecStartPre=/bin/sh -c 'zfs list -t snapshot -o name -s creation | grep -E "^testing/postgres@pre_*" | head -n -$${KEEP_SNAPSHOTS} | xargs -r -n1 zfs-helperctl destroy'
EOF
systemctl --user daemon-reload
systemctl --user restart postgres-data-volume.service
zfs list -t snapshot testing/postgres
```

Now each time the volume service starts, it will delete old pre-start snapshots beyond the specified limit after creating a new snapshot. This keeps the snapshot count manageable automatically. No fancy retention policies manager services or cron jobs needed. Just let the volume service handle it for you. You can verify this by restarting the volume service multiple times and checking the snapshot list:

```bash
for restart in $(seq 1 10); do
  echo "snapshot with $restart"
  systemctl --user restart postgres-data-volume.service
  sleep 3
  zfs list -t snapshot testing/postgres
done
```

## 4. Connecting the volume to the PostgreSQL container quadlet

Finally, let's connect the volume quadlet to the PostgreSQL container quadlet so the container can use the persistent storage with snapshotting and test it out. Update the PostgreSQL container quadlet to include the volume:

```bash
mkdir -p ~/.config/containers/systemd/
cat > ~/.config/containers/systemd/test-postgresql.container <<'EOF'
[Unit]
Description=PostgreSQL Container
PropagatesStopTo=postgres-data-volume.service
BindsTo=postgres-data-volume.service
After=postgres-data-volume.service

[Container]
Image=postgres:16
ContainerName=test-postgresql
Environment=POSTGRES_PASSWORD=password
Volume=/home/lingeruser/postgres:/var/lib/postgresql/data
PublishPort=5432:5432
UserNS=keep-id

[Service]
Restart=always
TimeoutStartSec=300
StartLimitBurst=5
RestartSec=10s

[Install]
WantedBy=default.target
EOF

systemctl --user daemon-reload
systemctl --user start test-postgresql.service
```

### Notes on PropagatesStopTo=, BindsTo=, PartOf=, and After=

These directives are used to manage the relationships between systemd services:

- `PartOf=`: This directive indicates that the service is part of another service. If the parent service is stopped or restarted, the child service will also be affected. We made the volume quadlet part of the container quadlet so that stopping the container also stops the volume service. If it was shared we would not opt to do this.
- `BindsTo=`: This directive creates a stronger link between services. If the service specified in `BindsTo=` is stopped, the current service will also be stopped. Make no sense to run the database without its volume right? Before the volume stops the database is shutdown allowing it to sync its buffers to prevent corruption.
- `After=`: This directive specifies that the current service should be started only after the specified service has been started. Doh! The database needs its volume to be ready before it starts.
- `PropagatesStopTo=`: This directive allows a service to propagate stop signals to its dependencies. When the container is stopped, it will also stop the volume service. This way stopping the container actually stops the volume service too. Without this directive a systemctl restart of the container service will not stop and start the volume service.

Systemd is extremely powerful and an amazing init system. In fact it goes well beyond an init system. It is an entire service management platform. Learning to use its features effectively can greatly enhance your ability to manage services and their dependencies. Unfortunately, we snuck these in, with only slight mentions as describe here to limit the scope of this lab.

With Quadlets, Podman has taken a revolutionary approach to container management by integrating deeply with systemd. This integration leverages systemd's capabilities to manage the container lifecycle and their dependencies effectively. It brings the power of systemd to container management, allowing for more robust and reliable containerized applications closer to the system. Its use of core system primitives instead of reinventing the wheel is a form of convergence. It's a game-changer for how we think about and manage containers in a Linux environment.

## 5. Testing data persistence and snapshot rollbacks

>**IMPORTANT**: For this section you will need to keep two shells open, one logged into the VM as the `vagrant` user and one logged in as the `lingeruser`. This is because rolling back snapshots requires elevated privileges that only the `vagrant` user has via sudo. The `lingeruser` will be used to interact with the PostgreSQL container and perform database operations.

You can now connect to the PostgreSQL container and verify that the data is being stored in the ZFS-backed volume in the `lingeruser` shell:

```bash
export PGPASSWORD=password
psql -h "$(ip route | grep 'kernel scope link' | awk '{print $9}')" -p 5432 -U postgres -c '\l'
```

You should see the default PostgreSQL databases listed. Let's create a new database, insert some data, and then restart the container to see that the data persists across restarts thanks to the ZFS-backed volume in the `lingeruser` shell:

```bash
# Capture the latest snapshot before creating the test database
before_testdb_snapshot=$(zfs list -t snapshot testing/postgres | grep -v NAME | awk '{print $1}' | tail -n 1)
echo "Latest snapshot before the testdb: $before_testdb_snapshot"

# Let's create the database, the test table, and insert the test data
export PGPASSWORD=password
psql -h "$(ip route | grep 'kernel scope link' | awk '{print $9}')" -p 5432 -U postgres -c 'CREATE DATABASE testdb;'
psql -h "$(ip route | grep 'kernel scope link' | awk '{print $9}')" -p 5432 -U postgres -d testdb -c 'CREATE TABLE testtable (id SERIAL PRIMARY KEY, name VARCHAR(50));'
psql -h "$(ip route | grep 'kernel scope link' | awk '{print $9}')" -p 5432 -U postgres -d testdb -c "INSERT INTO testtable (name) VALUES ('testname');"
psql -h "$(ip route | grep 'kernel scope link' | awk '{print $9}')" -p 5432 -U postgres -d testdb -c 'SELECT * FROM testtable;'

# Bounce the server to create another snapshot with the new db, table, and one testname record
systemctl --user restart test-postgresql.service
testname_snapshot=$(zfs list -t snapshot testing/postgres | grep -v NAME | awk '{print $1}' | tail -n 1)
echo "Latest snapshot with only testable: $testname_snapshot"
sleep 5

# Verify the data is still there and add another name, anthony
psql -h "$(ip route | grep 'kernel scope link' | awk '{print $9}')" -p 5432 -U postgres -d testdb -c 'SELECT * FROM testtable;' 
psql -h "$(ip route | grep 'kernel scope link' | awk '{print $9}')" -p 5432 -U postgres -d testdb -c "INSERT INTO testtable (name) VALUES ('anthony');"
psql -h "$(ip route | grep 'kernel scope link' | awk '{print $9}')" -p 5432 -U postgres -d testdb -c 'SELECT * FROM testtable;'

# Bounce the server to create another snapshot with the new data
systemctl --user restart test-postgresql.service
anthony_snapshot=$(zfs list -t snapshot testing/postgres | grep -v NAME | awk '{print $1}' | tail -n 1)
echo "Latest snapshot with anthony: $anthony_snapshot"
sleep 5

psql -h "$(ip route | grep 'kernel scope link' | awk '{print $9}')" -p 5432 -U postgres -d testdb -c 'SELECT * FROM testtable;'
# Delete the testname entry
psql -h "$(ip route | grep 'kernel scope link' | awk '{print $9}')" -p 5432 -U postgres -d testdb -c "DELETE FROM testtable WHERE name = 'testname';"
# Verify the deletion
psql -h "$(ip route | grep 'kernel scope link' | awk '{print $9}')" -p 5432 -U postgres -d testdb -c 'SELECT * FROM testtable;'

# Shutdown the server to rollback
systemctl --user stop test-postgresql.service
no_testname_snapshot=$(zfs list -t snapshot testing/postgres | grep -v NAME | awk '{print $1}' | tail -n 1)
echo "Last snapshot without testname: $no_testname_snapshot"
```

I know that's a long block of code but it creates a database, a table, then inserts two rows, and deletes one row. It creates snapshots at each step. You can see how the data changes over time with each snapshot. The code block also saves each snapshot name into variables for later use to experiment with rollbacks. Do not close the lingeruser shell yet so we can use those values to rollback.

Let's try reverting to a previous snapshot to see the table revert to its earlier state. However, OpenZFS's delegation model has another shortcoming that bites us yet again: rollback delegation is not implemented. So to rollback we have to escalate privileges with the `vagrant` user via sudo. We already shut the server down at the end of the code block so we can rollback without corrupting the database.

>**WARNING**: Never roll back a dataset while it's mounted and in use. Always stop the container first to unmount the volume cleanly. This is especially important with databases that can be corrupted when the underlying disk changes leaving them in inconsistent states.

Let's go back to the `vagrant` user's shell to elevate privileges for these rollback operations:

```bash
sudo zfs rollback -r "[copy the anthony_snapshot variable's value from the lingeruser shell here]"
```

>Switching user shells we cannot just echo the $anthony_snapshot variable since it's not defined in the vagrant user's context. Just re-echo the value and copy it from within the lingeruser's shell. Now let's restart the PostgreSQL container and verify the data in the table:

```bash
systemctl --user start test-postgresql.service
export PGPASSWORD=password
psql -h "$(ip route | grep 'kernel scope link' | awk '{print $9}')" -p 5432 -U postgres -d testdb -c 'SELECT * FROM testtable;'
```

Now you should see the previously deleted 'testname', and the 'anthony' entry present again since we rolled back to the snapshot taken before 'testname' was deleted towards the end of the code block. It was as if the delete of the `testname` entry never happened. Amazing!

Now try rolling back successively further in time to snapshots taken before 'anthony' was added on your own. This will solidify your understanding of whats going on here. Just remember, we captured the past snapshot values in the `testname_snapshot` and `before_testdb_snapshot` variables in the `lingeruser` shell. Don't forget to shutdown the database in the vagrant user shell before each rollback.

### Why didn't you use zfs-helper for rollbacks?

The `zfs-helper` is explicitly designed to avoid delegated calls from the command line or elsewhere. It **ONLY** services requests from systemd unit services and ties authorization to systemd unit identity and kernel authenticated unix socket clients. These facilities work together to provide a secure and auditable mechanism for delegated management of ZFS datasets.

### How does it work?

Systemd services have well-defined identities and can be authenticated and authorized based on their unit names with secure cgroup checks. These checks connects `zfs-helper` policies with systemd units to enforce strict access controls for zfs management operations on a per dataset (noun), per user (subject), and per operation (verb) basis.

During each `zfs-helperctl` call it reads `/proc/self/cgroup`, maps that back to the calling unit name, and refuses requests unless that unit appears in the unit allow-list (the `units.list` you maintain under `/etc/zfs-helper/policy.d/`). That cgroup check is why you can list allowed datasets per operation and know the request actually came from the expected quadlet rather than an arbitrary process the user might spawn.

A few tightly stitched mechanisms work together to provide additional security. Unix sockets, `AF_UNIX`, and its `SO_PEERCRED` options allow the `zfs-helper` to retrieve the UID/GID/PID credentials of the calling service. The kernel verifies the identity of the socket client. The server socket is created by systemd when the `zfs-helper` service starts and is under its control. Only users in the `zfshelper` system group can access this socket. Group access further ensures that **ONLY** authorized users can communicate with the `zfs-helper` service.

In summary, when a service connects to this socket to make a request, the kernel provides the credentials of the calling process through the socket. An additional systemd user cgroup check ensures that the calling service is running within the correct user and systemd context. The allow lists control what operations can be performed on which datasets by which services. This multi-layered approach ensures that only authorized services can perform specific ZFS operations on designated datasets, providing a robust security model for delegated ZFS management.

### Can't a compromised unprivileged account create a service with the same name?

If an attacker only gets shell access as a user who happens to be in the `zfshelper` group, the cgroup gate keeps them from just running `zfs-helperctl` by hand—their shell sits in a session cgroup, not the whitelisted service cgroup, so the helper denies the request. This blocks one of the easiest privilege-abuse paths and gives you auditability (“we know postgres-data-volume.service asked for that snapshot”). Another reason why a separate volume quadlet as a separate service is a good security practice.

A compromised account can still try to install a brand new user service, but it won't match anything in `units.list`, so the helper still rejects those calls. To succeed they'd have to hijack an already-authorized unit name; at that point they can trigger whatever operations you've granted to that unit (destroy snapshots, etc.), but only on the datasets/verbs you explicitly allowed. They cannot grant themselves access to other datasets or new verbs without compromising the root-maintained policy files.

Bottom line: membership in the `zfshelper` group is powerful and should be restricted, yet the cgroup check sharply limits post-compromise blast radius to the exact dataset+operation pairs you've whitelisted. The helper doesn't stop a user from abusing the privileges you intentionally delegated, so continue to scope those policy entries tightly and monitor the corresponding services' behavior.

## 6. Rollback (Optional)

When you're done testing, you can revert the VM to the snapshot created before starting this lab in step #0. This operation occurs on the host outside of the VM (not to be confused with ZFS snapshots):

```bash
vagrant snapshot restore before-volumes
```

## Lessons Learned

We saw how adding a `Volume=` directive into container quadlets perform a bind mount of an external host directory into the container as a `source:destination` mapping. Containers like the nginx image implement a common pattern where they "import" configurations managed externally in host directories using bind mounts. BTW this was a consequence of how containers were originally used in development. The hosted site content to be served also was shared using such a bind mount.

We then moved on to create an actual volume quadlet to manage a persistent ZFS-backed volume for a PostgreSQL container. We saw how ZFS snapshots can be created before starting the container to allow rollbacks in case of corruption. However, we also encountered the limitations of OpenZFS's delegation model that complicate snapshot management for unprivileged users. To solve this, we introduced the `zfs-helper` service that runs with elevated privileges and performs snapshot management tasks on behalf of unprivileged user-scoped container services based on a defined access policy with authentication.

With `zfs-helper` in place, we updated the volume quadlet to use it for secure yet convenient snapshot creation and deletion. It removed a lot of the pain we experienced while manually managing snapshots. We also implemented automatic snapshot cleanup (retention policy) in the simplest way to maintain a manageable number of snapshots. Finally, we connected the volume quadlet to the PostgreSQL container quadlet and tested data persistence and snapshot rollbacks, demonstrating the effectiveness and power of ZFS snapshots and rollbacks for data protection and instant recovery.

Hopefully by now you've started to understand why a separate volume quadlet is a good thing. It allows you to manage the lifecycle of persistent storage independently from the container, share volumes between multiple containers, and leverage advanced features like ZFS snapshots for data protection and recovery. It is also a good security practice. Although we could also do this inside the container quadlet directly, separating concerns into dedicated quadlets leads to cleaner, more secure, and more maintainable configurations.
