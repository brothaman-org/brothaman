# Volumes

In this lab we create bind mounts of existing directories in dot.container quadlet descriptors so they're available in the container. We also create persistent volumes and access them in dot.volume descriptors tying them into container descriptors. We also use ZFS and/or BTRFS to demonstrate how they can be used to archive / rollback changes.

1. Serve Brothaman's docs adding two `Volume=` directives to test-quadlet.container's [Container] section
2. Create a ZFS database volume Quadlet for postgresql
3. Create a postgresql container using the persistent ZFS based volume
4. Enable automated ZFS snapshots on the volume dataset (with zfs delegation)
5. Enable automated ZFS mount and unmount on container start/stop with zfs-helper
6. Create a volume snapshot policy manager service

## 0. Snapshot

Since this test runs inside a Vagrant VM, capture a baseline snapshot before making any changes. From your host machine (in the same directory as your Vagrantfile):

```shell
if ! vagrant snapshot list 2>&1 | grep before-volumes >/dev/null; then
  vagrant snapshot save before-volumes;
else
  echo before-volumes snapshot already exists;
fi
```

This allows you to roll the VM back to its exact pre-test state later.

## 1. Serve existing mkdocs directory with a nginx container

In the last lab we created a simple nginx based container quadlet. By default without any configuration it serves a default test page. Let's go one step further and have it serve the brothaman website.

If you look into the Vagrantfile an rsync command is used to copy the local `docs` directory into the VM at `/brothaman` with your lingeruser's uid:

```ruby
  config.vm.synced_folder "docs", "/brothaman",
    type: "rsync",
    create: true,
    owner: 1001,
    group: 1001,
    rsync__chown: true,
    rsync__auto: false,
    rsync__args: [
      "--verbose",
      "--archive",
    ]
```

To have nginx serve this content, a virtual server configuration needs to be passed telling it how and from where to serve the content. The following nginx server configuration tells nginx to use /var/www/brothaman as its server root:

```ini
mkdir ~/nginx-conf.d/
cat > ~/nginx-conf.d/brothaman.org.conf <<'EOF'
server {
    listen 80;
    server_name brothaman.org www.brothaman.org;

    # put your site files here (we just made it above)
    root /var/www/brothaman;
    index index.html index.htm;

    # log to container stdout/stderr (best for containers)
    access_log /dev/stdout;
    error_log  /dev/stderr warn;

    location / {
        try_files $uri $uri/ =404;
    }
}
EOF
```

The container author configured nginx to look for server configurations in `/etc/nginx/conf.d/`. Our configuration needs to reside there to get picked up at startup. Instead of copying the file into the container image, we can use a bind mount to map our configuration directory onto the container at `/etc/nginx/conf.d/` the quadlet descriptor, `Volume=/home/lingeruser/nginx-conf.d:/etc/nginx/conf.d`. Another `Volume=` mapping, `Volume=/brothaman:/var/www/brothaman`, also directly within the test-quadlet's `[Container]` section. Together nginx can then serve this directory as using the brothaman.org virtual server:

```bash
mkdir -p ~/.config/containers/systemd/
cat > ~/.config/containers/systemd/test-quadlet.container <<'EOF'
[Unit]
Description=Quadlet Test Container

[Container]
Image=nginx:alpine
Exec=nginx -g 'daemon off;'
ContainerName=test-quadlet
PublishPort=8080:80
Volume=/brothaman:/var/www/brothaman
Volume=/home/lingeruser/nginx-conf.d:/etc/nginx/conf.d

[Service]
Restart=always
TimeoutStartSec=300
StartLimitIntervalSec=60s
StartLimitBurst=5
RestartSec=10s

[Install]
WantedBy=default.target
EOF
```

Before starting the container notice no separate volume quadlet was needed: meaning we did not create a `dot.volume` file. Both directives were put into the container quadlet's `[Container]` section directly. There's only one service, the test-quadlet container service, and it can now serve the mkdocs content. Reload systemd and start the container:

```bash
systemctl --user daemon-reload
systemctl --user restart test-quadlet.service
curl "http://$(ip route | grep 'kernel scope link' | awk '{print $9}'):8080"
```

You should see the bro site contents. You can point your browser to the VM's IP address at port 8080 to see it as well. Don't worry you don't have to set up DNS for brothaman.org, just accessing the IP:8080 will work fine. With one virtual server configuration nginx just uses it as the default site.

## 2. What's the gained by creating a volume quadlet?

The example above is so simple, no separate volume quadlet was needed. However, for more complex scenarios where you:

* share the volume with multiple containers, or
* you want to manage the lifecycle of the volume separately from the container, or
* you want to use advanced volume features like ZFS snapshots

It's beneficial to create a dedicated volume quadlet. In this lab section, the concept will be put to the test so we can see how it works and helps.

### 2.1 Create a ZFS backed volume quadlet for PostgreSQL

Let's consider a database container like PostgreSQL which needs persistent storage for its data. Before the container starts, we want to take a snapshot of the data. In case anything goes wrong during the container's operation, we can roll back to the previous snapshot. In such cases, a dedicated volume quadlet is the way to go to isolate the functionality.

Let's use a ZFS-backed volume to take snapshots of the data volume between container restarts. We start by creating a ZFS dataset for PostgreSQL data storage and set it to mount at `/home/lingeruser/postgres`. Then we create a volume quadlet to manage this dataset.

Make sure you're logged as the `vagrant` user to escalate privileges to root with `sudo`. Below a ZFS dataset, `testing/postgres`, is created then its mountpoint set to `/home/lingeruser/postgres`: it mounts automatically using the ZFS mount service. The mountpoint's user and group ownership permissions are also set to be owned by lingeruser so lineruser's unprivileged containers can access it.

```bash
sudo zfs create testing/postgres
sudo zfs set mountpoint=/home/lingeruser/postgres testing/postgres
sudo chown lingeruser:lingeruser /home/lingeruser/postgres
sudo -iu lingeruser ls -ld postgres
df -h /home/lingeruser/postgres/
```

The dataset is now ready to be used as a persistent volume for our PostgreSQL container. Now switch back to the `lingeruser` again with `sudo su - lingeruser`:

```bash
mkdir -p ~/.config/containers/systemd/
cat > ~/.config/containers/systemd/postgres-data.volume <<'EOF'
[Unit]
Description=PostgreSQL Volume

[Volume]
VolumeName=postgres-data

[Service]
ExecStartPre=/usr/sbin/zfs snapshot testing/postgres@pre
EOF

systemctl --user daemon-reload
systemctl --user start postgres-data-volume.service
```

>**NOTICE**: The `postgres-data.volume` quadlet is started and stopped using a service name that combines the volume name, the `-volume` suffix, and the `.service` extension. The service name construction pattern is automatically handled by podman when you create a volume quadlet.

### 2.2 Troubleshooting Problems

How did it go for you? Using `journalctl --user -xe -u postgres-data-volume.service`, here are the failure errors in the service journal log:

```shell
Oct 26 14:13:16 debian12.localdomain systemd[904]: Starting postgres-data-volume.service - PostgreSQL Volume...
Oct 26 14:13:16 debian12.localdomain postgres-data-volume[45001]: cannot create snapshots : permission denied
Oct 26 14:13:16 debian12.localdomain systemd[904]: postgres-data-volume.service: Control process exited, code=exited, status=1/FAILURE
Oct 26 14:13:16 debian12.localdomain systemd[904]: postgres-data-volume.service: Failed with result 'exit-code'.
Oct 26 14:13:16 debian12.localdomain systemd[904]: Failed to start postgres-data-volume.service - PostgreSQL Volume.
```

The permission error is expected. The `lingeruser` does not have permission to create snapshots. Try this as `vagrant` to escalate to sudo and delegate snapshot permission to `lingeruser` on the dataset:

```shell
sudo zfs allow lingeruser snapshot testing/postgres
```

Now sudo back to `lingeruser` and start the volume service again:

```shell
systemctl --user start postgres-data-volume.service
systemctl --user status postgres-data-volume.service
zfs list -t snapshot testing/postgres
```

You should see the volume service started successfully this time and a snapshot was created. Amazing! Now try stopping the volume service and starting it again now with:

```shell
systemctl --user status postgres-data-volume.service
systemctl --user start postgres-data-volume.service
```

This fails too because the snapshot with the same name already exists. Feel free to check the error message with `journalctl --user -xe -u postgres-data-volume.service`. Let's create snapshots with unique names using timestamps prefixed to the base name of the snapshot:

```bash
mkdir -p ~/.config/containers/systemd/
cat > ~/.config/containers/systemd/postgres-data.volume <<'EOF'
[Unit]
Description=PostgreSQL Volume

[Volume]
VolumeName=postgres-data

[Service]
ExecStartPre=/usr/sbin/zfs snapshot testing/postgres@pre_$(date +%Y%m%d%H%M%S)
EOF

```

This too fails and the output looks a little cryptic in the logs:

```log
Oct 28 08:28:45 debian12.localdomain systemd[904]: Starting postgres-data-volume.service - PostgreSQL Volume...
Oct 28 08:28:45 debian12.localdomain postgres-data-volume[78548]: usage:
Oct 28 08:28:45 debian12.localdomain postgres-data-volume[78548]:         snapshot [-r] [-o property=value] ... <filesystem|volume>@<snap> ...
Oct 28 08:28:45 debian12.localdomain postgres-data-volume[78548]: For the property list, run: zfs set|get
Oct 28 08:28:45 debian12.localdomain postgres-data-volume[78548]: For the delegated permission list, run: zfs allow|unallow
Oct 28 08:28:45 debian12.localdomain systemd[904]: postgres-data-volume.service: Control process exited, code=exited, status=2/INVALIDARGUMENT
Oct 28 08:28:45 debian12.localdomain systemd[904]: postgres-data-volume.service: Failed with result 'exit-code'.
```

What's going on here? The clue is in the `usage` complaint by the zfs CLI program. It's not able to parse the command line. The problem is that the `$(date +%Y%m%d%H%M%S)` command substitution is not being interpreted by systemd as expected. To fix this, we need to wrap the command substitution in a shell execution context. Update the `ExecStartPre` line as follows and try again:

```bash
mkdir -p ~/.config/containers/systemd/
cat > ~/.config/containers/systemd/postgres-data.volume <<'EOF'
[Unit]
Description=PostgreSQL Volume

[Volume]
VolumeName=postgres-data

[Service]
ExecStartPre=/bin/sh -c '/usr/sbin/zfs snapshot testing/postgres@pre_"$(date +%Y%m%d%H%M%S)"'
EOF

systemctl --user daemon-reload
systemctl --user start postgres-data-volume.service
```

This too fails. Take a look at why in the journal, but there's a bunch of junk generated instead of the date string we wanted to append. That happens because systemd is performing specifier expansion. The '%' and '$' characters have special meanings in systemd unit files. They need to be escaped to be interpreted literally. Update the `ExecStartPre` line again as follows:

```bash
mkdir -p ~/.config/containers/systemd/
cat > ~/.config/containers/systemd/postgres-data.volume <<'EOF'
[Unit]
Description=PostgreSQL Volume

[Volume]
VolumeName=postgres-data

[Service]
ExecStartPre=/bin/sh -c '/usr/sbin/zfs snapshot testing/postgres@pre_"$$(date +%%Y%%m%%d%%H%%M%%S)"'
EOF

systemctl --user daemon-reload
systemctl --user start postgres-data-volume.service
zfs list -t snapshot testing/postgres
```

Now this works! Each time the volume service is started a new snapshot with a unique timestamped name is created. Great! Keep trying it and test by starting and restarting the service. Here let me help you:

```bash
for restart in $(seq 1 3); do
  systemctl --user restart postgres-data-volume.service
  sleep 2
done
zfs list -t snapshot testing/postgres
```

### 2.3 Let's delete the extra snapshots

That's looking good. Multiple snapshots were created with unique names. Perfect! But now we got that `pre` without the prefix let's try deleting that still logged in as the `lingeruser`:

```bash
zfs destroy testing/postgres@pre
```

In fact we cannot delete any snapshot. You should see they all fail due to a lack of permissions. That's because the `lingeruser` does not have permission to destroy snapshots. Let's switch back to `vagrant` and delegate the destroy permission on the dataset to `lingeruser`:

```bash
sudo zfs allow lingeruser destroy,snapshot testing/postgres
```

Now if you then try the destroy on the snapshot again it still does not work. What's going on here? The reason is ZFS supports the delegation of snapshot creation, but not the destruction of snapshots. Even the dataset owner can not destroy snapshots. But try destroy the pre snapshot with the `vagrant` user as root:

```bash
vagrant@debian12:~$ sudo zfs destroy testing/postgres@pre
vagrant@debian12:~$ sudo zfs list -t snapshot testing/postgres
NAME                                  USED  AVAIL     REFER  MOUNTPOINT
testing/postgres@pre-20251028085229     0B      -     40.0M  -
testing/postgres@pre-20251028085533     0B      -     40.0M  -
testing/postgres@pre-20251028085535     0B      -     40.0M  -
testing/postgres@pre-20251028085537     0B      -     40.0M  -
vagrant@debian12:~$
```

## 3. Switching to the zfs-helper for snapshot management

So to manage snapshot deletion, we need to be perform the destroy operation with elevated privileges with the vagrant sudo enabled user. This is a limitation of OpenZFS's delegation model. The whole situation should be pretty irritating to you. Especially if you want to automate snapshot management in unprivileged containers and have to login and out of one user to sudo from another all the time right?

>**WARNING**: What many lazy admins (who you should never hire) often do in this case is make their unprivileged users sudoers without password prompt for zfs commands. Even adding having a sudo password should be forbidden on production systems. Unprivileged users for application sandboxing purposes should **NEVER** have the ability to escalate privileges with or without passwords. WTF is the point anyway right?

Then what if you want out of this login, log out hell, and want some consistency. First off you should be automating the process with DevOps tools, but let me not go there right now. It's still pretty irritating even if you automate things. Plus doing so you still need two different users, one with sudo, and the target non sudoers user contexts to properly manage snapshots. Ugh.

### 3.1 Setup lingeruser to use zfs-helper

Although there are several ways to skin this cat, the best is to use a helper service that runs with elevated privileges and performs zfs management tasks on behalf of unprivileged user-scoped services. Such a service needs a solid policy framework to validate and authorize requests from unprivileged users. Its authorization controls should limit what operations can be performed on which datasets by which users. Luckily we did just that for brothaman and it is called [`zfs-helper`](https://github.com/brothaman-org/zfs-helper). The stock package is already installed in the VM. All we need to do is configure it to allow our user's `postgres-data-volume.service` to manage snapshots on the `testing/postgres` dataset. See the edits added to the zfs-helper configuration files at `/etc/zfs-helper/policy.d/`:

>Perform these operations as the `vagrant` user.

```bash
sudo usermod -a -G zfshelper lingeruser
sudo loginctl kill-user lingeruser
sudo loginctl user-status lingeruser --no-pager
sudo loginctl enable-linger lingeruser
sudo loginctl user-status lingeruser --no-pager
sudo -iu lingeruser id

pushd . || true
sudo mkdir -p /etc/zfs-helper/policy.d/lingeruser
cd /etc/zfs-helper/policy.d/lingeruser
cat <<'EOF' | sudo tee -a units.list
postgres-data-volume.service
EOF

cat <<'EOF' | sudo tee -a mount.list unmount.list snapshot.list rollback.list create.list destroy.list
lingeruser testing/postgres
EOF
popd

```

Now let's use the service to manage snapshots for us. First, update the volume quadlet to use zfs-helper for snapshot creation instead of calling zfs directly. We could do this before directly since we were granted delegation, but let's do it now again to be consistent:

Issue these commands as the `lingeruser`:

```bash
mkdir -p ~/.config/containers/systemd/
cat > ~/.config/containers/systemd/postgres-data.volume <<'EOF'
[Unit]
Description=PostgreSQL Volume

[Volume]
VolumeName=postgres-data

[Service]
ExecStartPre=/bin/sh -c '/usr/bin/zfs-helperctl snapshot testing/postgres@pre_"$$(date +%%Y%%m%%d%%H%%M%%S)"'
EOF

systemctl --user daemon-reload
systemctl --user restart postgres-data-volume.service
zfs list -t snapshot testing/postgres
```

### 3.2 Delete pre-start snapshots above a limit

Now let's enhance the volume quadlet further to delete old snapshots above a certain limit each time the volume service starts. This way we can keep the number of snapshots manageable without manual intervention. Update the volume quadlet as follows to add a snapshot cleanup step before creating a new snapshot:

```bash
mkdir -p ~/.config/containers/systemd/
cat > ~/.config/containers/systemd/postgres-data.volume <<'EOF'
[Unit]
Description=PostgreSQL Volume
PartOf=test-postgresql.service

[Volume]
VolumeName=postgres-data

[Service]
Environment=KEEP_SNAPSHOTS=5
ExecStartPre=/bin/sh -c '/usr/bin/zfs-helperctl snapshot testing/postgres@pre_"$$(date +%%Y%%m%%d%%H%%M%%S)"'
ExecStartPre=/bin/sh -c 'zfs list -t snapshot -o name -s creation | grep -E "^testing/postgres@pre_*" | head -n -$${KEEP_SNAPSHOTS} | xargs -r -n1 zfs-helperctl destroy'
EOF
systemctl --user daemon-reload
systemctl --user restart postgres-data-volume.service
zfs list -t snapshot testing/postgres
```

Now each time the volume service starts, it will delete old pre-start snapshots beyond the specified limit after creating a new snapshot. This keeps the snapshot count manageable automatically. No fancy retention policies manager services or cron jobs needed. Just let the volume service handle it for you. You can verify this by restarting the volume service multiple times and checking the snapshot list:

```bash
for restart in $(seq 1 10); do
  echo "snapshot with $restart"
  systemctl --user restart postgres-data-volume.service
  sleep 3
  zfs list -t snapshot testing/postgres
done
```

## 4. Connecting the volume to the PostgreSQL container quadlet

Finally, let's connect the volume quadlet to the PostgreSQL container quadlet so the container can use the persistent storage with snapshotting and test it out. Update the PostgreSQL container quadlet to include the volume:

```bash
mkdir -p ~/.config/containers/systemd/
cat > ~/.config/containers/systemd/test-postgresql.container <<'EOF'
[Unit]
Description=PostgreSQL Container
PropagatesStopTo=postgres-data-volume.service
BindsTo=postgres-data-volume.service
After=postgres-data-volume.service

[Container]
Image=postgres:16
ContainerName=test-postgresql
Environment=POSTGRES_PASSWORD=password
Volume=/home/lingeruser/postgres:/var/lib/postgresql/data
PublishPort=5432:5432

[Service]
Restart=always
TimeoutStartSec=300
StartLimitBurst=5
RestartSec=10s

[Install]
WantedBy=default.target
EOF

systemctl --user daemon-reload
systemctl --user start test-postgresql.service
```

You can now connect to the PostgreSQL container and verify that the data is being stored in the ZFS-backed volume. You can also test snapshot creation and rollback by restarting the container and checking the snapshots created by the volume service.

```bash
export PGPASSWORD=password
psql -h "$(ip route | grep 'kernel scope link' | awk '{print $9}')" -p 5432 -U postgres -c '\l'
```

You should see the default PostgreSQL databases listed. You can create a new database, insert some data, and then restart the container to see that the data persists across restarts thanks to the ZFS-backed volume.

```bash
export PGPASSWORD=password
psql -h "$(ip route | grep 'kernel scope link' | awk '{print $9}')" -p 5432 -U postgres -c 'CREATE DATABASE testdb;'
psql -h "$(ip route | grep 'kernel scope link' | awk '{print $9}')" -p 5432 -U postgres -d testdb -c 'CREATE TABLE testtable (id SERIAL PRIMARY KEY, name VARCHAR(50));'
psql -h "$(ip route | grep 'kernel scope link' | awk '{print $9}')" -p 5432 -U postgres -d testdb -c "INSERT INTO testtable (name) VALUES ('testname');"
psql -h "$(ip route | grep 'kernel scope link' | awk '{print $9}')" -p 5432 -U postgres -d testdb -c 'SELECT * FROM testtable;'

systemctl --user restart test-postgresql.service
zfs list -t snapshot testing/postgres
sleep 5
psql -h "$(ip route | grep 'kernel scope link' | awk '{print $9}')" -p 5432 -U postgres -d testdb -c 'SELECT * FROM testtable;' 
psql -h "$(ip route | grep 'kernel scope link' | awk '{print $9}')" -p 5432 -U postgres -d testdb -c "INSERT INTO testtable (name) VALUES ('anthony');"
psql -h "$(ip route | grep 'kernel scope link' | awk '{print $9}')" -p 5432 -U postgres -d testdb -c 'SELECT * FROM testtable;' 
systemctl --user restart test-postgresql.service
zfs list -t snapshot testing/postgres | tail -n 1
sleep 5
psql -h "$(ip route | grep 'kernel scope link' | awk '{print $9}')" -p 5432 -U postgres -d testdb -c 'SELECT * FROM testtable;'
```

Let's try reverting to a previous snapshot if needed. After stopping the container and its linked volume (as lingeruser), go back to being vagrant, and identify the snapshot you want to roll back to from the list of snapshots using the vagrant sudo user:

```bash
sudo zfs list -t snapshot testing/postgres
```

Choose a snapshot name from the list (e.g., `testing/postgres@pre_20251028090000`) and perform the rollback:

```bash
sudo zfs rollback testing/postgres@pre_20251028090000
```

Obviously you have to do this on your own since the snapshots are unique with timestamps. After the rollback, restart the PostgreSQL container and verify the data in table or even the presence of the table itself.
