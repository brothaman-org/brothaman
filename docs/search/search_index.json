{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Brothaman Overview","text":"<p>Tools and scripts (so far) that help manage unprivileged rootless Podman containers:</p> <ul> <li>[x] <code>bro-user</code> \u2014 create/remove rootless users (lingering, subuid/gid, per-user storage).</li> <li>[x] <code>bro-volume</code> \u2014 create/remove/manage rootless ZFS-backed Podman volumes for unprivileged containers.</li> <li>[x] <code>bro-activate</code> \u2014 create/remove/manage Podman container systemd socket activation.</li> <li>[x] <code>bro-install-podman</code> \u2014 installs or upgrades podman to latest from Alvistack.</li> <li>[ ] <code>bro-decompose</code> - decomposes a compose file to many quadlets and bro commands with the help of podlet.</li> </ul> <p>Brothaman is a work in progress. More utilities to come!</p> <p>Principles: small scripts, idempotency, rootless-first, ZFS-backed, socket-activated composition, cross-user/host by on demand using sockets instead of orchestrating services.</p> <p>DISCLAIMER: Brothaman is a personal project and is not affiliated with or endorsed by the Podman project or its maintainers. The code and labs were enhanced using AI. Use at your own risk.</p>"},{"location":"#why-brothaman","title":"Why brothaman?","text":"<p>Honestly, I never intended to create a project like this. I started writing up a lab series to teach people how to use Podman quadlets securely with systemd for unprivileged rootless containers. But as I wrote the labs, I realized there were a lot of repetitive tasks and boilerplate configurations that could be automated. So I created some scripts to help with that, and before I knew it, I had a whole suite of tools that could make managing rootless containers much easier and more securely without all the rote manual labor and human error.</p> <p>And because sometimes everyone needs a good brothaman to help carry the burden.</p> <p>I truly hope that brothaman can helps others save time and avoid the pitfalls and challenges I faced when learning to use Podman Quadlets with systemd and my own personal system security patterns. By providing a set of scripts that automate common tasks and enforce best practices, I believe we can make rootless container management more accessible and secure for everyone.</p> <p>In the end, I would love for brothaman to go away, and become obsolete because Podman natively supports all these utilities out of the box. But until then, I hope brothaman can serve as a useful tool for anyone looking to run rootless containers securely and efficiently.</p>"},{"location":"architecture/","title":"Architecture Overview","text":"<p>Administration best practices + systemd primitives + podman features</p> <ul> <li>secure container services and applications</li> <li>one minimal authoritative configuration</li> <li>fine control over noisy neighbors, renegades, and compromised containers</li> <li>elegantly managed cross-over container dependencies</li> <li>higher performance and more capabilities</li> </ul> <p>Brothaman uses a combination of system administration best practices with systemd primitives and Podman features to elegantly simplify, secure, and manage containerized services and compositions of dependent containers via docker-compose descriptor files.</p> <ul> <li>unprivileged podman</li> <li> <p>quadlet-patterns</p> </li> <li> <p>Quadlet based authoritative service configuration in systemd --user scopes</p> </li> <li>ZFS using fuse-overlayfs</li> <li>Systemd proxyd</li> <li>Systemd socket</li> <li>Fast networking with Pasta</li> </ul> <p>The best aspect differentiating Podman is its relentless pursuit to work harmoniously with systemd at the operating system level. Podman users easily hook containers into systemd as services using its <code>generate systemd</code> sub-command. Now there are even more powerful Quadlets.</p> <p>The plethora of systemd features makes it hard to notice glorious ways to make amazing things happen.</p> <p>Thankfully, systemd harmony is even better after this command was deprecated in favor of Quadlet. Unfortunately, many are reluctant to upgrade.</p> <p>Brothaman builds on systemd and Quadlet to run rootless containers that are activated by traffic.</p> <ol> <li>A <code>.socket</code> unit listens on a host port (or specific interfaces).</li> <li>When a client connects, <code>systemd-socket-proxyd</code> (<code>-Service</code>) forwards to a loopback internal port.</li> <li>The proxied connection activates the container via a Quadlet <code>.container</code> unit.</li> <li>No <code>-p</code> port mapping is used; the proxy handles external exposure.</li> </ol> <p>This pattern composes across users and hosts: a client of a service uses it and activation follows demand.</p>"},{"location":"compose-conversion/","title":"Compose Conversion","text":"<p>WIP: Need to look at how we will deal with all aspects of docker-compose files. We need to look at how various entities in the compose file are best converted into Quadlet files (i.e. volumes).</p> <p><code>bro-compose</code> reads <code>docker-compose.yml</code> and synthesizes per-service calls to <code>bro-service</code> and potentially other new Brothaman commands:</p> <ul> <li>Each published port becomes a <code>.socket</code> + proxyd pair</li> <li>Containers bind to container interface ports accessible to the host and proxyd</li> <li>Dependencies prefer usage (client connects to server socket) rather than orchestration</li> <li>Optional service\u2192user mapping allows separation of privileges</li> </ul>"},{"location":"development/","title":"Development Guide","text":"<p>This guide covers development practices, conventions, and contribution guidelines for the brothaman project.</p>"},{"location":"development/#naming-conventions","title":"Naming Conventions","text":"<p>Brothaman follows a consistent dual naming convention across all components:</p>"},{"location":"development/#package-names","title":"Package Names","text":"<p>All Debian packages use the full prefix <code>brothaman-*</code>:</p> <ul> <li><code>brothaman-helper</code> - Network namespace helper utility</li> <li><code>brothaman-compose</code> - Container compose management</li> <li><code>brothaman-service</code> - Service lifecycle management  </li> <li><code>brothaman-install-deps</code> - Dependency installation utilities</li> <li><code>brothaman-zfs</code> - ZFS management tools</li> </ul> <p>Rationale: Full package names provide clear identification in APT repositories and package listings.</p>"},{"location":"development/#command-names","title":"Command Names","text":"<p>All executable commands use the short prefix <code>bro-*</code>:</p> <ul> <li><code>bro-helper</code> - Network namespace helper</li> <li><code>bro-compose</code> - Container compose management</li> <li><code>bro-service</code> - Service lifecycle management</li> <li><code>bro-install-deps</code> - Install system dependencies</li> <li><code>bro-install-zfs</code> - Install ZFS utilities</li> </ul> <p>Rationale: Short command names are convenient for daily terminal usage and tab completion.</p>"},{"location":"development/#examples","title":"Examples","text":"<pre><code># Install the package\nsudo apt install brothaman-helper\n\n# Use the command\nbro-helper --netns /run/user/1000/netns/podman -- curl ifconfig.me\n\n# Search for all brothaman packages\napt search brothaman-*\n\n# Tab complete all bro commands  \nbro-&lt;TAB&gt;&lt;TAB&gt;\n</code></pre>"},{"location":"development/#package-structure","title":"Package Structure","text":"<p>Each brothaman component follows this standard Debian package structure:</p> <pre><code>pkgs/brothaman-&lt;name&gt;/\n\u251c\u2500\u2500 DEBIAN/\n\u2502   \u251c\u2500\u2500 control          # Package metadata\n\u2502   \u251c\u2500\u2500 postinst         # Post-installation script\n\u2502   \u2514\u2500\u2500 prerm            # Pre-removal script (if needed)\n\u251c\u2500\u2500 usr/local/bin/       # Executable binaries\n\u2502   \u2514\u2500\u2500 bro-&lt;name&gt;       # Main executable\n\u251c\u2500\u2500 src/                 # Source code (for compiled tools)\n\u2502   \u251c\u2500\u2500 Makefile         # Build configuration\n\u2502   \u2514\u2500\u2500 *.c/*.go/etc     # Source files\n\u2514\u2500\u2500 build.sh             # Build script (optional)\n</code></pre>"},{"location":"development/#build-process","title":"Build Process","text":"<p>Packages are built using the containerized build system:</p> <pre><code># Build all packages\n./scripts/build.sh\n\n# Build specific components\n./scripts/mkdebs.sh\n\n# Generate documentation\n./scripts/mkdocs.sh\n\n# Create repository\n./scripts/mkrepo.sh\n</code></pre>"},{"location":"development/#development-workflow","title":"Development Workflow","text":"<ol> <li>Create Package Structure: Add new package to <code>pkgs/brothaman-&lt;name&gt;/</code></li> <li>Add DEBIAN Control Files: Define package metadata and dependencies  </li> <li>Implement Functionality: Add source code and build configuration</li> <li>Update Documentation: Add or update relevant markdown files</li> <li>Test Build: Run <code>./scripts/build.sh</code> to verify package creation</li> <li>Update Navigation: Add new docs to <code>mkdocs.yml</code> navigation</li> </ol>"},{"location":"development/#code-standards","title":"Code Standards","text":"<ul> <li>Shell Scripts: Follow bash best practices with <code>set -euo pipefail</code></li> <li>C Code: Use standard GNU C with appropriate compiler warnings</li> <li>Go Code: Follow standard Go formatting and conventions</li> <li>Documentation: Use clear, concise markdown with examples</li> </ul>"},{"location":"development/#security-considerations","title":"Security Considerations","text":"<ul> <li>All network namespace operations validate ownership</li> <li>Capabilities are set via postinst scripts, not SUID binaries</li> <li>Commands use allowlists for permitted operations</li> <li>ZFS operations respect user permissions and quotas</li> </ul>"},{"location":"development/#testing","title":"Testing","text":"<ul> <li>Unit tests in <code>tests/</code> directory</li> <li>Integration tests via Vagrant environments  </li> <li>Package installation testing in clean containers</li> <li>Documentation validation via MkDocs build</li> </ul>"},{"location":"development/#contributing","title":"Contributing","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch: <code>git checkout -b feature/new-tool</code></li> <li>Follow naming conventions for packages and commands</li> <li>Add tests and documentation</li> <li>Submit a pull request</li> </ol>"},{"location":"development/#release-process","title":"Release Process","text":"<ol> <li>Update version numbers in package control files</li> <li>Run full build: <code>./scripts/build.sh</code> </li> <li>Sign repository: <code>./scripts/signrepo.sh</code></li> <li>Publish changes: <code>./scripts/publish.sh</code></li> <li>Create release: <code>./scripts/release.sh</code></li> </ol>"},{"location":"dictionary/","title":"Dictionary","text":"<ul> <li>Bimodal script: A wrapper that runs an Ansible play by default and a shell implementation with <code>--script</code>.</li> <li>Lingering: Systemd capability allowing user units to run without an interactive login.</li> <li>Quadlet: Systemd generator that converts <code>.container</code> / <code>.volume</code> files into native units for Podman.</li> <li>Socket activation: Starting services upon first connection to a <code>.socket</code> listener.</li> <li>systemd-socket-proxyd: Small proxy service that forwards accepted sockets to a target <code>IP:PORT</code>.</li> <li>Graphroot: Podman\u2019s content store path; per-user when running rootless.</li> <li>User Scope: Refers to systemd services running the same system account: these services have user scope and can be ordered with respect to one another using dependency directives</li> <li>Unprivileged account</li> <li>Unprivileged user</li> <li>Unprivileged podman</li> <li>Non-root account</li> <li>OS account</li> </ul>"},{"location":"directions/","title":"Directions","text":"<ol> <li> <p>Create a <code>${major}.${minor}.${micro}</code> component based version comparison function in the common library to compare version numbers. It takes two version arguments. If the first is higher and more recent it returns 1. If the second is higher and more recent it returns -1. If both are the same it returns 0. WARNING: make sure comparisons are not lexical but numeric value based on all components. This will be used to compare installed versions against what is available using the ALVISTACK_VERSION.</p> </li> <li> <p>Always favor of the <code>alvistack</code> to install from OpenSuse instead of the distributions packaged podman. The <code>bro-install</code> installer script (which also uninstalls) first checks if the <code>alvistack</code> package repository is setup with its keys. If not, any existing podman is removed and the repository is installed. If the repository is already setup, the installer script checks if podman is installed, and if so, the installer script checks the installed version to see if it is above or equal to ALVISTACK_VERSION. If the installed version is lower, it is removed and podman is reinstalled from the <code>alvistack</code> repository.</p> </li> <li> <p>The use of CoW filesystems for backing stores will be optional and an independent axis of operation. The script uses whatever it finds (ZFS, BTRFS, or none). Right now in the last <code>rootless-podman</code> script, if ZFS is not present, no backing store is used for user homes under the USER_HOME_BASE which is a good fallback behavior. Without CoW backing stores, users miss out on leveraging snapshotting features for various operations, but things still work.</p> </li> </ol> <p>NOTE: it turns out, the quadlet generated by chatgpt for the postgres container is faulty and not the podman installation. I tried a minimal quadlet and it worked. Let us incorporate that into the training and progression towards the final brothaman configuration.</p>"},{"location":"project-plan/","title":"Project Plan","text":""},{"location":"project-plan/#goals","title":"Goals","text":"<ul> <li>Modularize rootless Podman + ZFS workflow into small, idempotent tools.</li> <li>Prefer systemd primitives (socket activation + proxyd) over container port publishing.</li> <li>Ensure each script maps 1:1 to an Ansible role (default mode), with a fallback shell mode via <code>--script</code>.</li> </ul>"},{"location":"project-plan/#work-breakdown-structure","title":"Work Breakdown Structure","text":"<ol> <li>ZFS Installation \u2014 <code>bro-install-zfs</code></li> <li>Test Zpool \u2014 <code>bro-test-zpool</code></li> <li>Dependencies \u2014 <code>bro-install-deps</code></li> <li>User Management \u2014 <code>bro-user</code></li> <li>Service Generator \u2014 <code>bro-service</code></li> <li>Compose Converter \u2014 <code>bro-compose</code></li> <li>Doctor \u2014 <code>bro-doctor</code></li> <li>Documentation &amp; Diagrams \u2014 mkdocs site, mermaid diagrams</li> </ol>"},{"location":"project-plan/#deliverables","title":"Deliverables","text":"<ul> <li>Executable <code>bro-*</code> wrappers (bimodal: Ansible default, <code>--script</code> for shell fallback).</li> <li>Ansible collection skeleton: <code>akarasulu.brothaman</code> with roles matching each script.</li> <li>MkDocs site: this plan, architecture, how-tos, and a project dictionary.</li> <li>Example Vagrant environment for Debian 12 smoke testing.</li> </ul>"},{"location":"project-plan/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li>All scripts are idempotent and safe to re-run.</li> <li>Socket-activated services start on demand via <code>systemd-socket-proxyd</code>.</li> <li><code>bro-compose</code> generates deterministic plans and compatible service units.</li> <li>Documentation is sufficient to onboard a new operator using only this site.</li> </ul>"},{"location":"project-plan/#future-extensions","title":"Future Extensions","text":"<ul> <li>Cross-host activation cookbook.</li> <li>Secrets and TLS patterns.</li> <li>CI smoke tests for Vagrant-based runs.</li> </ul>"},{"location":"quadlet-patterns/","title":"Quadlet Patterns","text":"<p>Common fields we will generate: - <code>Image=</code> to select the container image - <code>EnvironmentFile=</code> and <code>Volume=</code> entries - <code>ContainerName=</code> for stable names - <code>Exec=</code> or <code>Args=</code> for explicit command overrides - <code>After=</code>/<code>Requires=</code> for optional intra-user ordering</p>"},{"location":"socket-activation/","title":"Socket Activation","text":"<ul> <li><code>NAME.socket</code>: Listens on <code>IP:PORT</code> in the user systemd scope.</li> <li><code>NAME-proxy.service</code>: Runs <code>systemd-socket-proxyd</code>, forwarding accepted fds to <code>127.0.0.1:INTERNAL_PORT</code>.</li> <li><code>NAME.container</code>: Quadlet container that binds only to loopback on the internal port.</li> </ul> <p>Flow: Client \u2192 <code>NAME.socket</code> \u2192 <code>NAME-proxy.service</code> \u2192 container <code>127.0.0.1:INTERNAL_PORT</code>.</p>"},{"location":"unprivileged-podman/","title":"Unprivileged Podman","text":"<p>Unprivileged Podman refers to the use of Podman by restricted non-root OS accounts rather than the privileged root user. Podman commands and containers run as non-root system account processes while using bounded resources with quota restricted limits specifically assigned to the unprivileged account; i.e. cpu, memory and disk quotas.</p> <p>Brothaman creates Podman containers and composable container applications on isolated unprivileged OS accounts.</p>"},{"location":"unprivileged-podman/#security-pattern","title":"Security Pattern","text":"<p>Running applications and services pulled from repositories on the Internet in restricted jails is a sensible security pattern right? You don't want to run other people's shit on your shit as root. Each container adds to the overall attack surface in their own [un]predictable ways.</p> <p>Running containers (or applications composed of containers) in restricted environments minimizes the blast radius on compromise. Even without compromise, noisy neighbors or renegade containers MUST BE limited and throttled to functionally protect system resources. With compromise, the account, its processes, and resources need to be frozen and quarantined. Perhaps snapshotted and archived before being immediately destroyed for post-mortem forensic analysis.</p> <p>See zfs</p>"},{"location":"unprivileged-podman/#storage-concerns","title":"Storage Concerns","text":"<p>Unprivileged means much more than just jailed service and application processes with their CPU and memory quotas. It includes all resources with storage being the most critical of them all. After all, the most constraining resources on most systems is almost always storage. Even older CPU's with slower clocks and memory buses can almost always saturate storage I/O even with storage technologies 10-years into the future.</p> <p>Storage is the key resource needing the most protection, yet we often protect it least of all. Brothaman forces the use of ZFS and cgroup v2 blkio limits to cover your back. ZFS as a copy-on-write (CoW) filesystem with capacity quotas makes snapshotting, capacity limiting, and quarantining of containerized services and applications a cake walk.</p>"},{"location":"user-scope/","title":"User Scope","text":""},{"location":"users-and-lingering/","title":"Users &amp; Lingering","text":"<p>Rootless services run in user systemd scope. To run without an active login: - Enable lingering: <code>loginctl enable-linger USER</code></p> <p>Per-user containers config (<code>~/.config/containers/storage.conf</code>): <pre><code>[storage]\ndriver = \"overlay\"\nrunroot = \"/run/user/UID\"\ngraphroot = \"/var/lib/containers/unprivileged/USER\"\n\n[storage.options.overlay]\nmount_program = \"/usr/bin/fuse-overlayfs\"\n</code></pre></p> <p>Subuid/subgid mappings must be present for rootless Podman.</p>"},{"location":"zfs/","title":"ZFS","text":"<p>Brothaman uses ZFS to implement storage I/O throttling of unprivileged container services. As the premier copy-on-write (CoW) file system with rapid low cost snapshotting and rollback ZFS is also ideal for snapshotting, quarantining and archiving compromised containers for forensic analysis.</p>"},{"location":"zfs/#unprivileged-with-zfs","title":"Unprivileged with ZFS","text":"<p>Podman's ZFS driver uses ZFS snapshots and mountpoints for container layering, but it is not stable and only supports privileged Podman. ZFS delegation just does not provide unprivileged users with the key mounting privileges needed. Once it does Brothaman can reconsider using it as the main driver. Don't hold your breath on it though; Linux might never let ZFS do this as long as it keeps one global mount space.</p>"},{"location":"zfs/#fuse-overlayfs-on-zfs","title":"FUSE Overlayfs on ZFS","text":"<p>Brothaman DOES NOT use ZFS for image layer management but it does use it as the underlying backing store for unprivileged users. Brothaman uses the Overlayfs driver instead of the default vfs driver to greatly improve performance and reduce the overhead of layer storage. To do this in unprivileged environments requires the use of the <code>fuse-overlayfs</code> program as the <code>mount_program</code> (and <code>mountopt = 'nodev'</code>) within the unprivileged account's storage configuration file at <code>${USER_HOME}/.config/containers/storage.conf</code>.</p>"},{"location":"zfs/#zfs-dataset-settings","title":"ZFS Dataset Settings","text":"<p>Each unprivileged user's home directory is mounted using a new dedicated ZFS data set. That dataset is configured with the following attributed values:</p> <ul> <li><code>xattr=sa</code></li> <li><code>acltype=posixacl</code></li> <li><code>aclinherit=passthrough</code></li> <li><code>aclmode=passthrough</code></li> <li><code>mountpoint=\"${USER_HOME}\"</code></li> <li><code>compression=zstd</code></li> <li><code>atime=off</code></li> <li><code>recordsize=128</code></li> </ul>"},{"location":"zfs/#brothaman-tools","title":"Brothaman Tools","text":"<ul> <li><code>bro-install-zfs</code> installs ZFS on Debian 12 (no pools).</li> <li><code>bro-test-zpool</code> creates a file-backed pool for development (default name <code>brotest</code>).</li> </ul> <p>Recommended dataset for per-user graphroots: * <code>POOL/containers/USER</code> mounted at <code>/var/lib/containers/unprivileged/USER</code> * Suggested props: <code>compression=zstd</code>, <code>atime=off</code></p>"},{"location":"architecture/","title":"Architecture Overview","text":"<p>This section covers the architectural design and patterns used in Brothaman for containerized service management.</p>"},{"location":"architecture/#key-concepts","title":"Key Concepts","text":"<ul> <li>Socket Activation: How systemd socket activation enables on-demand service startup</li> <li>Quadlet Patterns: Standardized container deployment patterns using Podman Quadlets  </li> <li>System Design: Overall architecture and component interactions</li> </ul>"},{"location":"architecture/#documentation","title":"Documentation","text":"<ul> <li>System Architecture - Core architectural principles and design</li> <li>Socket Activation - systemd socket activation implementation</li> <li>Quadlet Patterns - Container deployment patterns</li> </ul>"},{"location":"architecture/#visual-diagrams","title":"Visual Diagrams","text":"<ul> <li>System Architecture Diagram - Overall system architecture</li> <li>Service Pattern Diagram - Per-service deployment patterns</li> </ul>"},{"location":"architecture/#design-principles","title":"Design Principles","text":"<ol> <li>Security First: Unprivileged containers with proper isolation</li> <li>SystemD Integration: Native systemd service management  </li> <li>ZFS Storage: Advanced storage features with snapshots</li> <li>Declarative Configuration: Infrastructure as Code approach</li> </ol>"},{"location":"components/","title":"Components Overview","text":"<p>Brothaman consists of several key components that work together to provide secure, manageable containerized services.</p>"},{"location":"components/#core-components","title":"Core Components","text":""},{"location":"components/#runtime-execution","title":"Runtime &amp; Execution","text":"<ul> <li>Network Helper - Network namespace management and socket activation</li> <li>Container Runtime - Rootless Podman configuration and management</li> <li>User Management - User session and service scope management</li> </ul>"},{"location":"components/#storage-persistence","title":"Storage &amp; Persistence","text":"<ul> <li>ZFS Backend - Advanced storage features with snapshots and delegation</li> <li>User Sessions - SystemD user session management</li> </ul>"},{"location":"components/#component-interactions","title":"Component Interactions","text":"<pre><code>graph TD\n    A[User Request] --&gt; B[bro-helper]\n    B --&gt; C[Network Namespace]\n    B --&gt; D[Socket Activation]\n    D --&gt; E[Container Service]\n    E --&gt; F[ZFS Volume]\n    F --&gt; G[Persistent Data]\n\n    H[SystemD] --&gt; I[User Session]\n    I --&gt; E\n    I --&gt; J[Service Lingering]</code></pre>"},{"location":"components/#security-model","title":"Security Model","text":"<p>All components operate under the principle of least privilege: - Containers run as unprivileged users - ZFS operations use delegated permissions - Network access is carefully controlled - SystemD manages service lifecycles</p>"},{"location":"diagrams/architecture/","title":"System Architecture Diagram","text":"<p>This diagram shows the overall system architecture for Brothaman's socket-activated container services.</p> <pre><code>graph LR\nClient((Client)) --&gt;|TCP| NAME_socket[NAME.socket]\nNAME_socket --&gt; NAME_proxy[NAME-proxy.service (systemd-socket-proxyd)]\nNAME_proxy --&gt;|forward to 127.0.0.1:INTERNAL_PORT| NAME_container[NAME.container (Quadlet)]</code></pre>"},{"location":"diagrams/architecture/#architecture-components","title":"Architecture Components","text":"<ul> <li>Client: External client connecting to the service</li> <li>NAME.socket: systemd socket unit that listens for incoming connections</li> <li>NAME-proxy.service: systemd-socket-proxyd service that forwards connections</li> <li>NAME.container: Podman Quadlet container running the actual service</li> </ul>"},{"location":"diagrams/architecture/#flow-description","title":"Flow Description","text":"<ol> <li>Client connects to the external port managed by the socket unit</li> <li>systemd activates the proxy service when a connection is received</li> <li>The proxy forwards the connection to the container's internal port</li> <li>The container handles the actual service logic</li> </ol> <p>This architecture enables: - Socket activation: Services start only when needed - Resource efficiency: Containers don't run when idle - Security: Internal ports are not directly exposed - Systemd integration: Full lifecycle management through systemd</p>"},{"location":"diagrams/per-service-pattern/","title":"Per-Service Pattern Diagram","text":"<p>This diagram illustrates the per-service pattern used in Brothaman for socket-activated container services.</p> <pre><code>flowchart TD\nA[NAME.socket] --&gt; B[NAME-proxy.service]\nB --&gt; C[NAME.container]\nC --&gt;|binds| D[127.0.0.1:INTERNAL_PORT]</code></pre>"},{"location":"diagrams/per-service-pattern/#service-pattern-components","title":"Service Pattern Components","text":"<ul> <li>NAME.socket: Socket unit that defines the listening address and port</li> <li>NAME-proxy.service: Proxy service using systemd-socket-proxyd</li> <li>NAME.container: Quadlet container descriptor</li> <li>127.0.0.1:INTERNAL_PORT: Container's internal service port</li> </ul>"},{"location":"diagrams/per-service-pattern/#pattern-benefits","title":"Pattern Benefits","text":"<p>This standardized pattern provides:</p>"},{"location":"diagrams/per-service-pattern/#socket-activation","title":"Socket Activation","text":"<ul> <li>Services start automatically when clients connect</li> <li>Zero resource usage when idle</li> <li>Fast startup times for lightweight containers</li> </ul>"},{"location":"diagrams/per-service-pattern/#security-isolation","title":"Security Isolation","text":"<ul> <li>Containers bind only to localhost</li> <li>External access controlled through systemd socket units</li> <li>No direct container port exposure</li> </ul>"},{"location":"diagrams/per-service-pattern/#systemd-integration","title":"Systemd Integration","text":"<ul> <li>Full lifecycle management</li> <li>Logging integration via journald</li> <li>Service dependencies and ordering</li> <li>Resource limits and controls</li> </ul>"},{"location":"diagrams/per-service-pattern/#scalability","title":"Scalability","text":"<ul> <li>Each service follows the same pattern</li> <li>Easy to replicate for new services</li> <li>Consistent configuration across services</li> </ul>"},{"location":"diagrams/per-service-pattern/#implementation-example","title":"Implementation Example","text":"<p>For a service named <code>myapp</code>: - <code>myapp.socket</code> - listens on external port (e.g., 8080) - <code>myapp-proxy.service</code> - forwards to localhost:3000 - <code>myapp.container</code> - runs container with internal port 3000</p>"},{"location":"labs/","title":"Brothaman Labs","text":"<p>Welcome to the Brothaman hands-on laboratory series! These labs provide step-by-step tutorials for learning rootless container management with Podman Quadlets and systemd.</p>"},{"location":"labs/#purpose","title":"Purpose","text":"<p>The entire point to unprivileged rootless containers is to improve security by isolating container processes from the host system and other users. Doing so reduces the attack surface and limits potential damage from compromised containers to reduce the blast radius.</p> <p>Podman's Quadlet integration with systemd is revolutionary and allows us to manage these containers as first class systemd services, leveraging systemd's capabilities for service management, security, logging, and dependency handling. Podman features and different Quadlet types provide powerful constructs but knowing when and how to use them properly is key. These labs guide you through the essential concepts and practical steps needed to effectively use Podman Quadlets for managing rootless containers with a strong emphasis on security best practices.</p> <p>As we progress together, we will consider challenging scenarios. You may feel this is more a security lab rather than a container management one using Podman Quadlets. This is intentional. We have to consider real scenarios because understanding the security implications of running containers is crucial for any system administrator or developer working with containerized applications. We don't mind doing so since you will be able to reuse the knowledge and approaches gained to batten down the hatches in your own environments. Furthermore, by addressing these challenges, you'll gain a deeper understanding of how to securely manage rootless containers using Podman quadlets and systemd which is the whole point.</p>"},{"location":"labs/#lab-structure","title":"Lab Structure","text":"<p>The labs are organized into three main areas:</p>"},{"location":"labs/#foundation-labs","title":"Foundation Labs","text":"<p>Essential concepts and setup procedures: - Setup Environment - Configure your test environment with Vagrant - User Lingering - Understand systemd user services and lingering - Quadlet Basics - Learn basic Quadlet service descriptors</p>"},{"location":"labs/#storage-data-labs","title":"Storage &amp; Data Labs","text":"<p>Storage management and persistence: - Volumes &amp; ZFS - Bind mounts, persistent volumes, and ZFS snapshots - Debugging - Container troubleshooting and log analysis</p>"},{"location":"labs/#networking-services-labs","title":"Networking &amp; Services Labs","text":"<p>Advanced networking and service composition: - Network Configuration - Container networking and connectivity - Database Admin - PostgreSQL with risky pgAdmin application setup - Pod Management - Multi-container pod orchestration - Proxy Testing - Load balancing and proxy configuration</p>"},{"location":"labs/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic familiarity with Linux command line</li> <li>A cursory understanding of containers (Docker/Podman)</li> <li>Access to a Linux system with systemd</li> <li>Vagrant and KVM hypervisor installed (for VM-based labs)</li> </ul>"},{"location":"labs/#getting-started","title":"Getting Started","text":"<p>Begin with the Setup Environment lab to configure your testing environment, then proceed through the Foundation labs before moving to more advanced topics.</p> <p>Each lab includes: - Clear learning objectives - Step-by-step instructions - Troubleshooting guidance - Cleanup procedures - VM snapshot management</p> <p>Ready to begin? Start with Setup Environment!</p>"},{"location":"labs/0.%20setup/","title":"Lab Setup","text":"<p>These labs are designed to be run inside a virtual machine (VM) created using Vagrant and KVM/libvirt. This ensures a consistent environment for all users and avoids the need to install dependencies directly on your host system.</p>"},{"location":"labs/0.%20setup/#install-kvm-hypervisor","title":"Install KVM Hypervisor","text":"<p>You want to install vagrant and the libvirt provider on a preferably Debian-based Linux host hypervisor system:</p> <pre><code>sudo apt-get install -y git vagrant vagrant-libvirt \\\n  libvirt-daemon-system libvirt-clients virtinst virt-manager \\\n  qemu-kvm qemu-utils\n</code></pre> <p>If on other, non-debian package manager, based systems, please refer to the respective package manager documentation and repository information for installing analogs of these packages. And yes you could probably get by using other hypervisors supported by Vagrant such as VirtualBox even on other operating systems, but we strongly recommend using Linux.</p>"},{"location":"labs/0.%20setup/#clone-and-build-the-brothaman-z12-box","title":"Clone and build the brothaman-z12-box","text":"<p>A one time build of the brothaman/z12 (Debian 12 -- Bookworm with ZFS) vagrant box is required before first use:</p> <pre><code>git clone https://github.com/brothaman-org/brothaman-z12-box\ncd brothaman-z12-box\n./build.sh\n</code></pre>"},{"location":"labs/0.%20setup/#clone-and-fire-up-the-brothaman-lab-vm","title":"Clone and fire up the brothaman lab VM","text":"<p>Now we clone the brothaman repo and fire up the brothaman lab VM:</p> <pre><code>git clone https://github.com/brothaman-org/brothaman\ncd brothaman\nvagrant up &amp;&amp; vagrant snapshot save before-starting\n</code></pre>"},{"location":"labs/1.%20lingering/","title":"Lingering","text":"<p>In this lab we will prove that systemd lingering works when enabled for an unprivileged user and demonstrate its impact on user-scoped services that persist across reboots. You will:</p> <ol> <li>Create an unprivileged lingering user and enable systemd commands</li> <li>Create, install, and start test systemd user-scoped service</li> <li>Reboot with lingering</li> <li>Check if the test service runs after reboot (should be running)</li> <li>Disable lingering and reboot</li> <li>Check if the test service runs after reboot (should NOT be running)</li> <li>Re-enable lingering again and see what happens</li> <li>Lessons learned</li> </ol>"},{"location":"labs/1.%20lingering/#0-snapshot","title":"0. Snapshot","text":"<p>Since this test runs inside a Vagrant VM, capture a baseline snapshot before making any changes. From your host machine (in the same directory as your Vagrantfile):</p> <pre><code>if ! vagrant snapshot list 2&gt;&amp;1 | grep before-starting &gt;/dev/null; then\n    vagrant snapshot save before-starting;\nelse\n    echo before-starting snapshot already exists;\nfi\n</code></pre> <p>This allows you to roll the VM back to its exact pre-test state later.</p>"},{"location":"labs/1.%20lingering/#1-create-an-unprivileged-lingering-user-and-enable-systemd-commands","title":"1. Create an unprivileged lingering user and enable systemd commands","text":"<p>ATTENTION: Make sure to SSH into the vagrant vm with <code>vagrant ssh</code></p> <p>Create a dedicated unprivileged test user:</p> <pre><code># create and check user\nsudo useradd -m -s /bin/bash lingeruser\nid lingeruser\ngetent passwd lingeruser\nsudo -iu lingeruser ls -ld .\n</code></pre> <p>This ensures the user got created with home directory (<code>/home/lingeruser</code>) and we can execute commands as the user. Now let's make the user linger but notice the checks before and after for an instance of systemd running as the lingeruser:</p> <pre><code>echo \"checking if new user's systemd instance is running\"\nsudo ps -ux -U lingeruser | grep 'systemd --user' | grep -v grep\necho no systemd from the ps command \u261d\ufe0f\necho\n\n# make the user linger\nsudo loginctl enable-linger lingeruser\necho \"checking if new user's systemd instance is running\"\nsleep 3\nsudo ps -ux -U lingeruser | grep 'systemd --user' | grep -v grep\necho see systemd now from the ps command \u261d\ufe0f\necho\n\n# check if we can use systemctl\nsudo -iu lingeruser systemctl --user list-units --no-pager\n</code></pre>"},{"location":"labs/1.%20lingering/#whats-going-on-here","title":"What's going on here?","text":"<p>Systemd runs in system mode as root. When \"enabled to\", systemd can also run in user mode with the <code>--user</code> switch. The systemd process actually runs as a specific user. Here in this lab it runs as the lingeruser.</p> <p>If you noticed the lingeruser's systemd process was not present before the <code>loginctl enable-linger lingeruser</code> command was run. Running the command fired up the systemd infrastructure for the lingeruser and the ps command after it showed the user's systemd instance now running. Furthermore, the enable-linger command makes sure, the user's systemd instance is always running, even when the user is not logged in.</p> <p>Other things are also started. Namely a dbus broker for the user's system components bus. System components, including systemd, talk to one another using this messaging system. In fact, systemctl sends commands to the systemd user instance using this bus. However, to attach to the broker and send commands to the systemd user instance it needs to connect to the bus. These environment variables tell it where to find the bus (take a look):</p> <pre><code>echo XDG_RUNTIME_DIR=${XDG_RUNTIME_DIR}\necho DBUS_SESSION_BUS_ADDRESS=${DBUS_SESSION_BUS_ADDRESS}\n</code></pre> <p>The brothaman package being installed automatically sets these environment variables for all unprivileged users by placing a script in <code>/etc/profile.d/xdg_runtime.sh</code>. This script runs at login and sets the variables correctly so that commands like <code>systemctl --user</code> work as expected.</p>"},{"location":"labs/1.%20lingering/#user-manager","title":"User Manager","text":"<p>The user manager is the per-user systemd instance that manages all user-scope units. On a system with systemd, there are two main layers of system management:</p> Layer Runs as Controls System manager <code>PID 1</code> (<code>/lib/systemd/systemd</code>) system-wide units (<code>/etc/systemd/system/*.service</code>) User manager <code>/lib/systemd/systemd --user</code> (one per logged-in or lingering user) per-user units (<code>~/.config/systemd/user/*.service</code>) <p>So each logged-in user (or each \u201clingering\u201d user) gets their own lightweight systemd instance that controls user-scoped services, sockets, timers, and targets. You can see it with <code>ps -u lingeruser | grep systemd</code>.</p>"},{"location":"labs/1.%20lingering/#two-ways-the-user-manager-starts","title":"Two ways the user manager starts","text":"<ol> <li> <p>Interactive login (default behavior)     When a user logs in (via <code>ssh</code>, TTY, desktop, etc.), PAM runs <code>systemd --user</code>, creating <code>/run/user/&lt;uid&gt;/</code> and starting user services like <code>dbus.socket</code>, <code>xdg-desktop-portal.service</code>, etc.</p> </li> <li> <p>Lingering (non-interactive)     When you enable lingering via <code>loginctl enable-linger &lt;user&gt;</code>. Systemd ensures that the per-user manager (<code>user@&lt;uid&gt;.service</code>) starts at boot, even if the user never logs in. That service runs as root's child process under the system manager and spawns <code>/lib/systemd/systemd --user</code> for that UID.</p> </li> </ol>"},{"location":"labs/1.%20lingering/#relationship-to-runuserbus","title":"Relationship to /run/user/\\/bus <p>The D-Bus session bus socket at <code>/run/user/&lt;uid&gt;/bus</code> is created and managed by the user manager.</p> <ul> <li>The user manager starts <code>dbus.socket</code> and <code>dbus.service</code> (or <code>dbus-broker.service</code>). </li> <li><code>dbus.socket</code> creates the <code>bus</code> socket.</li> <li>Any client that connects to <code>/run/user/&lt;uid&gt;/bus</code> triggers D-Bus activation of the broker/service.</li> </ul> <p>If the user manager isn't running, there's nothing to start <code>dbus.socket</code>, so <code>/run/user/&lt;uid&gt;/bus</code> doesn't exist \u2014 even though <code>/run/user/&lt;uid&gt;/</code> might. See it all running:</p> <pre><code>vagrant@debian12:~$ sudo loginctl user-status lingeruser \nlingeruser (1001)\n           Since: Fri 2025-10-17 19:29:45 UTC; 24min ago\n           State: lingering\n          Linger: yes\n            Unit: user-1001.slice\n                  \u2514\u2500user@1001.service\n                    \u2514\u2500init.scope\n                      \u251c\u25001801 /lib/systemd/systemd --user\n                      \u2514\u25001803 \"(sd-pam)\"\n\n...\nOct 17 ... systemd[1801]: Starting dbus.socket - D-Bus User Message Bus Socket...\nOct 17 ... systemd[1801]: Listening on dbus.socket - D-Bus User Message Bus Socket.\n...\nOct 17 19:29:46 debian12 systemd[1801]: Startup finished in 147ms.\n</code></pre>","text":""},{"location":"labs/1.%20lingering/#2-create-install-and-start-test-systemd-user-service","title":"2. Create, install, and start test systemd user service","text":"<p>NOTE: Sudo into the lingeruser with <code>sudo su - lingeruser</code></p> <p>Let's create the service directory and add lingeruser's new user-scoped systemd service. The service loops forever printing to a log file in /tmp then sleeping for 10 seconds. </p> <pre><code>mkdir -p ~/.config/systemd/user\n\ncat &gt; ~/.config/systemd/user/test-linger.service&lt;&lt;'EOF'\n[Unit]\nDescription=Lingering Proof Test Service\n\n[Service]\nExecStart=/bin/bash -c 'while true; do echo \"$(date -Is) test-linger running\" &gt;&gt; /tmp/test-linger.log; sleep 10; done'\nRestart=always\nRestartSec=5\n\n[Install]\nWantedBy=default.target\n\nEOF\n</code></pre> <p>Enable and start the service like any other service, but for user scope we use the <code>--user</code> switch. We also confirm the service has started and is logging to the file:</p> <pre><code>systemctl --user daemon-reload\nsystemctl --user enable --now test-linger.service\nsystemctl --user status test-linger.service\nsystemctl --user list-units --type=service | grep \"test-linger\"\ntail -f /tmp/test-linger.log\n</code></pre> <p>NOTE: you might need to type q to quit the systemd unit listing command</p> <p>Ctrl-C to exit the tail output.</p>"},{"location":"labs/1.%20lingering/#3-reboot-with-lingering","title":"3. Reboot with lingering","text":"<p>Back as vagrant user:</p> <p><code>sudo reboot</code></p>"},{"location":"labs/1.%20lingering/#4-check-if-the-test-service-runs-after-reboot","title":"4. Check if the test service runs after reboot","text":"<p>SSH back into the vagrant machine and run this as the vagrant user:</p> <p><code>tail -f /tmp/test-linger.log</code></p> <p>You might press return then wait for another line to be printed in 10s. Press Ctrl-C to exit the tailed output.</p>"},{"location":"labs/1.%20lingering/#5-disable-lingering-and-reboot","title":"5. Disable lingering and reboot","text":"<pre><code>sudo loginctl disable-linger lingeruser\nsudo reboot\n</code></pre>"},{"location":"labs/1.%20lingering/#6-check-if-the-test-service-runs-after-reboot-should-not-be-running","title":"6. Check if the test service runs after reboot (should NOT be running)","text":"<p>SSH back into the vagrant machine and run this as the vagrant user:</p> <pre><code>~/Local/brothaman-labs vagrant ssh                                              Last login: Fri Oct 17 20:11:38 2025 from 192.168.122.1\nvagrant@debian12:~$ tail -f /tmp/test-linger.log\ntail: cannot open '/tmp/test-linger.log' for reading: No such file or directory\ntail: no files remaining\nvagrant@debian12:~$ sleep 10; tail -f /tmp/test-linger.log\ntail: cannot open '/tmp/test-linger.log' for reading: No such file or directory\ntail: no files remaining\nvagrant@debian12:~$ sudo ps -u lingeruser\n    PID TTY          TIME CMD\n</code></pre> <p>It is clear nothing is running any longer.</p>"},{"location":"labs/1.%20lingering/#7-re-enable-lingering-and-see-what-happens","title":"7. Re-enable lingering and see what happens","text":"<pre><code>echo \"checking if new user's systemd instance is running\"\nsudo ps -ux -U lingeruser | grep 'systemd --user' | grep -v grep\necho no systemd from the ps command \u261d\ufe0f\necho\n\n# make the user linger\nsudo loginctl enable-linger lingeruser\necho \"checking if new user's systemd instance is running\"\nsleep 3\nsudo ps -ux -U lingeruser | grep 'systemd --user' | grep -v grep\necho see systemd now from the ps command \u261d\ufe0f\nsudo ps -ux -U lingeruser | grep 'test-linger' | grep -v grep\necho see test-linger shell from the ps command \u261d\ufe0f\necho\n</code></pre> <p>Reboot and check again, the service should still be up so long as we do not turn off user lingering.</p>"},{"location":"labs/1.%20lingering/#8-use-bro-user-instead","title":"8. Use bro-user instead","text":"<p>The <code>bro-user</code> script including in the brothaman-scripts package can help create user-scoped lingering users and it does so using ZFS backing stores. It automatically enables the necessary lingering and runs commands to create the specified user and its datasets. Give it a try:</p> <pre><code>sudo bro-user postgres\n</code></pre> <p>Note if you do not have the latest podman installed it will install that too. After the command completes, check that the user exists:</p> <pre><code>id postgres\ngetent passwd postgres\nsudo -iu postgres ls -ld .\nzfs list\nsudo -iu postgres podman image list\nsudo -iu postgres podman run quay.io/podman/hello:latest\n</code></pre> <p>All <code>bro-user</code> created users are lingering by default. And their home directories are under the <code>/var/lib/containers/unprivileged/</code> base path which itself is a ZFS dataset. Life gets easier with <code>bro-user</code>!</p>"},{"location":"labs/1.%20lingering/#9-lessons-learned","title":"9. Lessons learned","text":"<p>Through this lab we verified how systemd's user-scoped manager behaves under different conditions and why lingering is such an important feature for background or headless service users.</p>"},{"location":"labs/1.%20lingering/#key-takeaways","title":"Key takeaways","text":"<ol> <li> <p>User-mode systemd is separate from system-mode.     Every user runs their own <code>systemd --user</code> instance, called the user manager.     This instance controls user-scoped units located under <code>~/.config/systemd/user/</code>.</p> </li> <li> <p>Lingering keeps the user manager alive across reboots and logouts.     Without lingering, <code>systemd --user</code> only starts when the user logs in through PAM.     With lingering enabled (<code>loginctl enable-linger &lt;user&gt;</code>), the user\u2019s <code>user@UID.service</code> starts at boot, even if they never log in.</p> </li> <li> <p>Environment variables are critical. <code>XDG_RUNTIME_DIR</code> and <code>DBUS_SESSION_BUS_ADDRESS</code> must point to <code>/run/user/&lt;uid&gt;</code> and its bus socket so that commands like <code>systemctl --user</code> can talk to the user manager.     Adding <code>/etc/profile.d/xdg_runtime.sh</code> ensures these variables exist for all unprivileged users.</p> </li> <li> <p>The D-Bus socket (<code>/run/user/&lt;uid&gt;/bus</code>) belongs to the user manager.     It only exists while that per-user <code>systemd --user</code> instance is running.     When lingering is disabled or the manager stops, the socket disappears and user-scoped commands fail.</p> </li> <li> <p>Services enabled under <code>default.target</code> survive reboots.     Our test service <code>test-linger.service</code> continued running after every reboot while lingering was enabled, and stopped existing once lingering was disabled.</p> </li> <li> <p>Lingering makes user accounts behave like lightweight system services.     This is ideal for rootless containers, background daemons (e.g., PostgreSQL, Gitea), or personal automation that must persist without an active session.</p> </li> <li> <p>Each lingered user consumes persistent resources.     Because their <code>user@UID.service</code> remains active, each lingering user adds a small amount of memory and process overhead\u2014acceptable for a few users, but not something to enable for hundreds indiscriminately.</p> </li> <li><code>bro-user</code> simplifies lingering user creation.     The <code>bro-user</code> script automates creating unprivileged lingering users with ZFS-backed home directories, making it easy to manage unprivileged service accounts.</li> </ol>"},{"location":"labs/1.%20lingering/#practical-implications","title":"Practical implications","text":"<ul> <li>Use lingering for any headless service accounts that must auto-start at boot and not depend on an interactive login.</li> <li>Always ensure that <code>dbus-user-session</code> or <code>dbus-broker</code> is installed so the per-user D-Bus bus can start.</li> <li>Confirm lingering state with <code>loginctl show-user &lt;user&gt; | grep Linger</code></li> <li>Verify the user manager is alive with<code>systemctl status user@$(id -u &lt;user&gt;).service</code></li> </ul>"},{"location":"labs/1.%20lingering/#final-observation","title":"Final observation","text":"<p>Once lingering is enabled, a normal unprivileged account becomes a first-class citizen in systemd's dependency graph \u2014 its services are started, stopped, and monitored just like system-level units.</p> <p>Disabling lingering cleanly removes that persistence, returning the user to an ephemeral, login-only environment.</p>"},{"location":"labs/1.%20lingering/#9-rollback","title":"9. Rollback","text":"<p>When finished, restore the VM to the baseline snapshot:</p> <pre><code>vagrant snapshot restore before-starting --no-provision\n</code></pre>"},{"location":"labs/2.%20quadlet/","title":"Quadlet","text":"<p>In this lab we install the latest Podman with Quadlet support and use it to create, install, and test a simple container service under an unprivileged lingering user.</p> <p>This builds on 1. lingering \u2014 make sure you have completed that experiment and verified that lingering is enabled for your test user. Here's a quick summary of steps:</p> <ol> <li>Uninstall system podman if present</li> <li>Install the latest podman from alvistack</li> <li>Drop in the test-quadlet.container</li> <li>Reboot test making sure it is up after reboot</li> </ol>"},{"location":"labs/2.%20quadlet/#0-snapshot","title":"0. Snapshot","text":"<p>Before making changes, capture a baseline snapshot of the VM so we can roll back cleanly if anything goes wrong:</p> <pre><code>if ! vagrant snapshot list 2&gt;&amp;1 | grep before-quadlet &gt;/dev/null; then\n  vagrant snapshot save before-quadlet;\nelse\n  echo before-quadlet snapshot already exists;\nfi\n</code></pre> <p>This ensures that you can easily revert to a clean pre-Quadlet state.</p>"},{"location":"labs/2.%20quadlet/#1-uninstall-system-podman-if-present","title":"1. Uninstall System Podman (if present)","text":"<p>First, remove any system-level Podman installation that might conflict with the user-space version we'll install next. If you installed the <code>bro-user</code> in the previous lab you will not have to do this.</p> <pre><code>sudo apt remove -y podman podman-rootless podman-plugins containers-common\nsudo apt autoremove -y\n</code></pre> <p>After removal, confirm that no residual binaries or systemd services remain:</p> <pre><code>which podman || echo \"Podman removed\"\nsudo systemctl disable --now podman.service podman.socket 2&gt;/dev/null || true\n</code></pre> <p>You should see \u201cPodman removed\u201d printed and no active Podman system services.</p>"},{"location":"labs/2.%20quadlet/#2-install-the-latest-podman-from-alvistack","title":"2. Install the Latest Podman from Alvistack","text":"<p>Back to the vagrant user</p> <p>We'll now install the latest version of Podman from the Alvistack repository, which typically ships newer Podman releases with full Quadlet support. Again this was done for you when you did the first <code>bro-user postgres</code> command in the last lab.</p>"},{"location":"labs/2.%20quadlet/#21-add-the-repository","title":"2.1 Add the Repository","text":"<pre><code>sudo apt update\nsudo apt install -y curl gnupg2\ncurl -fsSL https://downloadcontent.opensuse.org/repositories/home:/alvistack/Debian_12/Release.key | \\\n  sudo gpg --dearmor -o /usr/share/keyrings/alvistack.gpg\n\necho \"deb [signed-by=/usr/share/keyrings/alvistack.gpg] http://downloadcontent.opensuse.org/repositories/home:/alvistack/Debian_12 ./\" | \\\n  sudo tee /etc/apt/sources.list.d/alvistack.list\n</code></pre>"},{"location":"labs/2.%20quadlet/#22-install-podman","title":"2.2 Install Podman","text":"<pre><code>sudo apt update\nsudo apt install -y \\\n  podman conmon crun catatonit \\\n  netavark aardvark-dns \\\n  slirp4netns uidmap \\\n  fuse-overlayfs dbus-user-session \\\n  iptables nftables \\\n  passt\n</code></pre>"},{"location":"labs/2.%20quadlet/#23-verify-quadlet-support","title":"2.3 Verify Quadlet Support","text":"<p>Back to the lingeruser: <code>sudo su - lingeruser</code></p> <pre><code>podman quadlet --help\n</code></pre> <p>You should see subcommands like <code>install</code>, <code>list</code>, <code>rm</code>, and <code>print</code>.</p> <p>Next, check podman info:</p> <pre><code>podman system info\n</code></pre>"},{"location":"labs/2.%20quadlet/#24-kick-the-tires","title":"2.4 Kick the Tires","text":"<p>Run a quick smoke test to ensure Podman works for both root and unprivileged users:</p> <pre><code>podman run --rm quay.io/podman/hello\n</code></pre> <p>And print the version for reference:</p> <pre><code>podman version\n</code></pre> <p>Note the Client, Server, and Quadlet versions for your documentation. So basically <code>bro-user</code> set everything up by calling <code>bro-install-podman</code> for you when you created the user in the last lab. This is here to just show you how it is done and how much hassle it saves you.</p>"},{"location":"labs/2.%20quadlet/#3-drop-in-the-test-quadlet","title":"3. Drop in the Test Quadlet","text":"<p>In this section we'll create a simple Quadlet file for an unprivileged lingering user. This container will simply run a small web server to verify Quadlet automation.</p>"},{"location":"labs/2.%20quadlet/#31-switch-to-the-user","title":"3.1 Switch to the User","text":"<p>Back to the lingeruser: <code>sudo su - lingeruser</code></p>"},{"location":"labs/2.%20quadlet/#32-create-the-quadlet-file","title":"3.2 Create the Quadlet File","text":"<p>Create the Quadlet definition in the user's configuration directory:</p> <pre><code>mkdir -p ~/.config/containers/systemd/\ncat &gt; ~/.config/containers/systemd/test-quadlet.container &lt;&lt;'EOF'\n[Unit]\nDescription=Quadlet Test Container\n\n[Container]\nImage=nginx:alpine\nExec=nginx -g 'daemon off;'\nContainerName=test-quadlet\nPublishPort=8080:80\n\n[Service]\nRestart=always\nTimeoutStartSec=300\nStartLimitIntervalSec=60s\nStartLimitBurst=5\nRestartSec=10s\n\n[Install]\nWantedBy=default.target\nEOF\n</code></pre>"},{"location":"labs/2.%20quadlet/#33-reload-and-inspect-quadlet","title":"3.3 Reload and Inspect Quadlet","text":"<p>Tell systemd to reload the user unit files:</p> <pre><code>systemctl --user daemon-reload\n</code></pre> <p>Now check what Quadlet sees:</p> <pre><code>podman quadlet list\n</code></pre> <p>Start it:</p> <pre><code>systemctl --user start test-quadlet.service\n</code></pre> <p>And confirm that the generated <code>.service</code> file exists under:</p> <pre><code>cat /run/user/$UID/systemd/generator/test-quadlet.service\n</code></pre> <p>You should see the generated service file contents. Notice there was no service enable operation right? Then check the status:</p> <pre><code>systemctl --user status test-quadlet.service\n</code></pre> <p>You should see the Nginx container running with <code>Active: active (running)</code> status. Verify from Podman\u2019s perspective:</p> <pre><code>podman ps\n</code></pre> <p>Finally, test the container by curling the local port:</p> <pre><code>curl http://localhost:8080\n</code></pre> <p>You should see the Nginx welcome page HTML. This is a generic default server page confirming that the container is nginx server correctly when NOT configured.</p>"},{"location":"labs/2.%20quadlet/#4-reboot-and-persistence-test","title":"4. Reboot and Persistence Test","text":"<p>Reboot the VM to ensure the Quadlet container auto-starts thanks to lingering:</p> <pre><code>sudo reboot\n</code></pre> <p>After reboot, log back in as the same unprivileged lingeruser and check:</p> <pre><code>systemctl --user status test-quadlet.service\n</code></pre> <p>You should see it running automatically.</p> <p>Confirm from Podman:</p> <pre><code>podman ps\n</code></pre> <p>And once more verify with:</p> <pre><code>curl http://localhost:8080\n</code></pre> <p>If all looks good, Quadlet persistence is working correctly under the lingering user.</p>"},{"location":"labs/2.%20quadlet/#5-rollback-optional","title":"5. Rollback (Optional)","text":"<p>If you want to revert to the pre-Quadlet state, roll back using your saved snapshot:</p> <pre><code>vagrant snapshot restore before-quadlet\n</code></pre> <p>Don't bother though the next lab will build on this one.</p>"},{"location":"labs/2.%20quadlet/#lessons-learned","title":"Lessons Learned","text":"<ol> <li>Quadlet integrates tightly with systemd, turning container definitions into first-class services.</li> <li>Lingering users are essential for persistent unprivileged containers across reboots.</li> <li>Alvistack\u2019s Podman builds offer up-to-date features missing from Debian's stock packages.</li> <li>Generated <code>.service</code> units under <code>/run/user/&lt;uid&gt;/containers/systemd/</code> are ephemeral but reflect the active container states.</li> <li>Quadlet simplifies container lifecycle management \u2014 no manual <code>podman run</code> required once configured.</li> <li>When combined with lingering, Quadlet gives you reliable, self-starting, user-scoped container services that survive reboots without root privileges.</li> </ol>"},{"location":"labs/3.%20volumes/","title":"Volumes","text":"<p>In this lab you will orchestrate storage using Podman volume quadlets: first wiring bind mounts into container units so nginx can serve the synced MkDocs site, then defining dedicated volume units whose lifecycle is decoupled from their consuming containers. Along the way you'll manage PostgreSQL data on ZFS, practice snapshotting and rollbacks, and see how volume descriptors integrate cleanly with container services and why a separate volume quadlet is beneficial in many scenarios.</p> <ol> <li>Serve Brothaman's docs with <code>Volume=</code> directives in test-quadlet.container's <code>[Container]</code> section</li> <li>Creating a volume quadlet</li> <li>Switching to the zfs-helper for snapshot management</li> <li>Connecting the volume to the PostgreSQL container quadlet</li> <li>Testing data persistence and snapshot rollbacks</li> </ol>"},{"location":"labs/3.%20volumes/#0-snapshot","title":"0. Snapshot","text":"<p>Since this test runs inside a Vagrant VM, capture a baseline snapshot before making any changes. From your host machine (in the same directory as your Vagrantfile):</p> <pre><code>if ! vagrant snapshot list 2&gt;&amp;1 | grep before-volumes &gt;/dev/null; then\n  vagrant snapshot save before-volumes;\nelse\n  echo before-volumes snapshot already exists;\nfi\n</code></pre> <p>This allows you to roll the VM back to its exact pre-test state later.</p>"},{"location":"labs/3.%20volumes/#1-serve-brothamans-docs-with-volume-directives-in-test-quadletcontainers-container-section","title":"1. Serve Brothaman's docs with <code>Volume=</code> directives in test-quadlet.container's <code>[Container]</code> section","text":"<p>In the last lab we created a simple nginx based container quadlet. By default without any configuration it serves a default test page. Let's go one step further and have it serve the brothaman website.</p> <p>If you look into the Vagrantfile an rsync command is used to copy the local <code>docs</code> directory into the VM at <code>/brothaman</code> with your lingeruser's uid:</p> <pre><code>  config.vm.synced_folder \"docs\", \"/brothaman\",\n    type: \"rsync\",\n    create: true,\n    owner: 1001,\n    group: 1001,\n    rsync__chown: true,\n    rsync__auto: false,\n    rsync__args: [\n      \"--verbose\",\n      \"--archive\",\n    ]\n</code></pre> <p>To have nginx serve this content, a virtual server configuration needs to be passed telling it how and from where to serve the content. The following nginx server configuration tells nginx to use /var/www/brothaman as its server root:</p> <pre><code>mkdir ~/nginx-conf.d/\ncat &gt; ~/nginx-conf.d/brothaman.org.conf &lt;&lt;'EOF'\nserver {\n    listen 80;\n    server_name brothaman.org www.brothaman.org;\n\n    # put your site files here (we just made it above)\n    root /var/www/brothaman;\n    index index.html index.htm;\n\n    # log to container stdout/stderr (best for containers)\n    access_log /dev/stdout;\n    error_log  /dev/stderr warn;\n\n    location / {\n        try_files $uri $uri/ =404;\n    }\n}\nEOF\n</code></pre> <p>The container author configured nginx to look for server configurations in <code>/etc/nginx/conf.d/</code>. Our configuration needs to reside there to get picked up at startup. Instead of copying the file into the container image, we can use a bind mount to map our configuration directory onto the container at <code>/etc/nginx/conf.d/</code> the quadlet descriptor, <code>Volume=/home/lingeruser/nginx-conf.d:/etc/nginx/conf.d</code>. Another <code>Volume=</code> mapping, <code>Volume=/brothaman:/var/www/brothaman</code>, also directly within the test-quadlet's <code>[Container]</code> section. Together nginx can then serve this directory as using the brothaman.org virtual server:</p> <pre><code>mkdir -p ~/.config/containers/systemd/\ncat &gt; ~/.config/containers/systemd/test-quadlet.container &lt;&lt;'EOF'\n[Unit]\nDescription=Quadlet Test Container\n\n[Container]\nImage=nginx:alpine\nExec=nginx -g 'daemon off;'\nContainerName=test-quadlet\nPublishPort=8080:80\nVolume=/brothaman:/var/www/brothaman\nVolume=/home/lingeruser/nginx-conf.d:/etc/nginx/conf.d\n\n[Service]\nRestart=always\nTimeoutStartSec=300\nStartLimitIntervalSec=60s\nStartLimitBurst=5\nRestartSec=10s\n\n[Install]\nWantedBy=default.target\nEOF\n</code></pre> <p>Before starting the container notice no separate volume quadlet was needed: meaning we did not create a <code>dot.volume</code> file. Both directives were put into the container quadlet's <code>[Container]</code> section directly. There's only one service, the test-quadlet container service, and it can now serve the mkdocs content. Reload systemd and start the container:</p> <pre><code>systemctl --user daemon-reload\nsystemctl --user restart test-quadlet.service\ncurl \"http://$(ip route | grep 'kernel scope link' | awk '{print $9}'):8080\"\n</code></pre> <p>You should see the bro site contents and NOT the welcome the nginx page. You can point your browser to the VM's IP address at port 8080 to see it as well. Don't worry you don't have to set up DNS for brothaman.org, just accessing the IP:8080 will work fine. With one virtual server configuration nginx just uses it as the default site.</p>"},{"location":"labs/3.%20volumes/#2-creating-a-volume-quadlet","title":"2. Creating a volume quadlet","text":"<p>The example above is so simple, no separate volume quadlet was needed. However, for more complex scenarios where you:</p> <ul> <li>share the volume with multiple containers, or</li> <li>you want to manage the lifecycle of the volume separately from the container, or</li> <li>you want to use advanced volume features like ZFS snapshots</li> </ul> <p>It's beneficial to create a dedicated volume quadlet. In this lab section, the concept will be put to the test so we can see how it works and helps.</p>"},{"location":"labs/3.%20volumes/#21-create-a-zfs-backed-volume-quadlet-for-postgresql","title":"2.1 Create a ZFS backed volume quadlet for PostgreSQL","text":"<p>Let's consider a database container like PostgreSQL which needs persistent storage for its data. Before the container starts, we want to take a snapshot of the data. In case anything goes wrong during the container's operation, we can roll back to the previous snapshot. In such cases, a dedicated volume quadlet is the way to go to isolate the functionality.</p> <p>Let's use a ZFS-backed volume to take snapshots of the data volume between container restarts. We start by creating a ZFS dataset for PostgreSQL data storage and set it to mount at <code>/home/lingeruser/postgres</code>. Then we create a volume quadlet to manage this dataset.</p> <p>Make sure you're logged as the <code>vagrant</code> user to escalate privileges to root with <code>sudo</code>. Below a ZFS dataset, <code>testing/postgres</code>, is created then its mountpoint set to <code>/home/lingeruser/postgres</code>: it mounts automatically using the ZFS mount service. The mountpoint's user and group ownership permissions are also set to be owned by lingeruser so lineruser's unprivileged containers can access it.</p> <pre><code>sudo zfs create testing/postgres\nsudo zfs set mountpoint=/home/lingeruser/postgres testing/postgres\nsudo chown lingeruser:lingeruser /home/lingeruser/postgres\nsudo -iu lingeruser ls -ld postgres\ndf -h /home/lingeruser/postgres/\n</code></pre> <p>The dataset is now ready to be used as a persistent volume for our PostgreSQL container. Now switch back to the <code>lingeruser</code> again with <code>sudo su - lingeruser</code>:</p> <pre><code>mkdir -p ~/.config/containers/systemd/\ncat &gt; ~/.config/containers/systemd/postgres-data.volume &lt;&lt;'EOF'\n[Unit]\nDescription=PostgreSQL Volume\n\n[Volume]\nVolumeName=postgres-data\n\n[Service]\nExecStartPre=/usr/sbin/zfs snapshot testing/postgres@pre\nEOF\n\nsystemctl --user daemon-reload\nsystemctl --user start postgres-data-volume.service\n</code></pre> <p>NOTICE: The <code>postgres-data.volume</code> quadlet is started and stopped using a service name that combines the volume name, the <code>-volume</code> suffix, and the <code>.service</code> extension. The service name construction pattern is automatically handled by podman when you create a volume quadlet.</p>"},{"location":"labs/3.%20volumes/#22-troubleshooting-problems","title":"2.2 Troubleshooting Problems","text":"<p>How did it go for you? Using <code>journalctl --user -xe -u postgres-data-volume.service</code>, here are the failure errors in the service journal log:</p> <pre><code>Oct 26 14:13:16 debian12.localdomain systemd[904]: Starting postgres-data-volume.service - PostgreSQL Volume...\nOct 26 14:13:16 debian12.localdomain postgres-data-volume[45001]: cannot create snapshots : permission denied\nOct 26 14:13:16 debian12.localdomain systemd[904]: postgres-data-volume.service: Control process exited, code=exited, status=1/FAILURE\nOct 26 14:13:16 debian12.localdomain systemd[904]: postgres-data-volume.service: Failed with result 'exit-code'.\nOct 26 14:13:16 debian12.localdomain systemd[904]: Failed to start postgres-data-volume.service - PostgreSQL Volume.\n</code></pre> <p>The permission error is expected. The <code>lingeruser</code> does not have permission to create snapshots. Try this as <code>vagrant</code> to escalate to sudo and delegate snapshot permission to <code>lingeruser</code> on the dataset:</p> <pre><code>sudo zfs allow lingeruser snapshot testing/postgres\n</code></pre> <p>Now sudo back to <code>lingeruser</code> and start the volume service again:</p> <pre><code>systemctl --user start postgres-data-volume.service\nsystemctl --user status postgres-data-volume.service\nzfs list -t snapshot testing/postgres\n</code></pre> <p>You should see the volume service started successfully this time and a snapshot was created. Amazing! Now try stopping the volume service and starting it again now with:</p> <pre><code>systemctl --user stop postgres-data-volume.service\nsystemctl --user status postgres-data-volume.service -l --no-pager\nsystemctl --user start postgres-data-volume.service\n</code></pre> <p>This fails too because the snapshot with the same name already exists. Feel free to check the error message with <code>journalctl --user -xe -u postgres-data-volume.service</code>. Let's create snapshots with unique names using timestamps prefixed to the base name of the snapshot:</p> <pre><code>mkdir -p ~/.config/containers/systemd/\ncat &gt; ~/.config/containers/systemd/postgres-data.volume &lt;&lt;'EOF'\n[Unit]\nDescription=PostgreSQL Volume\n\n[Volume]\nVolumeName=postgres-data\n\n[Service]\nExecStartPre=/usr/sbin/zfs snapshot testing/postgres@pre_$(date +%Y%m%d%H%M%S)\nEOF\n</code></pre> <p>This too fails and the output looks a little cryptic in the logs:</p> <pre><code>Oct 28 08:28:45 debian12.localdomain systemd[904]: Starting postgres-data-volume.service - PostgreSQL Volume...\nOct 28 08:28:45 debian12.localdomain postgres-data-volume[78548]: usage:\nOct 28 08:28:45 debian12.localdomain postgres-data-volume[78548]:         snapshot [-r] [-o property=value] ... &lt;filesystem|volume&gt;@&lt;snap&gt; ...\nOct 28 08:28:45 debian12.localdomain postgres-data-volume[78548]: For the property list, run: zfs set|get\nOct 28 08:28:45 debian12.localdomain postgres-data-volume[78548]: For the delegated permission list, run: zfs allow|unallow\nOct 28 08:28:45 debian12.localdomain systemd[904]: postgres-data-volume.service: Control process exited, code=exited, status=2/INVALIDARGUMENT\nOct 28 08:28:45 debian12.localdomain systemd[904]: postgres-data-volume.service: Failed with result 'exit-code'.\n</code></pre> <p>What's going on here? The clue is in the <code>usage</code> complaint by the zfs CLI program. It's not able to parse the command line. The problem is that the <code>$(date +%Y%m%d%H%M%S)</code> command substitution is not being interpreted by systemd as expected. To fix this, we need to wrap the command substitution in a shell execution context. Update the <code>ExecStartPre</code> line as follows and try again:</p> <pre><code>mkdir -p ~/.config/containers/systemd/\ncat &gt; ~/.config/containers/systemd/postgres-data.volume &lt;&lt;'EOF'\n[Unit]\nDescription=PostgreSQL Volume\n\n[Volume]\nVolumeName=postgres-data\n\n[Service]\nExecStartPre=/bin/sh -c '/usr/sbin/zfs snapshot testing/postgres@pre_\"$(date +%Y%m%d%H%M%S)\"'\nEOF\n\nsystemctl --user daemon-reload\nsystemctl --user start postgres-data-volume.service\n</code></pre> <p>This too fails. Take a look at why in the journal, but there's a bunch of junk generated instead of the date string we wanted to append. That happens because systemd is performing specifier expansion. The '%' and '$' characters have special meanings in systemd unit files. They need to be escaped to be interpreted literally. Update the <code>ExecStartPre</code> line again as follows with double <code>$$</code> and <code>%%</code>:</p> <pre><code>mkdir -p ~/.config/containers/systemd/\ncat &gt; ~/.config/containers/systemd/postgres-data.volume &lt;&lt;'EOF'\n[Unit]\nDescription=PostgreSQL Volume\n\n[Volume]\nVolumeName=postgres-data\n\n[Service]\nExecStartPre=/bin/sh -c '/usr/sbin/zfs snapshot testing/postgres@pre_\"$$(date +%%Y%%m%%d%%H%%M%%S)\"'\nEOF\n\nsystemctl --user daemon-reload\nsystemctl --user start postgres-data-volume.service\nzfs list -t snapshot testing/postgres\n</code></pre> <p>Now this works! Each time the volume service is started a new snapshot with a unique timestamped name is created. Great! Keep trying it and test by starting and restarting the service. Here let me help you:</p> <pre><code>for restart in $(seq 1 3); do\n  systemctl --user restart postgres-data-volume.service\n  sleep 2\ndone\nzfs list -t snapshot testing/postgres\n</code></pre>"},{"location":"labs/3.%20volumes/#23-lets-delete-the-extra-snapshots","title":"2.3 Let's delete the extra snapshots","text":"<p>That's looking good. Multiple snapshots were created with unique names. Perfect! But now we got that <code>pre</code> without the prefix let's try deleting that still logged in as the <code>lingeruser</code>:</p> <pre><code>zfs destroy testing/postgres@pre\n</code></pre> <p>In fact we cannot delete any snapshot. You should see they all fail due to a lack of permissions. That's because the <code>lingeruser</code> does not have permission to destroy snapshots. Let's switch back to <code>vagrant</code> and delegate the destroy permission on the dataset to <code>lingeruser</code>:</p> <pre><code>sudo zfs allow lingeruser destroy,snapshot testing/postgres\n</code></pre> <p>Now if you then try the destroy on the snapshot again it still does not work. What's going on here? The reason is ZFS supports the delegation of snapshot creation, but not the destruction of snapshots. Even the dataset owner can not destroy snapshots. But try destroy the pre snapshot with the <code>vagrant</code> user as root:</p> <pre><code>vagrant@debian12:~$ sudo zfs destroy testing/postgres@pre\nvagrant@debian12:~$ sudo zfs list -t snapshot testing/postgres\nNAME                                  USED  AVAIL     REFER  MOUNTPOINT\ntesting/postgres@pre-20251028085229     0B      -     40.0M  -\ntesting/postgres@pre-20251028085533     0B      -     40.0M  -\ntesting/postgres@pre-20251028085535     0B      -     40.0M  -\ntesting/postgres@pre-20251028085537     0B      -     40.0M  -\nvagrant@debian12:~$\n</code></pre> <p>That worked but we had to exit the <code>lingeruser</code> and then escalate to root with <code>sudo</code> as the <code>vagrant</code> user. Not very convenient.</p>"},{"location":"labs/3.%20volumes/#3-switching-to-the-zfs-helper-for-snapshot-management","title":"3. Switching to the zfs-helper for snapshot management","text":"<p>So to manage snapshot deletion, we need to be perform the destroy operation with elevated privileges with the vagrant sudo enabled user. This is a limitation of OpenZFS's delegation model. The whole situation should be pretty irritating to you. Especially if you want to automate snapshot management in unprivileged containers and have to login and out of one user to sudo from another all the time right?</p> <p>WARNING: What many lazy admins (who you should never hire) often do in this case is make their unprivileged users sudoers without password prompt for zfs commands. Even adding having a sudo password should be forbidden on production systems. Unprivileged users for application sandboxing purposes should NEVER have the ability to escalate privileges with or without passwords. WTF is the point anyway right?</p> <p>Then what if you want out of this login, log out hell, and want some consistency. First off you should be automating the process with DevOps tools, but let me not go there right now. It's still pretty irritating even if you automate things. Plus doing so you still need two different users, one with sudo, and the target non sudoers user contexts to properly manage snapshots. Ugh.</p>"},{"location":"labs/3.%20volumes/#31-setup-lingeruser-to-use-zfs-helper","title":"3.1 Setup lingeruser to use zfs-helper","text":"<p>Although there are several ways to skin this cat, the best is to use a helper service that runs with elevated privileges and performs zfs management tasks on behalf of unprivileged user-scoped services. Such a service needs a solid policy framework to validate and authorize requests from unprivileged users. Its authorization controls should limit what operations can be performed on which datasets by which users. Luckily we did just that for brothaman and it is called <code>zfs-helper</code>. The stock package is already installed in the VM. All we need to do is configure it to allow our user's <code>postgres-data-volume.service</code> to manage snapshots on the <code>testing/postgres</code> dataset. See the edits added to the zfs-helper configuration files at <code>/etc/zfs-helper/policy.d/</code>:</p> <p>Perform these operations as the <code>vagrant</code> user.</p> <pre><code>sudo usermod -a -G zfshelper lingeruser\nsudo loginctl kill-user lingeruser\nsudo loginctl user-status lingeruser --no-pager\nsudo loginctl enable-linger lingeruser\nsudo loginctl user-status lingeruser --no-pager\nsudo -iu lingeruser id\n\npushd . || true\nsudo mkdir -p /etc/zfs-helper/policy.d/lingeruser\ncd /etc/zfs-helper/policy.d/lingeruser\ncat &lt;&lt;'EOF' | sudo tee -a units.list\npostgres-data-volume.service\nEOF\n\ncat &lt;&lt;'EOF' | sudo tee -a mount.list unmount.list snapshot.list rollback.list create.list destroy.list\nlingeruser testing/postgres\nEOF\npopd\n</code></pre> <p>Now let's use the service to manage snapshots for us. First, update the volume quadlet to use zfs-helper for snapshot creation instead of calling zfs directly. We could do this before directly since we were granted delegation, but let's do it now again to be consistent:</p> <p>Issue these commands as the <code>lingeruser</code>:</p> <pre><code>mkdir -p ~/.config/containers/systemd/\ncat &gt; ~/.config/containers/systemd/postgres-data.volume &lt;&lt;'EOF'\n[Unit]\nDescription=PostgreSQL Volume\n\n[Volume]\nVolumeName=postgres-data\n\n[Service]\nExecStartPre=/bin/sh -c '/usr/bin/zfs-helperctl snapshot testing/postgres@pre_\"$$(date +%%Y%%m%%d%%H%%M%%S)\"'\nEOF\n\nsystemctl --user daemon-reload\nsystemctl --user restart postgres-data-volume.service\nzfs list -t snapshot testing/postgres\n</code></pre>"},{"location":"labs/3.%20volumes/#32-delete-pre-start-snapshots-above-a-limit","title":"3.2 Delete pre-start snapshots above a limit","text":"<p>Now let's enhance the volume quadlet further to delete old snapshots above a certain limit each time the volume service starts. This way we can keep the number of snapshots manageable without manual intervention. Update the volume quadlet as follows to add a snapshot cleanup step before creating a new snapshot:</p> <pre><code>mkdir -p ~/.config/containers/systemd/\ncat &gt; ~/.config/containers/systemd/postgres-data.volume &lt;&lt;'EOF'\n[Unit]\nDescription=PostgreSQL Volume\nPartOf=test-postgresql.service\n\n[Volume]\nVolumeName=postgres-data\n\n[Service]\nEnvironment=KEEP_SNAPSHOTS=5\nExecStartPre=/bin/sh -c '/usr/bin/zfs-helperctl snapshot testing/postgres@pre_\"$$(date +%%Y%%m%%d%%H%%M%%S)\"'\nExecStartPre=/bin/sh -c 'zfs list -t snapshot -o name -s creation | grep -E \"^testing/postgres@pre_*\" | head -n -$${KEEP_SNAPSHOTS} | xargs -r -n1 zfs-helperctl destroy'\nEOF\nsystemctl --user daemon-reload\nsystemctl --user restart postgres-data-volume.service\nzfs list -t snapshot testing/postgres\n</code></pre> <p>Now each time the volume service starts, it will delete old pre-start snapshots beyond the specified limit after creating a new snapshot. This keeps the snapshot count manageable automatically. No fancy retention policies manager services or cron jobs needed. Just let the volume service handle it for you. You can verify this by restarting the volume service multiple times and checking the snapshot list:</p> <pre><code>for restart in $(seq 1 10); do\n  echo \"snapshot with $restart\"\n  systemctl --user restart postgres-data-volume.service\n  sleep 3\n  zfs list -t snapshot testing/postgres\ndone\n</code></pre>"},{"location":"labs/3.%20volumes/#4-connecting-the-volume-to-the-postgresql-container-quadlet","title":"4. Connecting the volume to the PostgreSQL container quadlet","text":"<p>Finally, let's connect the volume quadlet to the PostgreSQL container quadlet so the container can use the persistent storage with snapshotting and test it out. Update the PostgreSQL container quadlet to include the volume:</p> <pre><code>mkdir -p ~/.config/containers/systemd/\ncat &gt; ~/.config/containers/systemd/test-postgresql.container &lt;&lt;'EOF'\n[Unit]\nDescription=PostgreSQL Container\nPropagatesStopTo=postgres-data-volume.service\nBindsTo=postgres-data-volume.service\nAfter=postgres-data-volume.service\n\n[Container]\nImage=postgres:16\nContainerName=test-postgresql\nEnvironment=POSTGRES_PASSWORD=password\nVolume=/home/lingeruser/postgres:/var/lib/postgresql/data\nPublishPort=5432:5432\n\n[Service]\nRestart=always\nTimeoutStartSec=300\nStartLimitBurst=5\nRestartSec=10s\n\n[Install]\nWantedBy=default.target\nEOF\n\nsystemctl --user daemon-reload\nsystemctl --user start test-postgresql.service\n</code></pre> <p>Try starting the PostgreSQL container service. It should start successfully and use the ZFS-backed volume for data storage. You can verify this by checking the status of the container service and logging in as the postgres user on the server with psql:</p> <pre><code>systemctl --user status test-postgresql.service --no-pager -l\nexport PGPASSWORD=password\npsql -h \"$(ip route | grep 'kernel scope link' | awk '{print $9}')\" -p 5432 -U postgres -c '\\l'\n</code></pre> <p>If you don't have the psql client installed run this command as <code>vagrant</code> user:</p> <pre><code>sudo apt-get install -y postgresql-client\n</code></pre>"},{"location":"labs/3.%20volumes/#notes-on-propagatesstopto-bindsto-partof-and-after","title":"Notes on PropagatesStopTo=, BindsTo=, PartOf=, and After=","text":"<p>These directives are used to manage the relationships between systemd services:</p> <ul> <li><code>PartOf=</code>: This directive indicates that the service is part of another service. If the parent service is stopped or restarted, the child service will also be affected. We made the volume quadlet part of the container quadlet so that stopping the container also stops the volume service. If it was shared we would not opt to do this.</li> <li><code>BindsTo=</code>: This directive creates a stronger link between services. If the service specified in <code>BindsTo=</code> is stopped, the current service will also be stopped. Make no sense to run the database without its volume right? Before the volume stops the database is shutdown allowing it to sync its buffers to prevent corruption.</li> <li><code>After=</code>: This directive specifies that the current service should be started only after the specified service has been started. Doh! The database needs its volume to be ready before it starts.</li> <li><code>PropagatesStopTo=</code>: This directive allows a service to propagate stop signals to its dependencies. When the container is stopped, it will also stop the volume service. This way stopping the container actually stops the volume service too. Without this directive a systemctl restart of the container service will not stop and start the volume service.</li> </ul> <p>Systemd is extremely powerful and an amazing init system. In fact it goes well beyond an init system. It is an entire service management platform. Learning to use its features effectively can greatly enhance your ability to manage services and their dependencies. Unfortunately, we snuck these in, with only slight mentions as describe here to limit the scope of this lab.</p> <p>With Quadlets, Podman has taken a revolutionary approach to container management by integrating deeply with systemd. This integration leverages systemd's capabilities to manage the container lifecycle and their dependencies effectively. It brings the power of systemd to container management, allowing for more robust and reliable containerized applications closer to the system. Its use of core system primitives instead of reinventing the wheel is a form of convergence. It's a game-changer for how we think about and manage containers in a Linux environment.</p>"},{"location":"labs/3.%20volumes/#5-testing-data-persistence-and-snapshot-rollbacks","title":"5. Testing data persistence and snapshot rollbacks","text":"<p>IMPORTANT: For this section you will need to keep two shells open, one logged into the VM as the <code>vagrant</code> user and one logged in as the <code>lingeruser</code>. This is because rolling back snapshots requires elevated privileges that only the <code>vagrant</code> user has via sudo. The <code>lingeruser</code> will be used to interact with the PostgreSQL container and perform database operations.</p> <p>You can now connect to the PostgreSQL container and verify that the data is being stored in the ZFS-backed volume in the <code>lingeruser</code> shell:</p> <pre><code>export PGPASSWORD=password\npsql -h \"$(ip route | grep 'kernel scope link' | awk '{print $9}')\" -p 5432 -U postgres -c '\\l'\n</code></pre> <p>You should see the default PostgreSQL databases listed. Let's create a new database, insert some data, and then restart the container to see that the data persists across restarts thanks to the ZFS-backed volume in the <code>lingeruser</code> shell:</p> <pre><code># Capture the latest snapshot before creating the test database\nbefore_testdb_snapshot=$(zfs list -t snapshot testing/postgres | grep -v NAME | awk '{print $1}' | tail -n 1)\necho \"Latest snapshot before the testdb: $before_testdb_snapshot\"\n\n# Let's create the database, the test table, and insert the test data\nexport PGPASSWORD=password\npsql -h \"$(ip route | grep 'kernel scope link' | awk '{print $9}')\" -p 5432 -U postgres -c 'CREATE DATABASE testdb;'\npsql -h \"$(ip route | grep 'kernel scope link' | awk '{print $9}')\" -p 5432 -U postgres -d testdb -c 'CREATE TABLE testtable (id SERIAL PRIMARY KEY, name VARCHAR(50));'\npsql -h \"$(ip route | grep 'kernel scope link' | awk '{print $9}')\" -p 5432 -U postgres -d testdb -c \"INSERT INTO testtable (name) VALUES ('testname');\"\npsql -h \"$(ip route | grep 'kernel scope link' | awk '{print $9}')\" -p 5432 -U postgres -d testdb -c 'SELECT * FROM testtable;'\n\n# Bounce the server to create another snapshot with the new db, table, and one testname record\nsystemctl --user restart test-postgresql.service\ntestname_snapshot=$(zfs list -t snapshot testing/postgres | grep -v NAME | awk '{print $1}' | tail -n 1)\necho \"Latest snapshot with only testable: $testname_snapshot\"\nsleep 5\n\n# Verify the data is still there and add another name, anthony\npsql -h \"$(ip route | grep 'kernel scope link' | awk '{print $9}')\" -p 5432 -U postgres -d testdb -c 'SELECT * FROM testtable;' \npsql -h \"$(ip route | grep 'kernel scope link' | awk '{print $9}')\" -p 5432 -U postgres -d testdb -c \"INSERT INTO testtable (name) VALUES ('anthony');\"\npsql -h \"$(ip route | grep 'kernel scope link' | awk '{print $9}')\" -p 5432 -U postgres -d testdb -c 'SELECT * FROM testtable;'\n\n# Bounce the server to create another snapshot with the new data\nsystemctl --user restart test-postgresql.service\nanthony_snapshot=$(zfs list -t snapshot testing/postgres | grep -v NAME | awk '{print $1}' | tail -n 1)\necho \"Latest snapshot with anthony: $anthony_snapshot\"\nsleep 5\n\npsql -h \"$(ip route | grep 'kernel scope link' | awk '{print $9}')\" -p 5432 -U postgres -d testdb -c 'SELECT * FROM testtable;'\n# Delete the testname entry\npsql -h \"$(ip route | grep 'kernel scope link' | awk '{print $9}')\" -p 5432 -U postgres -d testdb -c \"DELETE FROM testtable WHERE name = 'testname';\"\n# Verify the deletion\npsql -h \"$(ip route | grep 'kernel scope link' | awk '{print $9}')\" -p 5432 -U postgres -d testdb -c 'SELECT * FROM testtable;'\n\n# Shutdown the server to rollback\nsystemctl --user stop test-postgresql.service\nno_testname_snapshot=$(zfs list -t snapshot testing/postgres | grep -v NAME | awk '{print $1}' | tail -n 1)\necho \"Last snapshot without testname: $no_testname_snapshot\"\n</code></pre> <p>I know that's a long block of code but it creates a database, a table, then inserts two rows, and deletes one row. It creates snapshots at each step. You can see how the data changes over time with each snapshot. The code block also saves each snapshot name into variables for later use to experiment with rollbacks. Do not close the lingeruser shell yet so we can use those values to rollback.</p> <p>Let's try reverting to a previous snapshot to see the table revert to its earlier state. However, OpenZFS's delegation model has another shortcoming that bites us yet again: rollback delegation is not implemented. So to rollback we have to escalate privileges with the <code>vagrant</code> user via sudo. We already shut the server down at the end of the code block so we can rollback without corrupting the database.</p> <p>WARNING: Never roll back a dataset while it's mounted and in use. Always stop the container first to unmount the volume cleanly. This is especially important with databases that can be corrupted when the underlying disk changes leaving them in inconsistent states.</p> <p>Let's go back to the <code>vagrant</code> user's shell to elevate privileges for these rollback operations:</p> <pre><code>sudo zfs rollback -r \"[copy the anthony_snapshot variable's value from the lingeruser shell here]\"\n</code></pre> <p>Switching user shells we cannot just echo the $anthony_snapshot variable since it's not defined in the vagrant user's context. Just re-echo the value and copy it from within the lingeruser's shell. Now let's restart the PostgreSQL container and verify the data in the table:</p> <pre><code>systemctl --user start test-postgresql.service\nexport PGPASSWORD=password\npsql -h \"$(ip route | grep 'kernel scope link' | awk '{print $9}')\" -p 5432 -U postgres -d testdb -c 'SELECT * FROM testtable;'\n</code></pre> <p>Now you should see the previously deleted 'testname', and the 'anthony' entry present again since we rolled back to the snapshot taken before 'testname' was deleted towards the end of the code block. It was as if the delete of the <code>testname</code> entry never happened. Amazing!</p> <p>Now try rolling back successively further in time to snapshots taken before 'anthony' was added on your own. This will solidify your understanding of whats going on here. Just remember, we captured the past snapshot values in the <code>testname_snapshot</code> and <code>before_testdb_snapshot</code> variables in the <code>lingeruser</code> shell. Don't forget to shutdown the database in the vagrant user shell before each rollback.</p>"},{"location":"labs/3.%20volumes/#why-didnt-you-use-zfs-helper-for-rollbacks","title":"Why didn't you use zfs-helper for rollbacks?","text":"<p>The <code>zfs-helper</code> is explicitly designed to avoid delegated calls from the command line or elsewhere. It ONLY services requests from systemd unit services and ties authorization to systemd unit identity and kernel authenticated unix socket clients. These facilities work together to provide a secure and auditable mechanism for delegated management of ZFS datasets.</p>"},{"location":"labs/3.%20volumes/#how-does-it-work","title":"How does it work?","text":"<p>Systemd services have well-defined identities and can be authenticated and authorized based on their unit names with secure cgroup checks. These checks connects <code>zfs-helper</code> policies with systemd units to enforce strict access controls for zfs management operations on a per dataset (noun), per user (subject), and per operation (verb) basis.</p> <p>During each <code>zfs-helperctl</code> call it reads <code>/proc/self/cgroup</code>, maps that back to the calling unit name, and refuses requests unless that unit appears in the unit allow-list (the <code>units.list</code> you maintain under <code>/etc/zfs-helper/policy.d/</code>). That cgroup check is why you can list allowed datasets per operation and know the request actually came from the expected quadlet rather than an arbitrary process the user might spawn.</p> <p>A few tightly stitched mechanisms work together to provide additional security. Unix sockets, <code>AF_UNIX</code>, and its <code>SO_PEERCRED</code> options allow the <code>zfs-helper</code> to retrieve the UID/GID/PID credentials of the calling service. The kernel verifies the identity of the socket client. The server socket is created by systemd when the <code>zfs-helper</code> service starts and is under its control. Only users in the <code>zfshelper</code> system group can access this socket. Group access further ensures that ONLY authorized users can communicate with the <code>zfs-helper</code> service.</p> <p>In summary, when a service connects to this socket to make a request, the kernel provides the credentials of the calling process through the socket. An additional systemd user cgroup check ensures that the calling service is running within the correct user and systemd context. The allow lists control what operations can be performed on which datasets by which services. This multi-layered approach ensures that only authorized services can perform specific ZFS operations on designated datasets, providing a robust security model for delegated ZFS management.</p>"},{"location":"labs/3.%20volumes/#cant-a-compromised-unprivileged-account-create-a-service-with-the-same-name","title":"Can't a compromised unprivileged account create a service with the same name?","text":"<p>If an attacker only gets shell access as a user who happens to be in the <code>zfshelper</code> group, the cgroup gate keeps them from just running <code>zfs-helperctl</code> by hand\u2014their shell sits in a session cgroup, not the whitelisted service cgroup, so the helper denies the request. This blocks one of the easiest privilege-abuse paths and gives you auditability (\u201cwe know postgres-data-volume.service asked for that snapshot\u201d). Another reason why a separate volume quadlet as a separate service is a good security practice.</p> <p>A compromised account can still try to install a brand new user service, but it won't match anything in <code>units.list</code>, so the helper still rejects those calls. To succeed they'd have to hijack an already-authorized unit name; at that point they can trigger whatever operations you've granted to that unit (destroy snapshots, etc.), but only on the datasets/verbs you explicitly allowed. They cannot grant themselves access to other datasets or new verbs without compromising the root-maintained policy files.</p> <p>Bottom line: membership in the <code>zfshelper</code> group is powerful and should be restricted, yet the cgroup check sharply limits post-compromise blast radius to the exact dataset+operation pairs you've whitelisted. The helper doesn't stop a user from abusing the privileges you intentionally delegated, so continue to scope those policy entries tightly and monitor the corresponding services' behavior.</p>"},{"location":"labs/3.%20volumes/#6-using-bro-commands","title":"6. Using Bro Commands","text":"<p>Now let's see how we can do all these things EZ PZ with Brothaman's <code>bro-volume</code> command. First, create a new postgres user with <code>bro-user</code> and a postgresql container quadlet:</p> <pre><code>sudo bro-user --remove postgres\nsudo bro-user postgres\nsudo -iu postgres podman run --rm quay.io/podman/hello\nsudo -iu postgres mkdir -p ~postgres/.config/containers/systemd/\ncat &lt;&lt;'EOF' | sudo -iu postgres tee ~postgres/.config/containers/systemd/postgresql.container\n[Unit]\nDescription=PostgreSQL Container\n\n[Container]\nImage=docker.io/library/postgres:16\nContainerName=postgresql\nEnvironment=POSTGRES_PASSWORD=password\nPublishPort=5432:5432\nNetwork=none\n\n[Service]\nRestart=always\nTimeoutStartSec=120\nStartLimitBurst=5\nRestartSec=10s\n\n[Install]\nWantedBy=default.target\nEOF\n\nsudo -iu postgres systemctl --user daemon-reload\nsudo -iu postgres systemctl --user start postgresql.service\nsudo -iu postgres systemctl --user status postgresql.service -l --no-pager\n\nexport PGPASSWORD=password\npsql -h \"$(ip route | grep 'kernel scope link' | awk '{print $9}')\" -p 5432 -U postgres -c '\\l'\n</code></pre> <p>All good right? Now create a ZFS-backed volume quadlet for the postgres user with <code>bro-volume</code>:</p> <pre><code>sudo bro-volume --name postgresql-data --owner postgres --container postgresql --container-path /var/lib/postgresql/data\nsudo bro-volume --name postgresql-config --owner postgres --container postgresql --container-path /etc/postgresql\nsudo -iu postgres systemctl --user daemon-reload\nsudo -iu postgres podman volume list\nsudo -iu postgres systemctl --user status postgresql-data-volume.service -l --no-pager\nsudo -iu postgres systemctl --user status postgresql-config-volume.service -l --no-pager\nsudo -iu postgres systemctl --user restart postgresql.service\n\nsudo -iu postgres podman volume list\nsudo -iu postgres systemctl --user status postgresql-data-volume.service -l --no-pager\nsudo -iu postgres systemctl --user status postgresql-config-volume.service -l --no-pager\nsudo -iu root ls -l ~postgres/volumes/postgresql-data\nexport PGPASSWORD=password\npsql -h \"$(ip route | grep 'kernel scope link' | awk '{print $9}')\" -p 5432 -U postgres -c '\\l'\n\nfor restart in $(seq 1 10); do\n  echo \"snapshot with $restart\"\n  sudo -iu postgres systemctl --user restart postgresql.service\n  sleep 5\n  sudo -iu root zfs list -t snapshot testing/var/lib/containers/unprivileged/postgres/volumes/postgresql-data\n  sudo -iu root zfs list -t snapshot testing/var/lib/containers/unprivileged/postgres/volumes/postgresql-config\ndone\n</code></pre> <p>Yup we just felt the awesome modularization potential of quadlets and how <code>bro-volume</code> makes it EZ PZ to create and manage volume quadlets for unprivileged users. The <code>bro-volume</code> command abstracts away the complexity of manually creating volume quadlets, setting up ZFS datasets, and configuring snapshot management. It streamlines the process into a few simple commands, allowing you to focus on using your containers rather than managing their storage intricacies.</p>"},{"location":"labs/3.%20volumes/#7-rollback-optional","title":"7. Rollback (Optional)","text":"<p>When you're done testing, you can revert the VM to the snapshot created before starting this lab in step #0. This operation occurs on the host outside of the VM (not to be confused with ZFS snapshots):</p> <pre><code>vagrant snapshot restore before-volumes\n</code></pre>"},{"location":"labs/3.%20volumes/#lessons-learned","title":"Lessons Learned","text":"<p>We saw how adding a <code>Volume=</code> directive into container quadlets perform a bind mount of an external host directory into the container as a <code>source:destination</code> mapping. Containers like the nginx image implement a common pattern where they \"import\" configurations managed externally in host directories using bind mounts. BTW this was a consequence of how containers were originally used in development. The hosted site content to be served also was shared using such a bind mount.</p> <p>We then moved on to create an actual volume quadlet to manage a persistent ZFS-backed volume for a PostgreSQL container. We saw how ZFS snapshots can be created before starting the container to allow rollbacks in case of corruption. However, we also encountered the limitations of OpenZFS's delegation model that complicate snapshot management for unprivileged users. To solve this, we introduced the <code>zfs-helper</code> service that runs with elevated privileges and performs snapshot management tasks on behalf of unprivileged user-scoped container services based on a defined access policy with authentication.</p> <p>With <code>zfs-helper</code> in place, we updated the volume quadlet to use it for secure yet convenient snapshot creation and deletion. It removed a lot of the pain we experienced while manually managing snapshots. We also implemented automatic snapshot cleanup (retention policy) in the simplest way to maintain a manageable number of snapshots. Finally, we connected the volume quadlet to the PostgreSQL container quadlet and tested data persistence and snapshot rollbacks, demonstrating the effectiveness and power of ZFS snapshots and rollbacks for data protection and instant recovery.</p> <p>Hopefully by now you've started to understand why a separate volume quadlet is a good thing. It allows you to manage the lifecycle of persistent storage independently from the container, share volumes between multiple containers, and leverage advanced features like ZFS snapshots for data protection and recovery. It is also a good security practice. Although we could also do this inside the container quadlet directly, separating concerns into dedicated quadlets leads to cleaner, more secure, and more maintainable configurations.</p>"},{"location":"labs/4.%20networking/","title":"Networking","text":"<p>Although a large part of this lab focuses on systemd socket activation, we will also cover custom Podman networks and inter-container connectivity. The lab builds upon the foundational knowledge from previous labs, particularly the PostgreSQL setup from the Volumes Lab.</p> <p>In this lab you will:</p> <ol> <li>Retrofit the test-postgresql quadlet with systemd socket activation</li> <li>Define a network quadlet for a private network</li> <li>Connect containers to the private network</li> <li>Discuss when to use cross-over ports vs. network quadlets for container communication</li> </ol>"},{"location":"labs/4.%20networking/#0-snapshot","title":"0. Snapshot","text":"<p>Before starting this lab, create a VM snapshot to allow easy rollback in case of issues.</p> <pre><code>vagrant snapshot save networking-start\n</code></pre>"},{"location":"labs/4.%20networking/#1-retrofit-postgresql-with-socket-activation","title":"1. Retrofit PostgreSQL with Socket Activation","text":"<p>The previous Volumes Lab had you create a <code>test-postgresql</code> quadlet that started a PostgreSQL container with a published port using the <code>PublishPort=5432:5432</code> directive. The directive uses Podman's built-in port forwarding to expose the container's PostgreSQL service on the host's port 5432. It works great, however another approach provides for systemd socket activation enabling cross-over dependencies and on-demand triggering.</p> <p>SECURITY NOTE: By default, <code>PublishPort=5432:5432</code> binds to all interfaces (0.0.0.0:5432), making the service accessible from any network interface on the host. For security, you can bind to specific IP addresses: * <code>PublishPort=127.0.0.1:5432:5432</code> - localhost only (most secure) * <code>PublishPort=192.168.1.100:5432:5432</code> - specific IP address only * <code>PublishPort=[::1]:5432:5432</code> - IPv6 localhost only</p> <p>Note: Podman does not support binding to interface names (like <code>eth0</code>). You must use specific IP addresses. To find interface IP addresses, use <code>ip addr show</code> or <code>hostname -I</code>.</p> <p>This interface binding control that <code>PublishPort=</code> has is also available in systemd socket activation, where the socket unit determines the listening interface and port. Try to restrict as much as possible instead of binding to every interface.</p> <p>In this lab section, you will modify that quadlet to use systemd socket activation with <code>systemd-socket-proxyd</code> instead of Podman's proxy infrastructure with its <code>PublishPort</code> directive. This change could improve resource utilization using on-demand triggering, starting the PostgreSQL container only with incoming connections. This involves creating a <code>.socket</code> unit that listens on the desired port and a corresponding <code>.service</code> unit that starts the container when a connection is made.</p>"},{"location":"labs/4.%20networking/#11-motivation","title":"1.1 Motivation","text":"<p>Now that's tightly packed so let's break it down. In systemd, user-scoped services running under unprivileged user accounts cannot use dependency directives like <code>After=</code> or <code>Requires=</code> to depend on user-scoped services running under other unprivileged user accounts. By \"cross-over\" we mean across systemd scope boundaries which are security boundaries, essentially across different unprivileged user accounts. The impact of these limitations is that other containers depending on the <code>test-postgresql</code> quadlet cannot express a dependency on it when running under a different unprivileged user account.</p> <p>NOTE: User-scoped services can however depend on system-wide \"like\" (user scoped pseudo) targets like <code>networking-online.target</code> or <code>default.target</code> in <code>WantedBy=</code> directives. </p> <p>User scope cross-over dependencies between unprivileged users are not supported, but we can work around this limitation using systemd socket activation while also benefiting from on-demand activation. This is a powerful primitive that allows services to start on-demand when a connection is made to a listening socket. There are also ways to shut down services that are not being used and have been idle for some time. By using socket activation, other containers can depend on an active socket instead of a service unit, allowing for cross-over dependencies to actually work.</p>"},{"location":"labs/4.%20networking/#12-background","title":"1.2 Background","text":"<p>Systemd socket activation allows services to start on-demand, even shutting them down after being idle to reduce resource usage. A socket unit listens for incoming connections and activates the service unit when a connection is received, the <code>accept()</code> call. The mechanism is particularly useful for containers, as it allows them to remain stopped until they're needed.</p> <p>NOTE: On-demand triggering may introduce slight latency on the first connection as the container starts up. However, subsequent connections will be fast as the container remains running until explicitly stopped. Measures can be taken to shutdown the container after a period of inactivity if desired too. This makes sense in the case of large numbers of infrequently used services. In general though, the kernel is very efficient at managing idle resources across processes and those running in containers, so the benefits may be marginal. KVM VMs benefit more from being stopped when idle, because the Linux kernel cannot reclaim the guest's memory while it's running.</p> <p>It works by having the socket unit listen on a specified port (e.g., 5432 for PostgreSQL). When a client connects, systemd starts the associated service unit, which in turn starts the container then passes the file descriptors of the socket through the service to the container so it can handle the incoming connection. The socket hand off mechanism is efficient with near native performance. However the socket handoff requires support for systemd's socket passing mechanism. Although many services support socket activation natively, many do not. For those that do not, like PostgreSQL, we can use <code>systemd-socket-proxyd</code> as a proxy to forward connections from the host to the container's internal network namespace.</p> <p>Handing off the listening socket to the container is achieved using <code>systemd-socket-proxyd</code>, as intermediary, which forwards connections from the host to the container's internal network namespace. The extra proxying effort adds minimal latency while enabling socket activation for services that do not natively support it.</p>"},{"location":"labs/4.%20networking/#socket-descriptor-passing-support","title":"Socket Descriptor Passing Support","text":"<p>Podman added socket file descriptor passing support, see https://podman.io/blogs/2022/09/30/podman-4.0.html#socket-activation, allowing containers to receive incoming connections directly through the host's socket file descriptors. There's a long chain of forking involved to get the socket file descriptors to the container's service which must support systemd's socket descriptor passing. This support just added it to Podman and OCI containers which separately added support too. It does not include the network service wrapped in the container. There are two problems with this approach:</p> <ol> <li>Some key services do not natively support systemd socket descriptor passing, including PostgreSQL.</li> <li>Container image customizations are required to handle the socket passing mechanism: just see what it takes to make an echo service in an <code>echo.container</code> work with socket passing in the Podman documentation.</li> </ol> <p>This feels way too involved for most use cases. Instead, Brothaman uses <code>systemd-socket-proxyd</code> as a workaround for all network services, especially ones that DO NOT natively support socket descriptor passing. The proxy runs inside the container's network namespace and forwards connections from the host's service socket to the container's service port bound usually to the loopback. The configuration is straightforward, more secure, and works with any container image without modification.</p> <p>Once set up, the socket activation mechanism works as follows:</p> <ol> <li>The socket unit is started and listens on the host's port (e.g., 0.0.0.0:5432 for PostgreSQL)</li> <li>When a client connects (accept()), systemd activates the service unit passing it the socket file descriptors</li> <li>The service unit starts the container if it's not already running</li> <li>The service unit starts <code>systemd-socket-proxyd</code> inside the container's network namespace using <code>bro-helper</code>, forwarding connection traffic to the container's internal service port (e.g., 127.0.0.1:5432 for PostgreSQL)</li> <li>The container's network service (in this case PostgreSQL) handles the incoming connection</li> </ol> <p>Socket passing concerns are all handled by the <code>systemd-socket-proxyd</code> proxy, allowing the container's service to operate normally without any special socket descriptor handling considerations. There is a minimal performance cost for the proxy as with the PublishPort feature of Podman, but we're more than willing to bear it for the on-demand activation, simplicity of use, and compatibility with any container image that it provides.</p> <p>Brothaman's <code>bro-activate</code> command creates activator socket and service pairs as add-on infrastructure to enable any container quadlet to use socket activation with minimal effort. So say for a test-postgresql.container quadlet, it can create a <code>test-postgresql-activator.socket</code> and <code>test-postgresql-activator.service</code> pair that listens on port 5432 and starts the <code>test-postgresql.container</code> quadlet when a connection is made.</p> <p>Furthermore, it is fully compatible with cross-over dependencies between unprivileged user accounts regardless of the container and whether or not its service supports socket descriptor passing or not. For example, I can have two separate <code>bro-user</code> created <code>gitea</code> and <code>postgres</code> unprivileged user accounts, with a gitea container running gitea yet hitting the host port of the postgresql service running under the postgres unprivileged user account. Here the postgresql server be triggered to start on-demand via socket activation and both the gitea container and postgresql server can run under separate unprivileged user accounts. Heck I can even socket activate the gitea container's web port so it too fires up on-demand when a web request comes in!</p> <p>CONTRIBUTE: Anyone interested in adding a <code>bro-activate-direct</code> command to use Podman's native socket descriptor passing support directly without the <code>systemd-socket-proxyd</code> service proxying traffic is welcome to contribute it! A good benefit of this is that it avoids having an extra service and an extra hop which proxies traffic. Without this direct option we chose convenience over performance.</p>"},{"location":"labs/4.%20networking/#tcpudp-sockets-vs-unix-domain-sockets","title":"TCP/UDP Sockets vs. Unix Domain Sockets","text":"<p>Systemd socket activation can use TCP or UDP sockets as well as Unix Domain Sockets (UDS). Both activation mechanisms listen for connections: one works on remotely accessible network ports, whereas Unix Domain Sockets (UDS) are file-based pipes used for inter-process communication on the same host. UDS can also be used for cross-over dependencies between different user accounts, but those accounts must be on the same host. Also direct UDS file access with the right permissions is required: very doable using a common group.</p> <p>TCP/UDP sockets, on the other hand, used in systemd socket activation listen on network ports which can be accessed both locally and remotely to activate services on-demand. No file access permissions are need for it to work. This is particularly useful for other remote services to depend on TCP activated services over the network. Unlike UDS, TCP/UDP sockets do not require direct file access permissions. Remote access makes them more flexible for cross-over dependencies yet keep in mind this occurs at the price of greater risk since services are now exposed over the network. Proper firewalling and security measures should be in place to protect these services from unauthorized access.</p>"},{"location":"labs/4.%20networking/#hint-proxying-from-any-interface-to-any-other-interface","title":"Hint: Proxying from any interface to any other interface","text":"<p><code>systemd-socket-proxyd</code> can proxy from any host specific interface (not just <code>0.0.0.0</code>) to any container interface, not just its loopback, <code>127.0.0.1</code>. This includes Podman created networks. So, it could proxy from a container to a dedicated inter-container network bridge. It connects containers to networks in general, while also activating those same containers on-demand. The same mechanism enables connecting containers to inter-container network quadlets (network definitions), not just a particular host interfaces.</p>"},{"location":"labs/4.%20networking/#13-create-the-postgresql-activator-service-unit","title":"1.3 Create the PostgreSQL Activator Service Unit","text":"<p>Before creating the socket unit, we need to create the corresponding service unit that will be activated when a connection is made to the socket.</p> <p>When this service unit is activated it activates the <code>test-postgresql.service</code> quadlet if it is not already running. Once the container is running, the service unit retrieves the container's PID using <code>podman inspect</code> and then starts <code>systemd-socket-proxyd</code> inside the container's network namespace using <code>bro-helper</code>.</p> <p>In a <code>lingeruser</code> shell, a new file named <code>postgresql-activator.service</code> is created in the systemd user directory with the following content:</p> <pre><code>systemctl --user disable test-postgresql-activator.service\nmkdir -p ~/.config/systemd/user/\ncat &lt;&lt;EOF &gt; ~/.config/systemd/user/test-postgresql-activator.service\n[Unit]\nDescription=Test PostgreSQL Activator Service\nRequires=test-postgresql-activator.socket test-postgresql.service\nAfter=test-postgresql-activator.socket test-postgresql.service\n\n[Service]\nType=simple\n\n# Create the environment file for the proxy helper with the target container PID\nExecStartPre=/usr/bin/env sh -c \"/usr/bin/podman inspect -f 'TARGET_PID={{.State.Pid}}' test-postgresql &gt; %t/test-postgresql-proxyd.env\"\n\n# Environment variable file for the proxy helper does not have to exist with the dash\nEnvironmentFile=-%t/test-postgresql-proxyd.env\n\n# Wait until Postgres is accepting connections inside its netns\nExecStartPre=/usr/bin/env sh -c \"i=0; while [ \\$i -lt 20 ]; do \\\n  /usr/bin/podman unshare nsenter -t \\\"\\$TARGET_PID\\\" -n \\\n    pg_isready -h 127.0.0.1 -p 5432 &amp;&amp; exit 0; \\\n  i=\\$((i+1)); sleep 1; \\\ndone; exit 1\"\n\n# Key is the use of exec to replace the shell with podman process\n# Use podman unshare to enter user namespace, then bro-helper for network namespace\nExecStart=/usr/bin/env sh -c 'exec podman unshare /usr/local/bin/bro-helper --pid \"\\${TARGET_PID}\" -- /lib/systemd/systemd-socket-proxyd 127.0.0.1:5432'\n\nNoNewPrivileges=no\nPrivateTmp=yes\nProtectHome=yes\nProtectSystem=full\n\nRestart=on-failure\nRestartSec=5s\nTimeoutStopSec=120\n\n[Install]\nWantedBy=default.target\nEOF\n\nsystemctl --user daemon-reload\nsystemctl --user enable test-postgresql-activator.service\n</code></pre> <p>Let's break down each of the significant directives in this service unit:</p> <ul> <li><code>Requires=</code> and <code>After=</code>: Ensures that the socket unit and the PostgreSQL container service are started before this service can be activated.</li> <li><code>ExecStartPre=</code>: Several pre-start commands are used to gather the target container's PID and verify that PostgreSQL is ready to accept connections which makes the system much more robust.</li> <li><code>ExecStart=</code>: The main command starts <code>systemd-socket-proxyd</code> inside the container's network namespace using <code>bro-helper</code>.</li> <li><code>NoNewPrivileges</code>, <code>PrivateTmp</code>, <code>ProtectHome</code>, and <code>ProtectSystem</code>: Security directives to limit the service's privileges and access.</li> <li><code>Restart=</code> and <code>RestartSec=</code>: Configures the service to restart on failure.</li> </ul>"},{"location":"labs/4.%20networking/#14-create-the-socket-unit","title":"1.4 Create the Socket Unit","text":"<p>The <code>postgresql-activator.service</code> above is activated by the <code>postgresql-activator.socket</code> unit. This socket unit listens on port 5432 and triggers the service unit when a connection is made. In a <code>lingeruser</code> shell create a new file named <code>postgresql-activator.socket</code> in the systemd user directory with the following content:</p> <pre><code>systemctl --user stop test-postgresql.service\nmkdir -p ~/.config/systemd/user/\ncat &lt;&lt;EOF &gt; ~/.config/systemd/user/test-postgresql-activator.socket\n[Unit]\nDescription=PostgreSQL Activator Socket\n\n[Socket]\nListenStream=5432\nNoDelay=true\nReusePort=true\nBacklog=128\n\n[Install]\nWantedBy=default.target\nEOF\n\nsystemctl --user daemon-reload\nsystemctl --user enable test-postgresql-activator.socket\nsystemctl --user restart test-postgresql-activator.socket\n</code></pre> <p>The socket is now set up to listen on port 5432. When a connection is made, it will trigger the associated service unit with the same name. The socket unit configuration includes directives:</p> <ul> <li><code>ListenStream=5432</code>: Listens on TCP port 5432 for PostgreSQL (all interfaces)</li> <li><code>NoDelay=true</code>: Disables Nagle's algorithm for lower latency</li> <li><code>ReusePort=true</code>: Allows multiple sockets to bind to the same port</li> <li><code>Backlog=128</code>: Sets the maximum number of pending connections</li> </ul> <p>INTERFACE BINDING: Like <code>PublishPort=</code>, systemd socket units bind to all interfaces by default. For security, you can specify: * <code>ListenStream=127.0.0.1:5432</code> - localhost only * <code>ListenStream=192.168.1.100:5432</code> - specific IP * <code>ListenStream=[::1]:5432</code> - IPv6 localhost</p> <p>Multiple <code>ListenStream=</code> directives can bind to different interfaces simultaneously.</p> <p>In the beginning before anything occurs, the only enabled and started unit is the socket unit (postgresql-activator.socket).</p> <p>So what happens when a connection is made to the socket: i.e. with the psql client?</p> <pre><code>export PGPASSWORD=password\npsql -h localhost -U postgres\n</code></pre> <p>When a connection is made to port 5432, i.e. with psql, the socket unit activates its (same name) service unit: <code>postgresql-activator.service</code>. Activating <code>postgresql-activator.service</code> in turn activates <code>test-postgresql.service</code>. Once the container is up, as detected by looping checks in an <code>ExecStartPre=</code> directive's shell command, the container's PID is retrieved using <code>podman inspect</code>. Finally, <code>systemd-socket-proxyd</code> is started inside the PostgreSQL container's network namespace using <code>bro-helper</code>. This setup allows incoming connections on port 5432 to be proxied directly (in one hop) to the PostgreSQL service running inside the container.</p> <p>It's essentially a substitute for Podman's built-in port forwarding mechanism, but with the added benefits of on-demand socket activation which allows us to implement cross-over dependency triggering.</p>"},{"location":"labs/4.%20networking/#15-update-the-test-postgresql-quadlet","title":"1.5 Update the test-postgresql quadlet","text":"<p>Now we do not need Podman's built-in port forwarding anymore since systemd socket activation is handling incoming connections. Below we remove the <code>PublishPort=</code> directive from the <code>test-postgresql.container</code> quadlet. Also note we're not starting the service as usual after updating the quadlet and reloading. It will now be started by the activator service and its socket. Update the <code>test-postgresql.container</code> quadlet as follows: </p> <pre><code>if systemctl --user is-active test-postgresql.service; then\n  systemctl --user stop test-postgresql.service\nfi\nmkdir -p ~/.config/containers/systemd/\ncat &gt; ~/.config/containers/systemd/test-postgresql.container &lt;&lt;'EOF'\n[Unit]\nDescription=PostgreSQL Container\nPropagatesStopTo=postgres-data-volume.service\nBindsTo=postgres-data-volume.service\nAfter=postgres-data-volume.service\n\n[Container]\nImage=postgres:16\nContainerName=test-postgresql\nEnvironment=POSTGRES_PASSWORD=password\nVolume=/home/lingeruser/postgres:/var/lib/postgresql/data\nNetwork=none\n\n[Service]\nRestart=always\nTimeoutStartSec=300\nStartLimitBurst=5\nRestartSec=10s\n\n[Install]\nWantedBy=default.target\nEOF\n\nsystemctl --user daemon-reload\n</code></pre>"},{"location":"labs/4.%20networking/#16-validate-socket-activation","title":"1.6 Validate Socket Activation","text":"<p>Now that everything is set up, we can validate that the socket activation is working as expected. Remember we stopped the <code>test-postgresql.service</code> above, so it should not be running yet. Check the status of the socket and service units:</p> <pre><code>systemctl --user is-active test-postgresql.service\nsystemctl --user is-active test-postgresql-activator.socket\nsystemctl --user is-active test-postgresql-activator.service\n</code></pre> <p>Basically the <code>test-postgresql.service</code> should be inactive, the <code>test-postgresql-activator.socket</code> should be active, and the <code>test-postgresql-activator.service</code> should be inactive. Now try connecting to PostgreSQL using the <code>psql</code> client:</p> <pre><code>export PGPASSWORD=password\npsql -h localhost -U postgres\n# Try running a simple SQL command, e.g.: SELECT version();\n</code></pre> <p>The first connection attempt may take a few seconds as the container starts up and is fully available online. Once connected, you should be able to run SQL commands as usual. After disconnecting, check the status of the units again:</p> <pre><code>systemctl --user is-active test-postgresql.service\nsystemctl --user is-active test-postgresql-activator.socket\nsystemctl --user is-active test-postgresql-activator.service\n</code></pre> <p>All three units should now be active, indicating that the PostgreSQL container is running and ready to accept connections.</p>"},{"location":"labs/4.%20networking/#17-use-bro-activator-to-connect-and-activate-containers-over-the-network","title":"1.7 Use bro-activator to connect and activate containers over the network","text":"<p>WIP</p>"},{"location":"labs/4.%20networking/#20-define-a-network-quadlet","title":"2.0 Define a network quadlet","text":"<p>Again as the <code>lingeruser</code> run the following commands to create a network quadlet named <code>test.network</code> that generates a Podman network service:</p> <pre><code>mkdir -p ~/.config/containers/systemd/\ncat &gt; ~/.config/containers/systemd/test.network &lt;&lt;'EOF'\n[Unit]\nDescription=Test Network Quadlet\nAfter=network-online.target\n\n[Network]\nSubnet=192.168.100.0/24\nGateway=192.168.100.1\nDNS=192.168.100.1\n\n[Install]\nWantedBy=default.target\nEOF\nsystemctl --user daemon-reload\nsystemctl --user start test-network.service\nsystemctl --user status test-network.service -l --no-pager\npodman network list\npodman network inspect systemd-test\nif ping -W 0.5 -c 1 192.168.100.1 &gt; /dev/null; then\n  echo \"Ping to gateway 192.168.100.1 successful\"\nelse\n  echo \"Ping to gateway 192.168.100.1 failed\"\nfi\n</code></pre> <p>The quadlet creates a private bridge network with the specified subnet, gateway, and DNS settings in its own network namespace owned by the unprivileged user. Commands like the <code>ping</code> above and <code>brctl show</code> on the host's network namespace will NOT display the bridge. You can see for yourself by installing <code>bridge-utils</code> as the <code>vagrant</code> user if needed and try for yourself:</p> <pre><code>sudo apt-get install -y bridge-utils\nsudo brctl show\n</code></pre> <p>We can use the following command to switch to the unprivileged user's network namespace and see the bridge interface created by Podman for this network:</p> <pre><code>podman unshare --rootless-netns /usr/sbin/brctl show\n</code></pre> <p>The rootless-netns option allows us to enter the unprivileged user's network namespace where the Podman networks are created.</p> <p>But wait, you see nothing right? Well this was to show you that Podman does not activate the bridge until a container is connected to it. This could produce some gotchyas situations so we wanted to point this out. Try again after connecting a container to the network in the next major section.</p>"},{"location":"labs/4.%20networking/#30-connect-containers-to-the-network","title":"3.0 Connect containers to the network","text":"<p>This is great but we need to connect the container onto this network. Modify the <code>test-postgresql.container</code> quadlet adding <code>Network=test.network</code> to connect it to the <code>test</code> network as follows:</p> <pre><code>if systemctl --user is-active test-postgresql.service; then\n  systemctl --user stop test-postgresql.service\nfi\nmkdir -p ~/.config/containers/systemd/\ncat &gt; ~/.config/containers/systemd/test-postgresql.container &lt;&lt;'EOF'\n[Unit]\nDescription=PostgreSQL Container\nPropagatesStopTo=postgres-data-volume.service\nBindsTo=postgres-data-volume.service\nAfter=postgres-data-volume.service\n\n[Container]\nImage=postgres:16\nContainerName=test-postgresql\nEnvironment=POSTGRES_PASSWORD=password\nVolume=/home/lingeruser/postgres:/var/lib/postgresql/data\nNetwork=test.network\n\n[Service]\nRestart=always\nTimeoutStartSec=300\nStartLimitBurst=5\nRestartSec=10s\n\n[Install]\nWantedBy=default.target\nEOF\nsystemctl --user daemon-reload\nsystemctl --user start test-postgresql.service\nsystemctl --user status test-postgresql.service -l --no-pager\n</code></pre>"},{"location":"labs/4.%20networking/#31-verify-bridge-activation","title":"3.1 Verify bridge activation","text":"<p>Let's try that bridge lookup and ping the gateway again to verify the bridge is now active in the unprivileged user's (rootless) network namespace:</p> <pre><code>podman unshare --rootless-netns ip addr show\npodman unshare --rootless-netns /usr/sbin/brctl show\n# Uses the ping program inside the host but executes in the rootless netns\nif podman unshare --rootless-netns ping -W 0.5 -c 1 192.168.100.1 &gt; /dev/null; then\n  echo \"Ping to gateway 192.168.100.1 successful\"\nelse\n  echo \"Ping to gateway 192.168.100.1 failed\"\nfi\n</code></pre> <p>You'll see the <code>podman1</code> bridge interface and the ping to the gateway is successful from within the rootless network namespace. The <code>test-postgresql</code> container has its own namespace but the veth device connects them together. The veth device type is a virtual Ethernet pair that connects two network namespaces. One end of the veth pair is placed inside the container's network namespace, while the other end is attached to the bridge in the rootless network namespace. This setup allows network traffic to flow between the container and the bridge, enabling communication with other containers connected to the same bridge network as well as external networks through NAT. Let's demonstrate this clearly in the next section.</p>"},{"location":"labs/4.%20networking/#32-verify-network-namespaces-and-their-isolated-views","title":"3.2 Verify network namespaces and their isolated views","text":"<p>Let's print out the namespace inodes for the host, container, and rootless network namespace to see just how they are all different:</p> <pre><code># Container has its own network namespace (anonymous, not named)\n# Compare namespace inodes to verify isolation\necho \"Host netns inode: $(stat -c %i /proc/self/ns/net)\"\necho \"Container netns inode: $(podman exec test-postgresql stat -c %i /proc/self/ns/net)\"\necho \"Rootless netns inode: $(podman unshare --rootless-netns stat -c %i /proc/self/ns/net)\"\n</code></pre> <p>Notice that all network namespace inodes are different. Now let's look at the isolation this achieved. First let's look at the host's network namespace view of interfaces and bridges:</p> <pre><code>ip link show\n/usr/sbin/brctl show\n</code></pre> <p>You should see the <code>lo</code> loopback and your main network interface (e.g., <code>eth0</code> or <code>ens3</code>), but you will NOT see the Podman created bridge or veth endpoints. This is because the Podman networks are created in the unprivileged user's rootless network namespace, not the host's network namespace.</p> <p>Now let's look at the rootless (unprivileged user's) network namespace view of interfaces and bridges:</p> <pre><code># Rootless namespace can see the bridge and veth endpoints\npodman unshare --rootless-netns ip link show\npodman unshare --rootless-netns /usr/sbin/brctl show\n</code></pre> <p>Here we see the <code>podman1</code> bridge interface and the veth endpoint connected to it, including the one connected to the <code>test-postgresql</code> container. This demonstrates the isolation provided by separate network namespaces while still allowing connectivity through the veth pair that acts as a tunnel through the namespaces.</p> <p>Now let's look at the <code>test-postgresql</code> container's network namespace view of interfaces:</p> <pre><code># Container namespace can see its own veth endpoint only\npodman exec test-postgresql bash -c '\n  NONINTERACTIVE=1 apt-get update &amp;&amp;\n  NONINTERACTIVE=1 apt-get install -y bridge-utils'\npodman exec test-postgresql ip link show\npodman exec test-postgresql /usr/sbin/brctl show\n</code></pre> <p>Unlike the rootless network namespace, the container's network namespace cannot see the bridge or other veth endpoints. It can only see its own veth endpoint (usually named <code>veth0@ifX</code> where X is an number unless renamed). This demonstrates the isolation (different network device views) provided by separate network namespaces while still allowing connectivity through the veth pair that acts as a tunnel through the namespaces. Pretty cool right? This bridge network can now be used to connect multiple containers together on the same network each in their own namespace.</p>"},{"location":"labs/4.%20networking/#33-verify-container-connectivity","title":"3.3 Verify container connectivity","text":"<p>Let's peak inside the container to see its assigned IP address on the network:</p> <pre><code>podman exec -it test-postgresql bash -c '\n  ( NONINTERACTIVE=1 apt-get update &amp;&amp;\n  NONINTERACTIVE=1 apt-get install -y iproute2 inetutils-ping net-tools ) &gt;/dev/null &amp;&amp;\n  ip a s'\nIP=$(podman exec -it test-postgresql hostname -i)\ntrim_string() {\n    : \"${1#\"${1%%[![:space:]]*}\"}\"\n    : \"${_%\"${_##*[![:space:]]}\"}\"\n    printf '%s\\n' \"$_\"\n}\nIP=$(trim_string \"$IP\")\necho \"Container IP address is: $IP\"\nif podman exec -it test-postgresql bash -c 'ping -W 1 -c 1 192.168.100.1 &gt;/dev/null'; then\n  printf \"Ping from container %s to gateway 192.168.100.1 successful\\n\" \"$IP\"\nelse\n  printf \"Ping from container %s to gateway 192.168.100.1 failed\\n\" \"$IP\"\nfi\n</code></pre> <p>There you go! The container has an IP address assigned from the network bridge. Now we were able to ping the gateway from inside the container. In fact, maybe you notice before, we are also routing to the Internet and resolving DNS queries since apt pulled down our packages. This shows that the container is successfully connected to the custom Podman network defined by the <code>test.network</code> quadlet. If you want to disable Internet access you can modify the network quadlet by adding a <code>Internal=true</code> directive and restarting the <code>test-network.service</code>. The <code>Internal=true</code> directive in a Podman network quadlet creates an internal-only network that blocks external internet access while still allowing:</p> <ul> <li>Container-to-container communication within the same network</li> <li>Container-to-host communication (limited through the rootless namespace)</li> </ul> <p>This is useful for isolating containers from the outside world while still allowing them to communicate with each other and the host system.</p>"},{"location":"labs/4.%20networking/#34-ipam-settings","title":"3.4 IPAM Settings","text":"<p>Podman networks use an IPAM (IP Address Management) mechanism, not traditional DHCP. Here's how it works:</p> <ul> <li>Podman uses a static IPAM allocation system</li> <li>IP addresses are assigned sequentially from the subnet range</li> <li>No DHCP daemon running - it's handled by Podman's network stack</li> <li>IP assignments are persistent and deterministic</li> </ul> <p>How IP Assignment Works:</p> <ul> <li>Network Creation: Subnet defined (e.g., 192.168.100.0/24)</li> <li>Container Connection: Podman assigns next available IP from range</li> <li>Static Assignment: IP is statically configured in container's netns</li> <li>Persistence: Same container gets same IP on restart (usually)</li> </ul> <p>IPAM Configuration Options:</p> <pre><code># Network quadlet with IPAM control\n[Network]\nSubnet=192.168.100.0/24\nGateway=192.168.100.1\nIPRange=192.168.100.100-192.168.100.200  # Limit assignable range\n</code></pre> <p>You can also assign specific IPs to existing containers without restarting them:</p> <pre><code># Disconnect from current network first (if already connected)\npodman network disconnect systemd-test test-postgresql || true\n\n# Reconnect with a specific IP - hot assignment\npodman network connect --ip 192.168.100.157 systemd-test test-postgresql\n\n# Verify the new IP assignment\npodman exec test-postgresql hostname -i\n</code></pre> <p>Let's verify the setting by inspecting the network:</p> <pre><code># Extract just the subnet and gateway\npodman network inspect systemd-test | jq -r '.[0].containers[] | select(.name==\"test-postgresql\") | .interfaces.eth0.subnets[0].ipnet'\n\n# Extract container's IP address using jq\npodman inspect test-postgresql | jq -r '.[0].NetworkSettings.Networks.\"systemd-test\".IPAddress'\n</code></pre>"},{"location":"labs/4.%20networking/#35-ping-an-external-ip-from-inside-the-container","title":"3.5 Ping an external IP from inside the container","text":"<p>Let's play with a ping quirk before moving on. Try pinging <code>8.8.8.8</code> from inside the container:</p> <pre><code>if podman exec -it test-postgresql bash -c 'ping -W 1 -c 1 8.8.8.8 &gt;/dev/null'; then\n  echo \"Ping from container to external IP 8.8.8.8 successful\"\nelse\n  echo \"Ping from container to external IP 8.8.8.8 failed\"\nfi\n</code></pre> <p>ICMP ping typically doesn't work from inside rootless Podman containers for several reasons related to unprivileged containers and network namespaces. By default, unprivileged containers lack the necessary capabilities to create raw sockets required for ICMP operations. Additionally, network namespaces isolate the container's network stack from the host, preventing direct access to certain network functionalities. These factors combined result in the inability to successfully ping external IPs like <code>8.8.8.8</code>. Let's change this by changing the <code>net.ipv4.ping_group_range</code> (for UIDs) setting on the host to include the unprivileged user range. As the <code>vagrant</code> user run:</p> <pre><code>cat /proc/sys/net/ipv4/ping_group_range\nsudo sysctl -w net.ipv4.ping_group_range=\"0 2000000\"\ncat /proc/sys/net/ipv4/ping_group_range\n</code></pre> <p>Now <code>sudo su - lingeruser</code> and try the ping again:</p> <pre><code>if podman exec -it test-postgresql bash -c 'ping -W 1 -c 1 8.8.8.8 &gt;/dev/null'; then\n  echo \"Ping from container to external IP 8.8.8.8 successful\"\nelse\n  echo \"Ping from container to external IP 8.8.8.8 failed\"\nfi\n</code></pre> <p>Works!</p>"},{"location":"labs/4.%20networking/#34-connecting-and-disconnecting-containers-from-the-network","title":"3.4 Connecting and disconnecting containers from the network","text":"<p>You can connect and disconnect containers from the network using Podman commands. For example, to disconnect the <code>test-postgresql</code> container from the <code>test</code> network, run:</p> <pre><code>podman exec -it test-postgresql ip a s\npodman network disconnect systemd-test test-postgresql\nif podman exec -it test-postgresql bash -c 'ping -W 1 -c 1 8.8.8.8 &gt;/dev/null'; then\n  echo \"Ping from container to external IP 8.8.8.8 successful\"\nelse\n  echo \"Ping from container to external IP 8.8.8.8 failed\"\nfi\npodman exec -it test-postgresql ip a s\n</code></pre> <p>As you can see the container can no longer ping external IPs since it's disconnected from the network. The veth interface, the tap from the container into the bridge is gone. To reconnect the container to the network, run:</p> <pre><code>podman network connect systemd-test test-postgresql\nif podman exec -it test-postgresql bash -c 'ping -W 1 -c 1 8.8.8.8 &gt;/dev/null'; then\n  echo \"Ping from container to external IP 8.8.8.8 successful\"\nelse\n  echo \"Ping from container to external IP 8.8.8.8 failed\"\nfi\npodman exec -it test-postgresql ip a s\n</code></pre> <p>The container can now ping external IPs again since it's reconnected to the network. The veth interface is back, reconnecting the container to the bridge network.</p>"},{"location":"labs/4.%20networking/#35-network-quadlet-conventions-review","title":"3.5 Network quadlet conventions review","text":"<p>Let's break down network quadlets conventions:</p> <p>The <code>test.network</code> quadlet generates a <code>test-network.service</code> unit. It uses the convention of appending <code>-network</code> to the name before the dot in the quadlet name followed by <code>.service</code> for the service unit name.</p> <p>When listing the network in <code>podman network list</code> its network name is displayed as <code>systemd-test</code>. It uses the convention of taking the quadlet name before the dot and prefixing it with <code>systemd-</code>. If we want to change the listed name we can add a <code>NetworkName=</code> directive to the <code>[Network]</code> section.</p>"},{"location":"labs/4.%20networking/#36-network-quadlets-and-unprivileged-cross-over","title":"3.6 Network quadlets and unprivileged cross-over","text":"<p>By default, containers in separate unprivileged user accounts cannot access Podman network quadlet in other unprivileged user accounts due to them having separate rootless network namespace. One could create veth pairs to attach containers in one user's network namespace to a network in another user's namespace, but this would requires elevated privileges and its not recommended. Doing so goes against the principle of least privilege. The limitation is actually a good security measure. Each unprivileged user account is isolated from others to prevent unauthorized access and maintain system integrity.</p> <p>The best option for cross-over communication between containers owned by different unprivileged users is by way of port mappings via <code>PublishPort=</code> and the socket activation pattern demonstrated in section 1. If containers expose service ports to host interfaces, other unprivileged user accounts can connect to those services over the host's network interfaces. This method still maintains isolation between user accounts while allowing necessary communication between services across user boundaries.</p> <p>Example: If unprivileged user account 'A' runs container 'a' with <code>PublishPort=127.0.0.1:8080:80</code>, then container 'b' running under unprivileged user account 'B' can access container 'a's service by connecting to <code>127.0.0.1:8080</code> on the host's loopback interface. The traffic flows:</p> <pre><code>Container 'b' \u2192 Host loopback (127.0.0.1:8080) \u2192 port mapping \u2192 Container 'a' (port 80)\n</code></pre> <p>This works because:</p> <ul> <li>Both containers can access the host's network interfaces from within their separate network namespaces</li> <li>The host's loopback interface (127.0.0.1) is accessible from any container's network namespace</li> <li>Podman's port mapping bridges the gap between the host interface and the target container's service port</li> <li>No elevated privileges or network namespace bridging required</li> </ul> <p>The same principle works with all public interfaces included (e.g., <code>PublishPort=0.0.0.0:8080:80</code>) for remote access, though localhost binding is more secure and always desired for inter-container communication on the same host.</p> <p>Separate unprivileged user account isolation is important for several reasons:</p> <ol> <li> <p>Security: It helps to prevent unauthorized access to resources and services. By isolating user accounts, we reduce the risk of one user compromising another user's containers or data.</p> </li> <li> <p>Resource Management: Isolating user accounts allows for better resource management and allocation. Each user can have their own set of resources without interfering with others.</p> </li> <li> <p>Stability: If one user's container crashes or experiences issues, it won't affect the containers or services running under other user accounts.</p> </li> <li> <p>Compliance: In multi-tenant environments, isolation is often a requirement for compliance with security policies and regulations.</p> </li> </ol> <p>While it may seem limiting, these isolation measures are crucial for maintaining a secure and stable environment when running containers under unprivileged user accounts.</p>"},{"location":"labs/4.%20networking/#40-when-to-use-cross-over-dependencies-vs-network-quadlets","title":"4.0 When to use cross-over dependencies vs. network quadlets","text":"<p>So, when should cross-over dependencies between unprivileged user accounts be used vs. using a network quadlet to connect multiple containers within a single unprivileged user account? This is a great question. Effectively it is the same as asking, \"When should I bundle containers into the same unprivileged user account vs. when should I separate them into different unprivileged user accounts?\"</p> <p>Surprisingly, the answer is not black or white: it depends on specific use cases, the security posture imposed, and on resource and performance constraints. To speak the same language, let's define some terms first. This is an involved conversation.</p>"},{"location":"labs/4.%20networking/#41-our-own-terminology-definitions","title":"4.1 Our own terminology definitions","text":"<p>NOT EXHAUSTIVE</p>"},{"location":"labs/4.%20networking/#loopback-implications","title":"Loopback Implications","text":"<ul> <li> <p>Directly Exposed [containers|services]: Are containers or services exposing ports to the outside of a common security scope. The common security scope boils down to a network namespace where containers are concerned. What is considered the outside includes the global host's global network namespace including its loopback interface which actually can also be dangerous -- more on that later in this section. Directly exposed containers are typically intended to be accessed by external consumers (usually end users) of the service: like a web application server in a container exposing port 8443 to serve an application to the outside world to use.</p> </li> <li> <p>Indirectly Exposed [containers|services]: Are containers or services not exposing service ports directly, but instead are reachable by other containers (perhaps directly or indirectly) exposed to the outside world: i.e. a database server in a container that only exposes its database service port to a web application container within the same unprivileged user account and users access the application from the network.</p> </li> </ul> <p>The mentality here is, every part of a system, no matter how much, is still exposed to some extent if it is connected or in other words dependency relationships exist. The question is how it is exposed? How easily can attackers compromise it?</p> <p>Directly exposed containers and their services are at greater risk. A slew of network stack based vectors increase their attack surface. Indirectly exposed containers and their services are at lower risk with a smaller attack surface. They can still be compromised since they are reachable through other containers, but attachers have to first compromise the directly exposed containers before they can even attempt to pivot into the indirectly exposed containers which have more constrained interactions with directly exposed containers.</p>"},{"location":"labs/4.%20networking/#42-existing-terminology-adapted-to-our-domain","title":"4.2 Existing terminology adapted to our domain","text":"<p>NOT EXHAUSTIVE</p> <ul> <li> <p>Cross-over dependency: A container in one unprivileged user account depending on a service port exposed by a container in another unprivileged user account. This is typically done using Podman's <code>PublishPort=</code> feature or systemd socket activation to forward traffic from one container to another across user boundaries.</p> </li> <li> <p>Dependency graph: In our context, a set of containers that depend on each other to provide a complete application or service. For example, a web application container that depends on a database container and a cache container.</p> </li> <li> <p>Blast radius: The extent of impact or damage that can occur if a container is compromised. A smaller blast radius means that the compromise is contained within a limited scope, reducing the potential harm to the overall system. Sometimes it helps thinking about this in terms of contagion risk and in terms of concentric circles of impact.</p> </li> <li> <p>Attack surface: The total number of points (or vectors) where an unauthorized user can try to enter or extract data from a system. A larger attack surface means more potential vulnerabilities.</p> </li> <li> <p>Pivoting: The technique used by attackers to move from one compromised system or container to another within a network. This often involves exploiting vulnerabilities in interconnected systems to gain further access.</p> </li> <li> <p>Least privilege principle: A security concept that advocates for granting only the minimum necessary permissions to users or services to perform their tasks, reducing the risk of unauthorized access.</p> </li> </ul>"},{"location":"labs/4.%20networking/#43-contagion-risk","title":"4.3 Contagion Risk","text":"<p>From a security perspective containers depending on each other (an app to its database and cache server for example) represent a single combined attack surface together. This is because a request entering a container from the outside can still exploit vulnerabilities to pivot into potentially all its dependent containers. These containers form a dependency graph where attackers can penetrate and compromise each node in the entire dependency graph.</p> <p>These dependencies and the contagion risks they give rise to are due to one container requiring access to another container's service ports through network communication: i.e. the web application container needs to connect to the database container over the database's service port. They're all exposed, some are directly exposed while others are indirectly exposed. All containers in the dependency graph are exposed to a degree. We already pointed this out earlier.</p> <p>That said though, the deeper into the dependency graph, the more difficult it gets for attackers to pivot. Contagion risk decreases the deeper into the dependency graph you go. A web application container directly exposed to the outside world has a higher chance of compromise than its dependent database container which is only indirectly exposed to the outside world through the web application container. An attacker would have to first compromise the web application container before they can even attempt to pivot into the database container. Also, the database container typically has a more constrained interaction pattern with the web application container (e.g., only specific queries allowed) which further reduces the attack surface and contagion risk. This reduces the opportunities an attacker has to pivot into the database container.</p>"},{"location":"labs/4.%20networking/#every-container-into-its-own-unprivileged-user-account","title":"Every container into its own unprivileged user account","text":"<p>Let's play devil's advocate and consider the case where every container in a dependency graph is placed into its own unprivileged user account and connected using cross-over dependencies.</p> <p>Putting each container into its own unprivileged user account appears to reduce the blast radius and it often does, however attackers can potentially still pivot through the dependency graph spreading through unprivileged user accounts. We have to say this no matter how slim the chances are. With that said though, compromising an application running in an unprivileged container is one thing, and compromising the unprivileged execution environment to issue commands and access other containers in the unprivileged account is another. It all depends on the application, and what it has been allowed to do. If the application can fork and execute commands, then its easy to compromise the unprivileged container environment after compromising the application. There are many ways to harden unprivileged containers to reduce this risk, but it is still there. So, no matter how we skin this cat, the possibility of compromise through the dependency graph is still there. But remember contagion risk usually decreases the deeper into the dependency graph you go.</p> <p>Let's consider inter-container connectivity: namely, providing specific TCP/UDP port access verses full network connectivity between containers. Putting each container into its own unprivileged user account requires exposing ports for containers to interact over. Constraining access to specific ports for container communication is generally a better strategy than giving full connectivity between containers. However, the network quadlet is private without exposure to outside security contexts. Exposing many ports across unprivileged user accounts into outside security contexts increases complexity and the attack surface. There are real tradeoffs to consider.</p>"},{"location":"labs/4.%20networking/#all-containers-in-one-unprivileged-user-account-connected-via-a-network-quadlet","title":"All containers in one unprivileged user account connected via a network quadlet","text":"<p>Putting all containers on the same network quadlet in a single unprivileged user account could reduce complexity, the attack surface, and improve performance but now every container is included in the blast radius when the unprivileged account is compromised.</p> <p>The middle road is best. That is putting containers in a dependency graph into the same unprivileged user account and using a private network quadlet to connect them. At least one container in the graph publishes ports to the outside world. This reduces complexity and the attack surface since fewer ports are exposed to the outside world. Execution still flows through the entire set of dependent containers the same way as before in an unprivileged account using a private network quadlet. The blast radius is still limited to the unprivileged user account and there's no proxying of traffic between containers adding latency and performance penalties.</p>"},{"location":"labs/4.%20networking/#formal-rule","title":"Formal Rule","text":"<p>[RULE]: A dependency graph of containers with at least one container exposing ports to the outside should all go into a single unprivileged user account using private network connectivity with a network quadlet for all dependent inter-container communication.</p>"},{"location":"labs/4.%20networking/#resource-trade-offs","title":"Resource Trade-offs","text":"<p>There is yet another trade-off between security and resource efficiency, and the ease of maintenance. In enterprise environments security often takes precedence over resources since resources are usually abundant. In these scenarios, isolating an application container in its own unprivileged user account with a dedicated database container only for the application's data makes sense to minimize the blast radius and attack surface. You have dedicated application databases running everywhere, but who cares: enterprises have the developers and DBA's to tweak and manage them. For maximum security in enterprise environments, ALWAYS place applications and their dedicated databases into the same unprivileged user account and connect them over a dedicated network quadlet: NEVER USE SHARED RESOURCES. You can expand this mantra beyond databases to any kind of resource.</p> <p>Conversely, in a small business or in a home lab, resources and manpower are limited. Manpower is usually the primary constraint. You don't want a postgresql instance per application service requiring a separate dedicated database service to have to monitor, tweak, and manage. When resources and manpower are limited, efficiency and easy of maintenance becomes a greater priority and you start to compromise.</p> <p>In a home lab, a single shared (multi-tenant) postgresql database server may make sense. Databases are multi-tenant systems after all. Multiple applications can use the same database instance with each application having its own database user and schema on the same shared instance. Only one database server consumes resources, which is a significant advantage. Tuning and monitoring it can be done in a centralized manner which is even more advantageous. This configuration, although less secure, caters to the overwhelming need to reduce management overheads while making the most of limited resources.</p> <p>FUTURE: I see AI handling maintenance for us eventually so that problem will go away. Also hardware will become so cheap that resource constraints will also go away. In the end, if you can, use dedicated resources for maximum security. Until then, we have to make do with what we have.</p>"},{"location":"labs/4.%20networking/#reverse-proxies-and-selective-access-patterns","title":"Reverse Proxies and Selective Access Patterns","text":"<p>Combining the power of reverse proxies and selective access patterns can further reduce the attack surface and blast radius beyond what we've discussed so far. It's a powerful combination that can significantly enhance the security posture of containerized applications.</p> <p>Reverse proxies can do a lot of things as intermediaries but from a security perspective they can act as policy enforcement points. The reverse proxy acts as an authorization gateway between clients and backend services, allowing for more granular control over access and communication. For example, a reverse proxy can be configured to route requests to specific backend services based on identities, and authorization rules or reject them all together. This can reduce the attack surface by limiting the number of exposed ports and services.</p> <p>Selective access patterns can be implemented to restrict which services can communicate with each other. For instance, using firewall rules or network policies, we can define which containers are allowed to connect to specific services, further reducing the risk of lateral movement in case of a compromise. This approach allows for a more modular and secure architecture, where services can be isolated based on their roles and access requirements, rather than being grouped together solely based on their dependency relationships.</p> <p>Nice theoretical discussion right? Let's move on to concrete problems demonstrating the theories and rules in action, then actually implement real solutions in the next major section.</p>"},{"location":"labs/4.%20networking/#concrete-example","title":"Concrete Example","text":"<p>We have the usual web application server, a database server, and a cache server triad connected together using a network quadlet in a single unprivileged user account. The web application server exposes its service port to the host on its loopback interface. The database and cache servers are only indirectly exposed through the web application server. All are connected over a private network quadlet in the unprivileged user account.</p> <p>Only local clients on the host can access the web application server. From there, we can selectively expose it through a reverse proxy container (like Nginx, Caddy, or Traefik) running in a separate unprivileged user account. Remember all unprivileged user accounts have access to the host's loopback. The reverse proxy container can be port mapped to listen on 8443 for HTTPS traffic to route traffic back and forth from the web application server since it can connect to the web application server's port over the host's loopback interface. A firewall rule on the host forwards incoming traffic on port 443 to port 8443 for the reverse proxy since the reverse proxy operating unprivileged cannot bind to ports below 1024.</p> <p>The reverse proxy can do so much more. It can even wedge a WAF in front of the application. This way, only the reverse proxy is itself directly exposed to the world outside of the host, while multiple web application servers remain hidden behind it. Compromise of the reverse proxy does not necessarily compromise the web application servers since they are in separate unprivileged user accounts.</p> <p>These techniques further reduce the attack surface and blast radius by isolating services and controlling access through well-defined channels and access control points. They also allow for more flexible and scalable architectures, where services or groups of services can be added or modified without affecting the overall security posture especially when clustered and load balanced.</p> <p>In the next section we consider a very dangerous application, pgAdmin, and a full blown example of how to secure it properly.</p>"},{"location":"labs/4.%20networking/#50-rollback","title":"5.0 Rollback","text":"<p>You can roll back to the previous VM snapshot before we made all these changes with the following command on your host:</p> <pre><code>vagrant snapshot restore networking-start\n</code></pre>"},{"location":"labs/4.%20networking/#lessons-learned","title":"Lessons Learned","text":"<p>In this lab we learned how to create and manage custom Podman networks using systemd quadlets. We created a private bridge network with specific subnet, gateway, and DNS settings. We connected containers to this network and verified connectivity between containers and the outside world. We explored network namespaces and their isolated views of network interfaces. We discussed IPAM settings and how Podman assigns IP addresses to containers. Finally, we delved into security considerations when using cross-over dependencies vs. network quadlets for container communication across unprivileged user accounts.</p>"},{"location":"labs/5.%20example/","title":"Real World Example","text":"<p>This is a good point to consider a real world (concrete) example to solidify our understanding of the concepts covered so far.</p> <p>In this lab we will explore a system of applications that use the same shared database while running a very dangerous application, the pgAdmin application used for PostgreSQL database management. We will demonstrate how to mitigate the inherent risks of pgAdmin in a containerized environment using Podman quadlets and systemd.</p>"},{"location":"labs/5.%20example/#0-snapshot","title":"0. Snapshot","text":"<p>Before proceeding, create a VM snapshot to allow easy rollback after completing this lab. You can create a snapshot with the following command on your host:</p> <pre><code>vagrant snapshot save example-start\n</code></pre>"},{"location":"labs/5.%20example/#1-pgadmin","title":"1. pgAdmin","text":"<p>pgAdmin is a popular tool for managing PostgreSQL servers and databases. It provides a web-based interface for database administration tasks such as creating databases, running queries, managing users, and monitoring performance. It manages connection objects to PostgreSQL servers and databases, storing connection parameters and credentials (sometimes of database super users) within its own internal database. By its very nature it is an extremely dangerous application to host and to expose.</p> <p>WARNING: Enterprise production environments should never use pgAdmin or similar database management applications. They are simply too risky to expose and they promote one-off manual changes to database servers outside of DevOps automation processes making database servers snowflake servers. Such tools are a security nightmare and should be avoided at all costs in almost all environments much less production environments. Non-production developer environments, and perhaps home lab environments for those just learning to us Postgresql, I understand, but still I recommend against it. It's a crutch, just start getting used to using SQL, DDL, and DevOps automation instead: the investment amortizes in the end and you won't instill bad habits in yourself.</p>"},{"location":"labs/5.%20example/#11-get-triggered","title":"1.1 Get triggered","text":"<p>Anytime you see an application that:</p> <ul> <li>exposes a web interfaces for management</li> <li>manages connections and credentials</li> <li>has access to super user credentials</li> <li>has the ability to run arbitrary commands</li> <li>is always on and accessible over the network</li> </ul> <p>You should be immediately triggered to avoid using it at all costs. Such applications are a magnet for attackers and a security nightmare to manage. We're only toying with the idea as a challenging exercise to demonstrate how to mitigate the risks of such an application using Podman quadlets and systemd. Do not take this as an endorsement to use pgAdmin or similar classes of risky applications in your environments.</p> <p>When you do get triggered, and you don't have a choice (because higher authorities insists on using such an abomination of an application), then the protection patterns discussed here will help. In fact, these techniques are still recommended for any third party applications you cannot thoroughly assess the security implications of. They're best practices for securely hosting any third party applications in general.</p>"},{"location":"labs/5.%20example/#12-pgadmin-modes","title":"1.2 pgAdmin Modes","text":"<p>The pgAdmin application has two modes: the web server application mode and the desktop application mode. In web server application mode it runs centrally as a web application on a server that many users can access and log into. In desktop application mode it runs locally on a user's machine as a desktop application connecting directly to databases. Both provide the same common user interface running the same code base.</p> <p>At first glance, the desktop application seems more secure because it runs locally and does not expose an \"always-on\" web interface. In web application mode, the application is accessible over the network, which can introduce additional attack vectors. Theoretically, in desktop mode users open the locally installed application, use it for a while, then shut it down when done. However, the application may be left running unintentionally exposing it for long periods of time. Even when off, pgAdmin is still at risk residing while at rest on a less secure desktop.</p> <p>The desktop installations may not be as rigorously maintained and updated as web applications, leading to potential vulnerabilities on the host if software patches are not up to date. Generally desktops are more susceptible to local malware or unauthorized access especially when misconfigured. The more desktop installations the more points of attack. A centralized server alleviates these concerns since it can be centrally updated, managed, and monitored. In desktop mode, database access for multiple databases by multiple users will be required from multiple hosts. This is yet another management and security problem in itself. So the original question of which mode is more secure is a lot more complicated than it first appeared.</p> <p>As an exercise we will secure both modes of pgAdmin tool operation: as a desktop installed application and as a web application.</p>"},{"location":"labs/5.%20example/#13-setting-up-the-pgadmin-web-application","title":"1.3 Setting up the pgAdmin Web Application","text":"<p>Let's first set up the pgAdmin web application and postgresql running in a container using a Podman quadlet. The pgAdmin web application will run in its own unprivileged user account called <code>pgadmin</code>. The PostgreSQL server will run in its own unprivileged user account called <code>postgresql</code>. Both containers will run on the same host but isolated from each other in separate unprivileged users accounts. pgAdmin will connect to the PostgreSQL server over its systemd socket port on <code>localhost:5432</code>. Log into the virtual machine as the <code>vagrant</code> user and issue the following commands:</p> <pre><code>PG_USER=postgresql\nsudo bro-user --remove \"${PG_USER}\"\nsudo bro-user \"${PG_USER}\"\nPG_HOME=\"$(getent passwd \"${PG_USER}\" | cut -f 6 -d ':')\"\nCONTAINERS_CONFIG=\"${PG_HOME}\"/.config/containers/systemd/\n\nsudo -iu \"${PG_USER}\" mkdir -p \"${CONTAINERS_CONFIG}\"\ncat &lt;&lt;'EOF' | sudo -iu \"${PG_USER}\" tee \"${CONTAINERS_CONFIG}\"/postgresql.container\n[Unit]\nDescription=PostgreSQL Container\n\n[Container]\nImage=docker.io/library/postgres:16\nContainerName=postgresql\nEnvironment=POSTGRES_PASSWORD=password\nNetwork=none\n\n[Service]\nRestart=always\nTimeoutStartSec=120\nStartLimitBurst=5\nRestartSec=10s\n\n[Install]\nWantedBy=default.target\nEOF\n\nsudo bro-volume --name postgresql-data      \\\n    --owner \"${PG_USER}\"                    \\\n    --container postgresql                  \\\n    --container-path /var/lib/postgresql/data\n\nsudo bro-volume --name postgresql-config    \\\n    --owner \"${PG_USER}\"                    \\\n    --container postgresql                  \\\n    --container-path /etc/postgresql\n\nsudo -iu \"${PG_USER}\" bro-activator         \\\n    --name postgresql                       \\\n    --external-port 5432                    \\\n    --internal-port 127.0.0.1:5432\n\nsudo -iu \"${PG_USER}\" systemctl --user daemon-reload\nsudo -iu \"${PG_USER}\" systemctl             \\\n    --user enable --now postgresql-activator.socket\n\nexport PGPASSWORD=password\npsql -h \"$(ip route | grep 'kernel scope link' | awk '{print $9}')\" -p 5432 -U postgres -c '\\l'\n</code></pre> <pre><code>sudo bro-user pgadmin\n</code></pre>"},{"location":"labs/5.%20example/#13-strong-authentication","title":"1.3 Strong Authentication","text":"<p>There are directly exposed interactive end-user applications and indirectly exposed non-interactive backend services. Interactive end-user applications require user interaction to function, such as pgAdmin. We can exploit that fact to force the use of strong authentication which requires user interaction: a token button press, a biometric verification, or a TOTP one time code. Using such mechanisms helps secure access to such dangerous applications. In fact, strong password-less authentication should be used everywhere regardless of the application.</p> <p>Strong authentication mechanisms will be used to gate access to the application at the network level before an attacker can reach the application itself. This way, we generalize a reusable solution for any directly exposed end-user (interactive) application. Most web applications manage users and password credentials and many have integrations for strong authentication mechanisms such as MFA. However, we want to avoid the quagmire of application specific configuration details and coupling. We want one, decoupled and independent strong authentication approach that applies across all applications without having know anything about the application.</p> <p>As discussed in the last section, we can use a reverse proxy to enforce authentication, then (perform authorization to) allow or deny access to the application before even reaching the application. The reverse proxy can be combined with an Identity Provider (IdP) to support multiple strong authentication mechanisms and manage groups authorized to access downstream applications.</p> <p>IN APPLICATION API ACCESS: Some interactive applications also expose APIs for automation and thus are both non-interactive and interactive. Since these APIs are non-interactive, they cannot leverage user interaction for strong authentication. Instead, they often rely on API-token based authentication mechanisms, OAuth tokens, or JWTs. Application APIs have distinct and different URL paths than those presented to interactive users. The reverse proxy can map different authentication mechanisms based on URL paths to support both interactive end-user paths with strong authentication and non-interactive APIs using API-token based authentication.</p>"},{"location":"labs/5.%20example/#14-caddy-and-keycloak-to-the-rescue","title":"1.4 Caddy and Keycloak to the rescue","text":"<ul> <li>Caddy: Site Docker</li> <li>Keycloak: Site Docker</li> <li>pgAdmin: Site Docker</li> <li>PostgreSQL: Site Docker</li> <li>step-ca: Site Docker</li> <li>gitea: Site Docker</li> </ul> <p>Caddy will be used for the reverse proxy. Keycloak will be used an Identity Provider (IdP). Step-ca will be used to issue TLS certificates. These three components will run in a single unprivileged user account called <code>identity</code>. The pgAdmin web application will run in a container in its own unprivileged user account called <code>pgadmin</code>, and as a desktop client on test desktops. PostgreSQL will run in its own container in its own unprivileged user account called <code>postgresql</code>. Gitea will be used as another test application that non-interactively uses the same shared postgresql instance and it will run in its own unprivileged user account called <code>gitea</code>.</p> <p>Caddy will gate access and route traffic to the two web applications: pgAdmin and gitea. Caddy will use WebAuthn APIs for FIDO2 and TOTP strong authentication mechanisms to authenticate and authorize users using Keycloak as the IdP. Only authorized users in specific groups will be allowed access to each application. Caddy will also proxy Keycloak and step-ca endpoints for web applications to use. Here's a list of proxied endpoints:</p> <ul> <li>the Keycloak server web administration UI for user self-service and management and its WebAuthn endpoints</li> <li>the pgAdmin web application UI for postgresql database management</li> <li>the postgresql server for database connections from clients, for both interactive and non-interactive access</li> <li>the gitea application for non-interactive access to postgresql</li> <li>the step-ca ACME endpoint for certificate issuance and renewal</li> <li>the step-ca TLS client authentication endpoint</li> </ul> <pre><code>sequenceDiagram\n    participant Browser\n    participant Caddy as Caddy (Reverse Proxy + OIDC client)\n    participant Keycloak as Keycloak (IdP)\n    participant WebApp as Downstream Web Application\n\n    Note over Browser,Caddy: User requests web application\n    Browser-&gt;&gt;Caddy: HTTPS 443 GET https://app.example.com/\n    alt no session / not authenticated\n        Caddy-&gt;&gt;Browser: 302 Redirect to Keycloak Auth endpoint\n        Browser-&gt;&gt;Keycloak: HTTPS 443 GET /realms/webapps/protocol/openid-connect/auth?client_id=\u2026&amp;redirect_uri=\u2026\n        Keycloak-&gt;&gt;Browser: HTML login page (with MFA options: FIDO2 / TOTP)\n        Browser-&gt;&gt;Keycloak: POST login credentials + choose WebAuthn/TOTP\n        Keycloak-&gt;&gt;Browser: 302 Redirect back to redirect_uri with auth code\n        Browser-&gt;&gt;Caddy: HTTPS 443 GET https://app.example.com/callback?code=\u2026&amp;state=\u2026\n        Caddy-&gt;&gt;Keycloak: HTTPS 443 POST /realms/webapps/protocol/openid-connect/token (grant_type=authorization_code &amp; code=\u2026)\n        Keycloak-&gt;&gt;Caddy: JSON response { id_token, access_token, refresh_token, \u2026 }\n        Caddy-&gt;&gt;Keycloak: HTTPS 443 GET /realms/webapps/protocol/openid-connect/userinfo (Authorization: Bearer access_token)\n        Keycloak-&gt;&gt;Caddy: JSON userinfo { sub, email, groups:[\"webapp-users\",\u2026], \u2026 }\n        Caddy-&gt;&gt;Caddy: Validate token &amp; check claim groups contains \"webapp-users\"\n    end\n    alt authorised\n        Caddy-&gt;&gt;WebApp: HTTP/HTTPS internal proxy request to WebApp on internal port (e.g., 8080) with X-Auth-User &amp; X-Auth-Groups headers\n        WebApp--&gt;&gt;Browser: HTTP/HTTPS response via Caddy\n    else not authorised\n        Caddy--&gt;&gt;Browser: HTTP 403 Forbidden or redirect to access-denied page\n    end</code></pre>"},{"location":"labs/5.%20example/#overview-of-the-flow","title":"Overview of the flow","text":"<p>Users of an interactive web app \u2192 hit Caddy reverse-proxy \u2192 Caddy ensures they're authenticated (via Keycloak) and authorised (group membership) \u2192 if yes, proxy to downstream web application; if no, deny. Here's the typical step-by-step flow:</p> <ol> <li> <p>A user opens the browser and tries to access the web app through the Caddy proxy (e.g., https://app.example.com).</p> </li> <li> <p>Caddy detects that the user is not yet authenticated (via some session cookie or token).</p> </li> <li> <p>Caddy (or a plugin) sends the user to Keycloak for login (redirect). This uses the OIDC protocol (which is built on OAuth 2.0). Keycloak presents the login screen \u2014 and you can configure it to require FIDO2 / TOTP as part of the login flow (MFA).</p> </li> <li> <p>User completes authentication (with FIDO2 key or TOTP). Keycloak issues an ID token and/or access token (JWT) to the browser/app.</p> </li> <li> <p>Browser returns to Caddy (via the callback redirect) with the token or code, etc.</p> </li> <li> <p>Caddy validates the token (or exchanges an authorization code for a token) and checks the user's group/role claims to decide if the user is allowed to access the downstream web-app.</p> <ul> <li> <p>If allowed: Caddy forwards the request to the downstream web application, possibly passing a header (e.g., Authorization: Bearer token or a custom header with user info).</p> </li> <li> <p>If not allowed: Caddy returns 403 or redirects to an \u201caccess denied\u201d page.</p> </li> </ul> </li> <li> <p>For subsequent requests, the session is maintained (cookie or token) so the redirect to Keycloak is not repeated until session expiry.</p> </li> </ol> <p>So, in short: Caddy acts as an OIDC client (or uses an OIDC gate/plug-in) in front of your web-app; Keycloak is the OIDC provider.</p>"},{"location":"labs/5.%20example/#unprivileged-users-and-containers","title":"Unprivileged Users and Containers","text":"<p>In section 1.3 Strong Authentication mentioned the words, \"decoupled\" and \"independent\", with respect to the mechanism. What was meant now manifests concretely in the security contexts and boundaries in the diagram above. Notice that Caddy, Keycloak, and step-ca all run together in the same unprivileged user account called <code>identity</code> with a network quadlet to provide private container-to-container communication between them. They are tightly coupled to one another and together they provide strong authentication and authorization. However, they are decoupled and independent of the applications they gate access to: <code>pgAdmin</code> and <code>gitea</code>. These protected applications run in their own unprivileged user accounts called <code>pgadmin</code> and <code>gitea</code> respectively. There is no contagion risk between the identity services and the applications they protect.</p> <p>Compromising the <code>identity</code> unprivileged user account running Caddy and Keycloak does not compromise the applications they provide an additional layer of strong authentication to protect. Caddy knows nothing about the applications themselves other than their existence and how to proxy them. Keycloak registers hardware tokens for users and groups them based on their access to applications. Together they determine and proxy connections to applications. The private network quadlet for container-to-container communication allows Caddy connections to Keycloak and step-ca to remain private over the dedicated network without risking exposure of their service ports to the host or to the outside world; namely:</p> <ul> <li>Caddy's OIDC interactions with Keycloak to enforce policies and access to web applications at the network level</li> <li>Caddy's proxying of Keycloak endpoints</li> <li>Caddy's proxying of step-ca ACME and TLS client authentication endpoints</li> </ul> <p>Identity operations are fully isolated from application operations and Caddy is the first decoupled protective barrier, an independent PEP (Policy Enforcement Point), controlling network level access to applications based on authentication and authorization policies. Applications still manage their own users requiring them to login to the applications themselves: these are second independent authentication factors, the \"what you know\" factors. Again, compromising the unprivileged <code>identity</code> user or any one of the other unprivileged applications (pgAdmin and gitea) does not compromise the other unprivileged user accounts or its containers. Everything remains intact and independent even though there are perceived dependencies: i.e. Caddy still proxies pgAdmin and gitea.</p> <p>Furthermore, the pgAdmin and gitea containers running in their own separate unprivileged user accounts are never directly exposed to the network. These applications expose their HTTP ports on the host's loopback interface. Only the Caddy reverse proxy is directly exposed to proxy traffic outside of the host. Firewall rules in the host map privileged port 443 to Caddy's listening port 8443 since Caddy cannot bind to ports below 1024 when running unprivileged. Port 80 is always redirected by the firewall to port 443. Caddy also redirects port 8080 to 8443 to always ensure the use of TLS termination. The firewall may also be used to restrict access to Caddy from known IP sources and trusted networks.</p>"},{"location":"labs/5.%20example/#15-pgadmin-desktop-mode","title":"1.5 pgAdmin Desktop Mode","text":"<p>There will be multiple desktop mode pgAdmin application instances running across external desktop hosts, a.k.a. clients. Each client requires access to postgresql servers over the network: the ones they administer. In our case we just have one shared postgresql server instance. The host's external interface binds port 5432 for postgresql connections originating from desktop pgAdmin clients. Instead of exposing the postgresql server directly to clients, the same Caddy reverse proxy also handles postgresql server connections from desktop pgAdmin clients. The reverse proxy should also serve as a PEP before allowing access to the database: no network access granted unless authenticated and authorized.</p> <p>So what options do we have? How can we do this with a generalized (reusable) solution?</p> <p>Although the latest postgresql 18.0 release now supports OAuth 2.0 with bearer tokens for database connections, pgAdmin desktop mode does not support OAuth 2.0 or OpenID Connect (OIDC) for database connections. Therefore, we cannot use the same OIDC based strong authentication mechanism used for web applications to secure database connections from desktop pgAdmin clients. The industry, not just the pgAdmin people, are getting there though, so keep an eye out for better integration options; OIDC is the better way to go.</p>"},{"location":"labs/5.%20example/#ssh-tunneling-or-tls-client-certificates","title":"SSH Tunneling or TLS Client Certificates?","text":"<p>That said, two common denominator alternatives remain: they're both equal in terms of their strength and management overheads. One is the use of TLS client certificates for strong authentication, and the other is the use of SSH tunneling with SSH keys. Both are generalized and can be used beyond our specific use case of gating postgresql network access. Let's quickly analyze both approaches.</p> <p>Both require key material on the client: TLS client certificate private key or SSH private key. Both need to be generated, trusted (configured), and protected. Both can be compromised on the client side unless protected by a token or a vault via token access API's such as PKCS#11. Both are equally strong when implemented properly.</p> <p>WARNING: Storing private keys on clients is always too great a risk: if the client is compromised, the private key can be stolen and used to impersonate the client, so don't do it unless you can protect client keys using a hardware security token.</p> <p>TLS is a baked in standard for securing network communications. It operates at layer 4 of the OSI model, providing end-to-end encryption and authentication. TLS client certificates are widely supported by many applications and services, including postgresql. They're also supported by multiple hardware tokens and interfaces for proper protection on client systems. They can be easily integrated into existing infrastructure and provide a seamless user experience. It's a first class citizen of the network stack.</p> <p>SSH on the other hand is primarily designed for secure remote access and command execution. While SSH tunneling can be used to secure database connections, it introduces additional complexity and overhead. SSH requires managing separate user accounts and their keys, and this is what increases exposure and complexity (more moving parts). Additionally, not all applications natively support SSH tunneling, requiring additional configuration and more one offs. Let's play it safe and stick to the official standard. SSH keys do not work well with hardware tokens either, often requiring both gpg/tls certs and SSH keys representing them to be generated: it's still a problem but may improve later.</p>"},{"location":"labs/5.%20example/#using-tls-client-certificates","title":"Using TLS Client Certificates","text":"<p>Caddy will be configured to use TLS certificates trusted by the step-ca certificate authority (CA) for client authentication. This way, all traffic between pgAdmin desktop clients and Caddy is encrypted and TLS authenticated in both directions. To do so, Caddy exposes a separate connection path from outside of the host to the postgresql server.</p> <p>I'm so tempted to try Postgresql 18.0 which was just released support for OAuth 2.0 with bearer tokens. PostgreSQL 18.0 can be configured for external (non-loopback) database connection sources to switch authentication mechanisms and act as an OIDC client to Keycloak. Keycloak can enable short lived or one time use API tokens for DevOps automation. It would be wonderful to tie and track one time authorizations to DevOps jobs that modify databases in security logs. But I digress.</p> <p>Even if a desktop pgAdmin installation is compromised, the attacker cannot access the postgresql server without the client certificate. Securing the client certificate private key on the desktop is still paramount.</p> <p>We will NOT rely on IP sources since in dynamic environments (clients using DHCP) these change, and can easily be spoofed. Relying on IP sources make the system much more brittle in the long run and is not a solid security measure unless \"LAN authentication/authorization\" 802.1X or a VPN gateway is used: essentially some way to authenticate and authorize IP addresses on the network.</p>"},{"location":"specifications/bro-activator/","title":"Specification: <code>bro-activator</code> CLI Program","text":"<p>Previously referred to as <code>bro-socket-proxyd</code>.</p> <p>The <code>bro-activator</code> CLI program, as a bash script, is part of the Brothaman scripts package which creates systemd socket activator infrastructure for unprivileged containers. It is installed using the <code>bro-install</code> script or the <code>brothaman-scripts</code> Debian package.</p> <ul> <li>The <code>bro-activator</code> script DOES NOT require root privileges to run, and can be executed by unprivileged users.</li> <li>The <code>bro-activator</code> script creates a per container quadlet service pattern consisting of three components for socket-activated container services (two are created for a container quadlet):</li> <li>A systemd socket unit (<code>${CONTAINER_NAME}-activator.socket</code>) that listens for incoming connections on a specified external network interface and port.</li> <li>An activator service unit (<code>${CONTAINER_NAME}-activator.service</code>) that forwards incoming connections from the socket unit to the container's internal port using an instance of systemd-socket-proxyd running inside CONTAINER_NAME's network namespace.</li> <li>A Podman Quadlet container descriptor unit (<code>CONTAINER_NAME.container</code>) that defines the container to be run.</li> <li>The <code>bro-activator</code> script supports the following command line options:</li> <li><code>--name CONTAINER_NAME</code>: The base name for the service components (socket, proxy, container). With the CONTAINER_NAME the script generates a <code>${CONTAINER_NAME}-activator.socket</code> and a <code>${CONTAINER_NAME}-activator.service</code> for the <code>${CONTAINER_NAME}.container</code> quadlet which Podman happens to automatically generate a <code>${CONTAINER_NAME}.service</code> for the quadlet.</li> <li><code>--external-port ADDR:PORT</code>: The external interface address and port on which the socket unit will listen for incoming connections (0.0.0.0 for all interfaces). If address is omitted, defaults to all interfaces.</li> <li><code>--internal-port ADDR:PORT</code>: The internal port on which the container will listen for connections. The proxy service unit will forward connections to this port inside the container. If address is omitted, defaults to 127.0.0.1 (localhost).</li> <li><code>--help</code>: Display help information about the script usage.</li> <li> <p><code>--version</code>: Display the version of the <code>bro-activator</code> script.</p> </li> <li> <p>The <code>bro-activator</code> script generates the necessary systemd unit files for the socket unit, proxy and service unit, corresponding to the container quadlet unit based on the provided command line options. Use the lab <code>4. networking.md</code> as good guide / reference for how this is done since it provides examples perhaps not exactly following conventions (it's container name is test-postgresql but the activators use postgresql-activator as the base name) but its close. To avoid fragile inline heredocs, <code>bro-activator</code> now also writes a helper script <code>~/.config/systemd/user/${CONTAINER_NAME}-activator-helper.sh</code> that the generated service uses for all <code>ExecStart{Pre}</code> actions.</p> </li> <li>The helper script has three subcommands:<ul> <li><code>capture</code> - loops until it can <code>podman inspect -f '{{.State.Pid}}'</code> the target container and writes <code>%t/${CONTAINER_NAME}-activator.env</code> (containing <code>TARGET_PID</code>, <code>TARGET_HOST</code>, <code>TARGET_PORT</code>).</li> <li><code>wait</code> - sources the env file and uses <code>podman unshare nsenter -t \"${TARGET_PID}\" -n \u2026</code> to probe the internal port (currently via <code>nc -z</code>), retrying until it succeeds or times out.</li> <li><code>run</code> - sources the env file and launches <code>podman unshare bro-helper --pid \"${TARGET_PID}\" -- /lib/systemd/systemd-socket-proxyd \u2026</code>.</li> </ul> </li> <li>The generated service now references this helper script instead of embedding bash loops directly in systemd unit properties. This keeps the unit readable, eliminates quoting pitfalls, and makes it easier to evolve the orchestration logic.</li> <li>The created systemd units are placed in the appropriate XDG paths for systemd user services and quadlets under <code>~/.config/systemd/user/</code>.</li> <li>The <code>bro-activator</code> script can be extended in the future to support additional features as needed.</li> <li>The <code>bro-activator</code> script is intended to be used by system administrators and unprivileged users to create socket-activated container services in a standardized and efficient manner.</li> <li>The <code>bro-activator</code> script is part of the Brothaman project and is licensed under the ASL 2.0 License.</li> <li>The <code>bro-activator</code> script is maintained as part of the Brothaman scripts package and should be kept up to date with the latest features and security patches.</li> <li>The <code>bro-activator</code> script ensures that the created systemd units follow best practices for security, resource management, and systemd integration.</li> <li>The <code>bro-activator</code> script provides error handling and validation for the command line arguments to ensure that the specified parameters are valid and that the systemd units can be created successfully.</li> <li>The <code>bro-activator</code> script is intended to be used in conjunction with other Brothaman scripts, such as <code>bro-user</code> and <code>bro-volume</code>, to create a complete environment for running unprivileged rootless Podman containers with socket activation capabilities.</li> <li>The <code>bro-activator</code> script configures the container quadlet unit removing no longer needed PublishPort and Networking directives (setting Networking=none) when using the systemd-socket-proxyd mechanism instead.</li> <li>The <code>bro-activator</code> script provides documentation and usage examples to assist users in creating socket-activated container services using the man page facilities. Make sure a man page is created for it and installed properly along side the script.</li> <li>The <code>bro-activator</code> script verifies that the specified external and internal ports are available and not already in use by other services.</li> <li>The <code>bro-activator</code> script allows for customization of the created systemd units through additional command line options or configuration files in the future.</li> <li>The <code>bro-activator</code> script provides logging and debugging capabilities to assist users in troubleshooting any issues that may arise during the creation or operation of the socket-activated container services.</li> <li>The <code>bro-activator</code> script is compatible with the latest versions of Podman and systemd, ensuring that users can take advantage of the latest features and improvements in these technologies.</li> <li>The <code>bro-activator</code> script follows best practices for bash scripting, including proper error handling, input validation, and code organization.</li> <li>The <code>bro-activator</code> script is tested and validated to ensure that it functions correctly and reliably in various environments and use cases.</li> <li>The <code>bro-activator</code> script is documented with clear and concise comments to assist users in understanding its functionality and usage.</li> <li>The <code>bro-activator</code> script is intended to be a key component of the Brothaman project, providing a standardized and efficient way to create socket-activated container services for unprivileged users.</li> <li>The <code>bro-activator</code> script provides a consistent and repeatable process for creating socket-activated container services, making it easier for users to deploy and manage these services in their environments.</li> <li>The <code>bro-activator</code> script is designed to be user-friendly, with clear command line options and helpful error messages to guide users through the process of creating socket-activated container services.</li> <li>The <code>bro-activator</code> script is intended to be used in a variety of scenarios, including development, testing, and production environments, providing flexibility and adaptability for different use cases.</li> <li>The <code>bro-activator</code> script adds the proper dependencies to the -activator.service to ensure that it starts after the .service (the service generated for the quadlet .container) with the right Requires= for the socket and the quadlet service as well as the After= directives to ensure proper startup ordering. See how postgresql-activator.service is done in the <code>4. networking.md</code> lab for an example."},{"location":"specifications/bro-helper/","title":"Brothaman's Proxyd Helper","text":"<p>The <code>bro-helper</code> enables <code>systemd-socket-proxyd</code> instances to be exec'd inside the network namespace of containers. By doing so it allows it to proxy traffic from a systemd.socket's file descriptor to container ports that network services bind to on container interfaces.</p> <ol> <li>Allows for <code>--network=none</code> for toighter security</li> <li>Proxied network services can bind to loopback interfaces</li> <li>Single hop proxy (one less without podman's -p mappings)</li> <li>Enables traffic triggering and on-demand container start</li> <li>Primitive used to enable cross over dependencies between unprivileged containers</li> </ol>"},{"location":"specifications/bro-helper/#so-what-why-do-we-need-this","title":"So what? Why do we need this?","text":"<p>Unprivileged containers using <code>slirp4netns</code> or <code>pasta</code> (or <code>--network=none</code> for that matter) have no IP addresses exposed to the host, and hence no ports to proxy traffic to the container with <code>systemd-socket-proxyd</code>. So why do we even bother using <code>systemd-socket-proxyd</code> when the -p switch could be used to proxy traffic from the host to the container: i.e. <code>-p 8080:80</code>?</p> <p>ROOT CAUSE: Podman's proxy mechanism does not work with systemd.socket traffic triggering nor does it support server socket (listener) file descriptor passing while <code>systemd-socket-proxyd</code> does.</p>"},{"location":"specifications/bro-helper/#using-both-together-sucks","title":"Using both together sucks","text":"<p>You could make them both work together. Presume an NGINX container with the daemon running on its default port 80. Unprivileged containers cannot map container ports to host ports below 1024 so we make podman proxy host port 8080 to container port 80.</p> <p>Yet we still want to trigger containers to start when traffic appears using systemd sockets. Say we use a port for it at 8081. Then the <code>systemd-socket-proxyd</code> daemon proxies traffic to and from port 8080 to 8081.</p> <p>Using them together now doubles the proxied traffic overhead which adds latency and you have an extra bogus port sitting out there. With <code>bro-helper</code> one host port and one container port is needed. Most importantly, there's no additional stream copy and transfer across ports for absolutely no reason.</p>"},{"location":"specifications/bro-helper/#security-restrictions","title":"Security Restrictions","text":"<p>The <code>bro-helper</code> is a tiny c-program that uses file capabilities to enter network namespaces. It takes a container PID, joins its network namespace, and executes any command within that namespace. It works with containers running as the invoking user and preserves socket file descriptors for systemd socket activation.</p> <p>Least privilege is achieved by using a very narrow capability addition while using that capability in highly specific situations. Meanwhile we do not fork and honor the passed host file descriptors (listener socket) so are handed off to <code>systemd-socket-proxyd</code> running in the container.</p> <ul> <li>Find the netns you want (e.g., /proc/${PID}/ns/net),</li> <li>Join it with setns(fd, CLONE_NEWNET),</li> <li>Exec the target process.</li> </ul> <p>That's it\u2014no daemons, no forking trees, just a short, auditable path.</p> <p>The security model is simple and effective:</p>"},{"location":"specifications/bro-helper/#file-based-capability-assignment","title":"File-based capability assignment","text":"<p>The <code>bro-helper</code> binary uses file capabilities to obtain the minimal permission needed:</p> <ul> <li>Installation sets <code>cap_sys_admin+ep</code> on the binary via <code>setcap</code></li> <li>This allows any user to run bro-helper and enter network namespaces they own</li> <li>The capability is effective and permitted, but not inheritable</li> </ul>"},{"location":"specifications/bro-helper/#what-this-means-for-security","title":"What this means for security","text":"<ul> <li>Limited scope: Only <code>CAP_SYS_ADMIN</code> is granted, only for network namespace entry</li> <li>No privilege escalation: The executed command (like <code>systemd-socket-proxyd</code>) inherits no special capabilities</li> <li>Process isolation: Each execution is independent with no persistent privileged daemon</li> </ul>"},{"location":"specifications/bro-helper/#what-it-does-conceptually","title":"What it does (conceptually)","text":"<ul> <li>Inputs: <code>--pid &lt;PID&gt;</code>, <code>-- &lt;cmd&gt; [args\u2026]</code></li> <li>Open the namespace: <code>fd = open(\"/proc/PID/ns/net\", O_RDONLY|O_CLOEXEC)</code></li> <li>Join it: <code>setns(fd, CLONE_NEWNET)</code></li> <li>Update environment: Set <code>LISTEN_PID</code> for socket activation if needed</li> <li>Hand off: <code>execvp(target_command, argv)</code>     From here, the target runs inside that netns with normal user privileges.</li> </ul>"},{"location":"specifications/bro-helper/#why-it-needs-cap_sys_admin","title":"Why it needs CAP_SYS_ADMIN","text":"<ul> <li>To call <code>setns(CLONE_NEWNET)</code> into a network namespace, the kernel requires <code>CAP_SYS_ADMIN</code></li> <li>The capability is set via file capabilities: <code>setcap cap_sys_admin+ep /usr/local/bin/bro-helper</code></li> <li>This is the only capability needed for network namespace entry</li> <li>The executed program inherits no special capabilities - it runs with normal user privileges</li> </ul>"},{"location":"specifications/bro-helper/#more-security-options-for-the-next-iteration","title":"More security options for the next iteration","text":"<ul> <li>Validate the target: ensure the PID exists and (optionally) matches the expected user/UID for your trust model.</li> <li>Avoid TOCTOU: once you open the netns fd, you're safe; don't re-resolve paths again.</li> <li>Logging: on failure, log <code>errno</code> and the exact syscall (helps when seccomp/LSM interferes).</li> <li>Interface: support <code>--netns-path</code> as an alternative to <code>--pid</code>, so you can bind a stable symlink like <code>/run/netns/nginx-proxyd</code>.</li> </ul> <p>This is why the approach is safe and composable: minimal capability scope, file-based permissions, no persistent privileges, and the actual worker (<code>systemd-socket-proxyd</code>) runs with regular user privileges in the desired netns.</p>"},{"location":"specifications/bro-user/","title":"Specification: <code>bro-user</code> CLI Program","text":"<p>The <code>bro-user</code> CLI program, as a bash script, is part of the Brothaman scripts package which creates new system accounts for unprivileged containers to run under. It is installed using the <code>bro-install</code> script or the <code>brothaman-scripts</code> Debian package.</p> <p>ATTENTION: The brothaman-scripts package is has a hard dependency on ZFS and zfs-utils being installed on the host system. It also has a hard dependency on the <code>zfs-helper</code> debian package to allow unprivileged users to run delegated zfs commands without root privileges.</p> <ul> <li>If the path <code>/var/lib/containers/unprivileged</code> exists, the <code>bro-user</code> script creates a new user account under this base directory for unprivileged Podman containers.</li> <li>If the path does not exist, it fails with an error.</li> <li>Installation of the <code>brothaman-scripts</code> Debian package MUST create this base unprivileged user directory directory at <code>/var/lib/containers/unprivileged</code> backed by a ZFS dataset with appropriate properties and permissions.</li> <li>The created user account is in /etc/subuid and /etc/subgid with an exclusive (non-overlapping) range allocation of 65536 UIDs and GIDs assigned for rootless Podman containers.</li> <li>The created user account's home directory will be <code>/var/lib/containers/unprivileged/${USERNAME}</code>, where <code>USERNAME</code> is the first argument passed to the <code>bro-user</code> script. It too is backed by a ZFS dataset with appropriate properties and permissions assigned to the new user.</li> <li>Note that the home directory path may already be created as a ZFS dataset so the <code>bro-user</code> script will use the existing dataset as the home directory for the created user account without creating a new home directory.</li> <li>If the user account already exists, the <code>bro-user</code> script does nothing and exits with an error.</li> <li>The created user account is added to the <code>zfshelper</code> group to allow the zfs-helper to run delegated commands without root privileges for this user. The <code>zfs-helper</code> package is a hard dependency for the brothaman scripts.</li> <li>The created user account is configured for systemd user lingering to allow user services to run even when the user is not logged in.</li> <li>The created user account is configured with a default umask of <code>0027</code> to provide secure file permissions by default.</li> <li>The created user account is configured with a default shell of <code>/bin/bash</code>.</li> <li>The <code>bro-user</code> script requires root privileges to run.</li> <li>The created user account is intended to be used as the owner of rootless Podman containers managed via systemd user services and quadlets and should have the basic XDG directories created in its home directory for this purpose: <code>~/.config</code>, <code>~/.config/containers</code>, <code>~/.config/containers/systemd</code>, <code>~/.config/systemd</code>, <code>~/.config/systemd/user</code>, <code>~/.local/share/containers</code>, <code>~/.local/share/containers/storage</code> (the graphroot), and <code>~/.local/share/systemd</code>.</li> <li>The <code>bro-user</code> script does not set a password for the created user account. Password management is left to the system administrator.</li> <li>The <code>bro-user</code> script does not configure SSH access for the created user account. SSH configuration is left to the system administrator.</li> <li>The <code>bro-user</code> script forbids sudo access for the created user account. The whole point is to run unprivileged containers without elevated privileges and protect against privilege escalation.</li> <li>The skeleton files for the created user account are copied from <code>/etc/skel</code> as usual.</li> <li>The <code>bro-user</code> script does not configure any additional user account settings beyond those listed here.</li> <li>The <code>bro-user</code> script supports the following command line options:</li> <li><code>--system-user</code>: Create the user account as a system user with no login shell (<code>/usr/sbin/nologin</code>).</li> <li><code>--help</code>: Display help information about the script usage.</li> <li><code>--version</code>: Display the version of the <code>bro-user</code> script.</li> <li><code>USERNAME</code>: The username of the user account to create (positional argument).</li> <li><code>--remove</code>: Remove the specified user account and its home directory while stopping any running user services and containers for that account.</li> <li>The <code>bro-user</code> script can be extended in the future to support additional features as needed.</li> <li>The <code>bro-user</code> script is intended to be used by system administrators to create and manage unprivileged user accounts for running rootless Podman containers in a secure and isolated manner.</li> <li>The <code>bro-user</code> script is part of the Brothaman project and is licensed under the ASL 2.0 License.</li> <li>The <code>bro-user</code> script is maintained as part of the Brothaman scripts package and should be kept up to date with the latest features and security patches.</li> <li>The proper podman configurations are created in the appropriate files under XDG paths. By default container Networking should either default to none or to pasta (with none preferred).</li> <li>The <code>bro-user</code> script creates a per-user <code>containers.conf</code> file under <code>~/.config/containers/containers.conf</code>. It contains the following settings by default to optimize for unprivileged rootless containers:</li> <li><code>network_cmd = \"none\"</code>: Configures the default networking mode for containers to be <code>none</code>, ensuring that containers do not have network access unless explicitly configured otherwise.</li> <li><code>storage_driver = \"overlay\"</code>: Sets the storage driver for containers to <code>overlay</code>, which is suitable for most use cases and provides good performance.</li> <li><code>storage_options = [\"overlay.mountopt=nodev\"]</code>: Adds the <code>nodev</code> mount option to the overlay storage driver to enhance security by preventing device files from being created within container filesystems.</li> </ul>"},{"location":"specifications/bro-volume/","title":"Specification: <code>bro-volume</code> CLI Program","text":"<p>The <code>bro-volume</code> CLI program, as a bash script, is part of the Brothaman scripts package which creates ZFS backed volumes for unprivileged containers. It is installed using the <code>bro-install</code> script or the <code>brothaman-scripts</code> Debian package.</p> <p>ATTENTION: The brothaman-scripts package is has a hard dependency on ZFS and zfs-utils being installed on the host system. It also has a hard dependency on the <code>zfs-helper</code> debian package to allow unprivileged users to run delegated zfs commands without root privileges.</p> <ul> <li>The <code>bro-volume</code> script requires root privileges to run.</li> <li>The <code>bro-volume</code> script creates a ZFS dataset for use as a Podman volume for unprivileged rootless containers.</li> <li>The created ZFS dataset is owned by the specified unprivileged user account that will run the container using the volume.</li> <li>The created ZFS dataset has appropriate ZFS properties set for use as a Podman volume, including compression and atime settings.</li> <li>The created ZFS dataset is mounted at the specified mount point for use as a Podman volume quadlet.</li> <li>The <code>bro-volume</code> script supports the following command line options:</li> <li><code>--name NAME</code>: The name of the volume (ZFS dataset name) required.</li> <li><code>--mount-point PATH</code>: The mount point for the volume defaults to <code>/var/lib/containers/unprivileged/USERNAME/volumes/NAME</code> where USERNAME is the owner user.</li> <li><code>--container CONTAINER_NAME</code>: The name of the container quadlet that will use this volume (optional), adds a PartOf= directive to the volume quadlet and integrates the volume into the container quadlet.</li> <li><code>--container-path PATH</code>: The container mount path where the volume will be mounted inside the container (required when <code>--container</code> is used).</li> <li><code>--pre_snapshots RETENTION</code>: The number of ZFS 'pre_' start snapshots to retrain for the volume (ZFS dataset), default is 5, set to 0 to disable.</li> <li><code>--owner USERNAME</code>: The owner of the volume (unprivileged user account required).</li> <li><code>--remove</code>: Remove the specified volume, including its ZFS dataset, quadlet file, zfs-helper policy entries, and dependencies from container quadlets.</li> <li><code>--help</code>: Display help information about the script usage.</li> <li><code>--version</code>: Display the version of the <code>bro-volume</code> script.</li> <li>The <code>bro-volume</code> creates a <code>&lt;NAME&gt;.volume</code> quadlet for the specified unprivileged user in the appropriate XDG path: <code>~/.config/containers/systemd/&lt;NAME&gt;.volume</code>. The created volume quadlet contains the necessary configuration to use the created and prepared ZFS dataset as a Podman volume.</li> <li>The created volume quadlet includes the following directives:</li> <li><code>Description=</code>: A brief description of the volume.</li> <li><code>VolumeName=</code>: The name of the volume (ZFS dataset name).</li> <li><code>Environment=</code>: Sets environment variables for the volume, including the mount point, the ZFS dataset (same as volume) name, and the owner user as well as things like the RETENTION for pre_snapshots.</li> <li><code>Wants=</code>: Specifies that ZFS mounter is ready.</li> <li><code>PartOf=</code>: Associates the volume with the Podman user service instance working in conjunction with a --container option to link the volume to a container.</li> <li><code>ExecStartPre=</code>: Multiple used to prepare the volume for use by ensuring the mount point data set exists, creates it if necessary, and sets the correct ownership and permissions all using zfs-helperctl. Even takes the pre_snapshots argument into account. See how all this is done in the <code>3. volumes.md</code> lab. Must make sure to take specifier expansion and special characters into account. Look at examples in the lab where we escaped the <code>%</code> with <code>%%</code> and the <code>$</code> with <code>$$</code>.</li> <li><code>ExecStart=</code>: Mounts the ZFS dataset at the specified mount point.</li> <li><code>ExecStop=</code>: Unmounts the ZFS dataset when the volume is no longer needed.</li> <li>When the <code>--container</code> and <code>--container-path</code> options are provided, the <code>bro-volume</code> script automatically modifies the existing container quadlet to integrate the volume:</li> <li>Adds systemd service dependencies (<code>PropagatesStopTo=</code>, <code>BindsTo=</code>, and <code>After=</code>) in the <code>[Unit]</code> section to properly link the volume service lifecycle with the container service.</li> <li>Adds a <code>Volume=</code> directive in the <code>[Container]</code> section with the format <code>MOUNT_POINT:CONTAINER_PATH</code> to bind mount the ZFS dataset into the container at the specified path.</li> <li>Removes any existing volume-related dependencies and volume mounts to the same container path to prevent conflicts.</li> <li>Uses the correct systemd service naming convention where a volume quadlet named <code>NAME.volume</code> becomes the service <code>NAME-volume.service</code>.</li> <li>If the path <code>/var/lib/containers/unprivileged</code> exists, the <code>bro-volume</code> script creates a new user account under this base directory for unprivileged Podman containers by delegating user creation to the <code>bro-user</code> script if the specified owner user does not already exist.</li> <li>If the path does not exist, it fails with an error.</li> <li>The <code>bro-volume</code> script can be extended in the future to support additional features as needed.</li> <li>The <code>bro-volume</code> script is intended to be used by system administrators to create and manage ZFS backed volumes for running rootless Podman containers in a secure and isolated manner.</li> <li>The <code>bro-volume</code> script is part of the Brothaman project and is licensed under the ASL 2.0 License.</li> <li>The <code>bro-volume</code> script is maintained as part of the Brothaman scripts package and should be kept up to date with the latest features and security patches.</li> <li>The <code>bro-volume</code> script ensures that the created ZFS dataset has delegated permissions for the specified unprivileged user to manage the dataset without requiring root privileges. This is done by setting the appropriate unit.list and operational allow list files with entries for the zfs-helper in the /etc/zfs-helper/policy.d/ directories. <li>The <code>bro-volume</code> script verifies that the specified unprivileged user has the necessary permissions to use the created ZFS dataset as a Podman volume.</li> <li>The <code>bro-volume</code> script provides error handling and validation for the command line arguments to ensure that the specified parameters are valid and that the ZFS dataset can be created successfully. This includes:</li> <li>Requiring <code>--container-path</code> when <code>--container</code> is specified to ensure explicit container mount path specification.</li> <li>Preventing use of <code>--container-path</code> without <code>--container</code> to maintain logical argument relationships.</li> <li>Validating format and characters in volume names, owner usernames, and container names using regular expressions.</li> <li>Ensuring retention values are non-negative integers.</li> <li>Checking for existing volumes (ZFS dataset or quadlet file) and requiring explicit removal with <code>--remove</code> before recreating.</li> <li>Restricting <code>--remove</code> mode to only accept <code>--name</code> and <code>--owner</code> parameters for safety.</li> <li>The <code>bro-volume</code> script supports any type of container by requiring explicit specification of the container mount path via <code>--container-path</code>, removing previous hardcoded assumptions about specific container types (e.g., PostgreSQL).</li> <li>When using <code>--remove</code>, the script performs comprehensive cleanup including:</li> <li>Stopping the volume systemd service if running.</li> <li>Removing the volume quadlet file.</li> <li>Surgically removing only the specific volume's dependencies from all container quadlets that reference it, preserving other volumes.</li> <li>Destroying the ZFS dataset and all its snapshots.</li> <li>Cleaning up zfs-helper policy entries for the volume service and dataset (only if no other volumes use the same dataset).</li> <li>The removal process is designed to be safe and only affect the specified volume, leaving other volumes and their dependencies intact.</li> <li>The <code>bro-volume</code> script is intended to be used in conjunction with the <code>bro-user</code> script to create a complete environment for running unprivileged rootless Podman containers with ZFS backed volumes.</li>"}]}