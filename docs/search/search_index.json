{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Brothaman Overview","text":"<ul> <li>[ ] <code>bro-volume</code> \u2014 create/remove/manage rootless Podman volumes (ZFS-optional).</li> <li>[ ] <code>bro-network</code> \u2014 create/remove/manage rootless Podman networks (CNI + systemd-networkd).</li> <li>[v] <code>bro-user</code> \u2014 create/remove rootless users (lingering, subuid/gid, per-user storage).</li> <li>[ ] <code>bro-service</code> \u2014 create socket-activated services (Quadlet + systemd-socket-proxyd).</li> <li>[ ] <code>bro-compose</code> \u2014 convert docker-compose.yml into <code>bro-service</code> calls (no <code>-p</code> publishes).</li> <li>[ ] <code>bro-doctor</code> \u2014 optional diagnostics.</li> <li>[ ] <code>bro-install-podman</code> \u2014 installs or upgrades podman to latest from Alvistack.</li> </ul> <p>Principles: small scripts, idempotency, rootless-first, ZFS-optional, socket-activated composition, cross-user/host by on demand using sockets instead of orchestrating services.</p>"},{"location":"architecture/","title":"Architecture Overview","text":"<p>Administration best practices + systemd primitives + podman features</p> <ul> <li>secure container services and applications</li> <li>one minimal authoritative configuration</li> <li>fine control over noisy neighbors, renegades, and compromised containers</li> <li>elegantly managed cross-over container dependencies</li> <li>higher performance and more capabilities</li> </ul> <p>Brothaman uses a combination of system administration best practices with systemd primitives and Podman features to elegantly simplify, secure, and manage containerized services and compositions of dependent containers via docker-compose descriptor files.</p> <ul> <li>unprivileged podman</li> <li> <p>quadlet-patterns</p> </li> <li> <p>Quadlet based authoritative service configuration in systemd --user scopes</p> </li> <li>ZFS using fuse-overlayfs</li> <li>Systemd proxyd</li> <li>Systemd socket</li> <li>Fast networking with Pasta</li> </ul> <p>The best aspect differentiating Podman is its relentless pursuit to work harmoniously with systemd at the operating system level. Podman users easily hook containers into systemd as services using its <code>generate systemd</code> sub-command. Now there are even more powerful Quadlets.</p> <p>The plethora of systemd features makes it hard to notice glorious ways to make amazing things happen.</p> <p>Thankfully, systemd harmony is even better after this command was deprecated in favor of Quadlet. Unfortunately, many are reluctant to upgrade.</p> <p>Brothaman builds on systemd and Quadlet to run rootless containers that are activated by traffic.</p> <ol> <li>A <code>.socket</code> unit listens on a host port (or specific interfaces).</li> <li>When a client connects, <code>systemd-socket-proxyd</code> (<code>-Service</code>) forwards to a loopback internal port.</li> <li>The proxied connection activates the container via a Quadlet <code>.container</code> unit.</li> <li>No <code>-p</code> port mapping is used; the proxy handles external exposure.</li> </ol> <p>This pattern composes across users and hosts: a client of a service uses it and activation follows demand.</p>"},{"location":"bro-helper/","title":"Brothaman's Proxyd Helper","text":"<p>The <code>bro-helper</code> enables <code>systemd-socket-proxyd</code> instances to be exec'd inside the network namespace of containers. By doing so it allows it to proxy traffic from a systemd.socket's file descriptor to container ports that network services bind to on container interfaces.</p> <ol> <li>Allows for <code>--network=none</code> for toighter security</li> <li>Proxied network services can bind to loopback interfaces</li> <li>Single hop proxy (one less without podman's -p mappings)</li> <li>Enables traffic triggering and on-demand container start</li> <li>Primitive used to enable cross over dependencies between unprivileged containers</li> </ol>"},{"location":"bro-helper/#so-what-why-do-we-need-this","title":"So what? Why do we need this?","text":"<p>Unprivileged containers using <code>slirp4netns</code> or <code>pasta</code> (or <code>--network=none</code> for that matter) have no IP addresses exposed to the host, and hence no ports to proxy traffic to the container with <code>systemd-socket-proxyd</code>. So why do we even bother using <code>systemd-socket-proxyd</code> when the -p switch could be used to proxy traffic from the host to the container: i.e. <code>-p 8080:80</code>?</p> <p>ROOT CAUSE: Podman's proxy mechanism does not work with systemd.socket traffic triggering nor does it support server socket (listener) file descriptor passing while <code>systemd-socket-proxyd</code> does.</p>"},{"location":"bro-helper/#using-both-together-sucks","title":"Using both together sucks","text":"<p>You could make them both work together. Presume an NGINX container with the daemon running on its default port 80. Unprivileged containers cannot map container ports to host ports below 1024 so we make podman proxy host port 8080 to container port 80.</p> <p>Yet we still want to trigger containers to start when traffic appears using systemd sockets. Say we use a port for it at 8081. Then the <code>systemd-socket-proxyd</code> daemon proxies traffic to and from port 8080 to 8081.</p> <p>Using them together now doubles the proxied traffic overhead which adds latency and you have an extra bogus port sitting out there. With <code>bro-helper</code> one host port and one container port is needed. Most importantly, there's no additional stream copy and transfer across ports for absolutely no reason.</p>"},{"location":"bro-helper/#security-restrictions","title":"Security Restrictions","text":"<p>The <code>bro-helper</code> is a tiny c-program with slightly augmented capabilities allowing it to set the netns and spawns commands within it. It can exec <code>systemd-socket-proxyd</code>, <code>ip</code>, and <code>netstat</code> commands only. Furthermore, it only works on containers running as the invoking user.</p> <p>Least privilege is achieved by using a very narrow capability addition while using that capability in highly specific situations. Meanwhile we do not fork and honor the passed host file descriptors (listener socket) so are handed off to <code>systemd-socket-proxyd</code> running in the container.</p> <ul> <li>Find the netns you want (e.g., /proc/${PID}/ns/net),</li> <li>Join it with setns(fd, CLONE_NEWNET),</li> <li>Drop any capabilities it no longer needs,</li> <li>Exec the target process.</li> </ul> <p>That's it\u2014no daemons, no forking trees, just a short, auditable path.</p> <p>Two layers work together to maintain least privilege:</p>"},{"location":"bro-helper/#1-the-unit-file-constrains-what-the-process-can-ever-have","title":"1. The unit file constrains what the process can ever have","text":"<ul> <li><code>CapabilityBoundingSet=CAP_SYS_ADMIN CAP_NET_ADMIN</code>     \u2192 Even if the binary tried, it cannot gain caps outside this set.</li> <li><code>AmbientCapabilities=CAP_SYS_ADMIN CAP_NET_ADMIN</code> (or file caps on the binary)     \u2192 Ensures the helper starts with just the tiny set it needs.</li> <li><code>NoNewPrivileges=no</code>     \u2192 Required if you rely on ambient caps or file caps across <code>execve()</code>.</li> </ul>"},{"location":"bro-helper/#2-the-program-drops-capabilities-as-soon-as-its-done-with-setns","title":"2. The program drops capabilities as soon as it\u2019s done with <code>setns()</code>","text":"<p>Right after <code>setns()</code>, it clears its capability sets so the final <code>exec()</code>ed program runs unprivileged (or with only what you explicitly keep). Concretely:</p> <ul> <li>Use libcap to set effective+permitted+inheritable = empty before <code>execvp()</code>.</li> <li>Optionally lock things down further by dropping from the bounding set via <code>prctl(PR_CAPBSET_DROP, ...)</code> (irreversible for the lifetime of the process).</li> <li>Optionally set <code>PR_SET_NO_NEW_PRIVS</code> to 1 after dropping caps.</li> </ul> <p>Result: the helper temporarily uses <code>CAP_SYS_ADMIN</code> to perform one privileged kernel call, then throws away the keys.</p>"},{"location":"bro-helper/#what-it-does-conceptually","title":"What it does (conceptually)","text":"<ul> <li>Inputs: <code>--pid &lt;PID&gt;</code> (or sometimes <code>--netns-path &lt;path&gt;</code>), <code>-- &lt;cmd&gt; [args\u2026]</code></li> <li>Open the namespace: <code>fd = open(\"/proc/PID/ns/net\", O_RDONLY|O_CLOEXEC)</code></li> <li>Join it: <code>setns(fd, CLONE_NEWNET)</code></li> <li>Harden &amp; de-priv:</li> <li>Clear dangerous env (PATH, IFS if you're paranoid), set <code>umask(077)</code></li> <li>Drop capabilities (details below)</li> <li>Optionally set <code>prctl(PR_SET_NO_NEW_PRIVS, 1)</code> after dropping caps      </li> <li>Hand off: <code>execvp(\"systemd-socket-proxyd\", argv)</code>     From here, proxyd runs inside that netns, inheriting only the minimal privileges you allow.</li> </ul>"},{"location":"bro-helper/#why-it-needs-some-capabilities","title":"Why it needs (some) capabilities","text":"<ul> <li>To call <code>setns(CLONE_NEWNET)</code> into a netns you don't \u201cown\u201d, the kernel requires <code>CAP_SYS_ADMIN</code> in the owning user namespace of that netns. We own it but are still required to have the permission.</li> <li>You don\u2019t need <code>CAP_NET_ADMIN</code> to enter the netns, but tools you run inside (like <code>ip addr</code>) may need it to mutate interfaces. <code>systemd-socket-proxyd</code> typically doesn't need <code>CAP_NET_ADMIN</code>; it just opens sockets.</li> <li>So: minimum to enter is <code>CAP_SYS_ADMIN</code>. If you also run diagnostics like <code>ip</code> inside the netns, give <code>CAP_NET_ADMIN</code> too (and then drop it before you <code>exec proxyd</code> if proxyd doesn\u2019t need it).</li> </ul>"},{"location":"bro-helper/#more-security-options-for-the-next-iteration","title":"More security options for the next iteration","text":"<ul> <li>Validate the target: ensure the PID exists and (optionally) matches the expected user/UID for your trust model.</li> <li>Avoid TOCTOU: once you open the netns fd, you're safe; don't re-resolve paths again.</li> <li>Logging: on failure, log <code>errno</code> and the exact syscall (helps when seccomp/LSM interferes).</li> <li>Interface: support <code>--netns-path</code> as an alternative to <code>--pid</code>, so you can bind a stable symlink like <code>/run/netns/nginx-proxyd</code>.</li> </ul> <p>This is why the approach is safe and composable: short-lived privilege, immediate drop, tight unit caps, and the actual worker (<code>systemd-socket-proxyd</code>) runs with regular user privileges in the desired netns.</p>"},{"location":"compose-conversion/","title":"Compose Conversion","text":"<p>WIP: Need to look at how we will deal with all aspects of docker-compose files. We need to look at how various entities in the compose file are best converted into Quadlet files (i.e. volumes).</p> <p><code>bro-compose</code> reads <code>docker-compose.yml</code> and synthesizes per-service calls to <code>bro-service</code> and potentially other new Brothaman commands:</p> <ul> <li>Each published port becomes a <code>.socket</code> + proxyd pair</li> <li>Containers bind to container interface ports accessible to the host and proxyd</li> <li>Dependencies prefer usage (client connects to server socket) rather than orchestration</li> <li>Optional service\u2192user mapping allows separation of privileges</li> </ul>"},{"location":"development/","title":"Development Guide","text":"<p>This guide covers development practices, conventions, and contribution guidelines for the brothaman project.</p>"},{"location":"development/#naming-conventions","title":"Naming Conventions","text":"<p>Brothaman follows a consistent dual naming convention across all components:</p>"},{"location":"development/#package-names","title":"Package Names","text":"<p>All Debian packages use the full prefix <code>brothaman-*</code>:</p> <ul> <li><code>brothaman-helper</code> - Network namespace helper utility</li> <li><code>brothaman-compose</code> - Container compose management</li> <li><code>brothaman-service</code> - Service lifecycle management  </li> <li><code>brothaman-install-deps</code> - Dependency installation utilities</li> <li><code>brothaman-zfs</code> - ZFS management tools</li> </ul> <p>Rationale: Full package names provide clear identification in APT repositories and package listings.</p>"},{"location":"development/#command-names","title":"Command Names","text":"<p>All executable commands use the short prefix <code>bro-*</code>:</p> <ul> <li><code>bro-helper</code> - Network namespace helper</li> <li><code>bro-compose</code> - Container compose management</li> <li><code>bro-service</code> - Service lifecycle management</li> <li><code>bro-install-deps</code> - Install system dependencies</li> <li><code>bro-install-zfs</code> - Install ZFS utilities</li> </ul> <p>Rationale: Short command names are convenient for daily terminal usage and tab completion.</p>"},{"location":"development/#examples","title":"Examples","text":"<pre><code># Install the package\nsudo apt install brothaman-helper\n\n# Use the command\nbro-helper --netns /run/user/1000/netns/podman -- curl ifconfig.me\n\n# Search for all brothaman packages\napt search brothaman-*\n\n# Tab complete all bro commands  \nbro-&lt;TAB&gt;&lt;TAB&gt;\n</code></pre>"},{"location":"development/#package-structure","title":"Package Structure","text":"<p>Each brothaman component follows this standard Debian package structure:</p> <pre><code>pkgs/brothaman-&lt;name&gt;/\n\u251c\u2500\u2500 DEBIAN/\n\u2502   \u251c\u2500\u2500 control          # Package metadata\n\u2502   \u251c\u2500\u2500 postinst         # Post-installation script\n\u2502   \u2514\u2500\u2500 prerm            # Pre-removal script (if needed)\n\u251c\u2500\u2500 usr/local/bin/       # Executable binaries\n\u2502   \u2514\u2500\u2500 bro-&lt;name&gt;       # Main executable\n\u251c\u2500\u2500 src/                 # Source code (for compiled tools)\n\u2502   \u251c\u2500\u2500 Makefile         # Build configuration\n\u2502   \u2514\u2500\u2500 *.c/*.go/etc     # Source files\n\u2514\u2500\u2500 build.sh             # Build script (optional)\n</code></pre>"},{"location":"development/#build-process","title":"Build Process","text":"<p>Packages are built using the containerized build system:</p> <pre><code># Build all packages\n./scripts/build.sh\n\n# Build specific components\n./scripts/mkdebs.sh\n\n# Generate documentation\n./scripts/mkdocs.sh\n\n# Create repository\n./scripts/mkrepo.sh\n</code></pre>"},{"location":"development/#development-workflow","title":"Development Workflow","text":"<ol> <li>Create Package Structure: Add new package to <code>pkgs/brothaman-&lt;name&gt;/</code></li> <li>Add DEBIAN Control Files: Define package metadata and dependencies  </li> <li>Implement Functionality: Add source code and build configuration</li> <li>Update Documentation: Add or update relevant markdown files</li> <li>Test Build: Run <code>./scripts/build.sh</code> to verify package creation</li> <li>Update Navigation: Add new docs to <code>mkdocs.yml</code> navigation</li> </ol>"},{"location":"development/#code-standards","title":"Code Standards","text":"<ul> <li>Shell Scripts: Follow bash best practices with <code>set -euo pipefail</code></li> <li>C Code: Use standard GNU C with appropriate compiler warnings</li> <li>Go Code: Follow standard Go formatting and conventions</li> <li>Documentation: Use clear, concise markdown with examples</li> </ul>"},{"location":"development/#security-considerations","title":"Security Considerations","text":"<ul> <li>All network namespace operations validate ownership</li> <li>Capabilities are set via postinst scripts, not SUID binaries</li> <li>Commands use allowlists for permitted operations</li> <li>ZFS operations respect user permissions and quotas</li> </ul>"},{"location":"development/#testing","title":"Testing","text":"<ul> <li>Unit tests in <code>tests/</code> directory</li> <li>Integration tests via Vagrant environments  </li> <li>Package installation testing in clean containers</li> <li>Documentation validation via MkDocs build</li> </ul>"},{"location":"development/#contributing","title":"Contributing","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch: <code>git checkout -b feature/new-tool</code></li> <li>Follow naming conventions for packages and commands</li> <li>Add tests and documentation</li> <li>Submit a pull request</li> </ol>"},{"location":"development/#release-process","title":"Release Process","text":"<ol> <li>Update version numbers in package control files</li> <li>Run full build: <code>./scripts/build.sh</code> </li> <li>Sign repository: <code>./scripts/signrepo.sh</code></li> <li>Publish changes: <code>./scripts/publish.sh</code></li> <li>Create release: <code>./scripts/release.sh</code></li> </ol>"},{"location":"dictionary/","title":"Dictionary","text":"<ul> <li>Bimodal script: A wrapper that runs an Ansible play by default and a shell implementation with <code>--script</code>.</li> <li>Lingering: Systemd capability allowing user units to run without an interactive login.</li> <li>Quadlet: Systemd generator that converts <code>.container</code> / <code>.volume</code> files into native units for Podman.</li> <li>Socket activation: Starting services upon first connection to a <code>.socket</code> listener.</li> <li>systemd-socket-proxyd: Small proxy service that forwards accepted sockets to a target <code>IP:PORT</code>.</li> <li>Graphroot: Podman\u2019s content store path; per-user when running rootless.</li> <li>User Scope: Refers to systemd services running the same system account: these services have user scope and can be ordered with respect to one another using dependency directives</li> <li>Unprivileged account</li> <li>Unprivileged user</li> <li>Unprivileged podman</li> <li>Non-root account</li> <li>OS account</li> </ul>"},{"location":"directions/","title":"Directions","text":"<ol> <li> <p>Create a <code>${major}.${minor}.${micro}</code> component based version comparison function in the common library to compare version numbers. It takes two version arguments. If the first is higher and more recent it returns 1. If the second is higher and more recent it returns -1. If both are the same it returns 0. WARNING: make sure comparisons are not lexical but numeric value based on all components. This will be used to compare installed versions against what is available using the ALVISTACK_VERSION.</p> </li> <li> <p>Always favor of the <code>alvistack</code> to install from OpenSuse instead of the distributions packaged podman. The <code>bro-install</code> installer script (which also uninstalls) first checks if the <code>alvistack</code> package repository is setup with its keys. If not, any existing podman is removed and the repository is installed. If the repository is already setup, the installer script checks if podman is installed, and if so, the installer script checks the installed version to see if it is above or equal to ALVISTACK_VERSION. If the installed version is lower, it is removed and podman is reinstalled from the <code>alvistack</code> repository.</p> </li> <li> <p>The use of CoW filesystems for backing stores will be optional and an independent axis of operation. The script uses whatever it finds (ZFS, BTRFS, or none). Right now in the last <code>rootless-podman</code> script, if ZFS is not present, no backing store is used for user homes under the USER_HOME_BASE which is a good fallback behavior. Without CoW backing stores, users miss out on leveraging snapshotting features for various operations, but things still work.</p> </li> </ol> <p>NOTE: it turns out, the quadlet generated by chatgpt for the postgres container is faulty and not the podman installation. I tried a minimal quadlet and it worked. Let us incorporate that into the training and progression towards the final brothaman configuration.</p>"},{"location":"project-plan/","title":"Project Plan","text":""},{"location":"project-plan/#goals","title":"Goals","text":"<ul> <li>Modularize rootless Podman + ZFS workflow into small, idempotent tools.</li> <li>Prefer systemd primitives (socket activation + proxyd) over container port publishing.</li> <li>Ensure each script maps 1:1 to an Ansible role (default mode), with a fallback shell mode via <code>--script</code>.</li> </ul>"},{"location":"project-plan/#work-breakdown-structure","title":"Work Breakdown Structure","text":"<ol> <li>ZFS Installation \u2014 <code>bro-install-zfs</code></li> <li>Test Zpool \u2014 <code>bro-test-zpool</code></li> <li>Dependencies \u2014 <code>bro-install-deps</code></li> <li>User Management \u2014 <code>bro-user</code></li> <li>Service Generator \u2014 <code>bro-service</code></li> <li>Compose Converter \u2014 <code>bro-compose</code></li> <li>Doctor \u2014 <code>bro-doctor</code></li> <li>Documentation &amp; Diagrams \u2014 mkdocs site, mermaid diagrams</li> </ol>"},{"location":"project-plan/#deliverables","title":"Deliverables","text":"<ul> <li>Executable <code>bro-*</code> wrappers (bimodal: Ansible default, <code>--script</code> for shell fallback).</li> <li>Ansible collection skeleton: <code>akarasulu.brothaman</code> with roles matching each script.</li> <li>MkDocs site: this plan, architecture, how-tos, and a project dictionary.</li> <li>Example Vagrant environment for Debian 12 smoke testing.</li> </ul>"},{"location":"project-plan/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li>All scripts are idempotent and safe to re-run.</li> <li>Socket-activated services start on demand via <code>systemd-socket-proxyd</code>.</li> <li><code>bro-compose</code> generates deterministic plans and compatible service units.</li> <li>Documentation is sufficient to onboard a new operator using only this site.</li> </ul>"},{"location":"project-plan/#future-extensions","title":"Future Extensions","text":"<ul> <li>Cross-host activation cookbook.</li> <li>Secrets and TLS patterns.</li> <li>CI smoke tests for Vagrant-based runs.</li> </ul>"},{"location":"quadlet-patterns/","title":"Quadlet Patterns","text":"<p>Common fields we will generate: - <code>Image=</code> to select the container image - <code>EnvironmentFile=</code> and <code>Volume=</code> entries - <code>ContainerName=</code> for stable names - <code>Exec=</code> or <code>Args=</code> for explicit command overrides - <code>After=</code>/<code>Requires=</code> for optional intra-user ordering</p>"},{"location":"socket-activation/","title":"Socket Activation","text":"<ul> <li><code>NAME.socket</code>: Listens on <code>IP:PORT</code> in the user systemd scope.</li> <li><code>NAME-proxy.service</code>: Runs <code>systemd-socket-proxyd</code>, forwarding accepted fds to <code>127.0.0.1:INTERNAL_PORT</code>.</li> <li><code>NAME.container</code>: Quadlet container that binds only to loopback on the internal port.</li> </ul> <p>Flow: Client \u2192 <code>NAME.socket</code> \u2192 <code>NAME-proxy.service</code> \u2192 container <code>127.0.0.1:INTERNAL_PORT</code>.</p>"},{"location":"unprivileged-podman/","title":"Unprivileged Podman","text":"<p>Unprivileged Podman refers to the use of Podman by restricted non-root OS accounts rather than the privileged root user. Podman commands and containers run as non-root system account processes while using bounded resources with quota restricted limits specifically assigned to the unprivileged account; i.e. cpu, memory and disk quotas.</p> <p>Brothaman creates Podman containers and composable container applications on isolated unprivileged OS accounts.</p>"},{"location":"unprivileged-podman/#security-pattern","title":"Security Pattern","text":"<p>Running applications and services pulled from repositories on the Internet in restricted jails is a sensible security pattern right? You don't want to run other people's shit on your shit as root. Each container adds to the overall attack surface in their own [un]predictable ways.</p> <p>Running containers (or applications composed of containers) in restricted environments minimizes the blast radius on compromise. Even without compromise, noisy neighbors or renegade containers MUST BE limited and throttled to functionally protect system resources. With compromise, the account, its processes, and resources need to be frozen and quarantined. Perhaps snapshotted and archived before being immediately destroyed for post-mortem forensic analysis.</p> <p>See zfs</p>"},{"location":"unprivileged-podman/#storage-concerns","title":"Storage Concerns","text":"<p>Unprivileged means much more than just jailed service and application processes with their CPU and memory quotas. It includes all resources with storage being the most critical of them all. After all, the most constraining resources on most systems is almost always storage. Even older CPU's with slower clocks and memory buses can almost always saturate storage I/O even with storage technologies 10-years into the future.</p> <p>Storage is the key resource needing the most protection, yet we often protect it least of all. Brothaman forces the use of ZFS and cgroup v2 blkio limits to cover your back. ZFS as a copy-on-write (CoW) filesystem with capacity quotas makes snapshotting, capacity limiting, and quarantining of containerized services and applications a cake walk.</p>"},{"location":"user-scope/","title":"User Scope","text":""},{"location":"users-and-lingering/","title":"Users &amp; Lingering","text":"<p>Rootless services run in user systemd scope. To run without an active login: - Enable lingering: <code>loginctl enable-linger USER</code></p> <p>Per-user containers config (<code>~/.config/containers/storage.conf</code>):</p> <pre><code>[storage]\ndriver = \"overlay\"\nrunroot = \"/run/user/UID\"\ngraphroot = \"/var/lib/containers/unprivileged/USER\"\n\n[storage.options.overlay]\nmount_program = \"/usr/bin/fuse-overlayfs\"\n</code></pre> <p>Subuid/subgid mappings must be present for rootless Podman.</p>"},{"location":"zfs/","title":"ZFS","text":"<p>Brothaman uses ZFS to implement storage I/O throttling of unprivileged container services. As the premier copy-on-write (CoW) file system with rapid low cost snapshotting and rollback ZFS is also ideal for snapshotting, quarantining and archiving compromised containers for forensic analysis.</p>"},{"location":"zfs/#unprivileged-with-zfs","title":"Unprivileged with ZFS","text":"<p>Podman's ZFS driver uses ZFS snapshots and mountpoints for container layering, but it is not stable and only supports privileged Podman. ZFS delegation just does not provide unprivileged users with the key mounting privileges needed. Once it does Brothaman can reconsider using it as the main driver. Don't hold your breath on it though; Linux might never let ZFS do this as long as it keeps one global mount space.</p>"},{"location":"zfs/#fuse-overlayfs-on-zfs","title":"FUSE Overlayfs on ZFS","text":"<p>Brothaman DOES NOT use ZFS for image layer management but it does use it as the underlying backing store for unprivileged users. Brothaman uses the Overlayfs driver instead of the default vfs driver to greatly improve performance and reduce the overhead of layer storage. To do this in unprivileged environments requires the use of the <code>fuse-overlayfs</code> program as the <code>mount_program</code> (and <code>mountopt = 'nodev'</code>) within the unprivileged account's storage configuration file at <code>${USER_HOME}/.config/containers/storage.conf</code>.</p>"},{"location":"zfs/#zfs-dataset-settings","title":"ZFS Dataset Settings","text":"<p>Each unprivileged user's home directory is mounted using a new dedicated ZFS data set. That dataset is configured with the following attributed values:</p> <ul> <li><code>xattr=sa</code></li> <li><code>acltype=posixacl</code></li> <li><code>aclinherit=passthrough</code></li> <li><code>aclmode=passthrough</code></li> <li><code>mountpoint=\"${USER_HOME}\"</code></li> <li><code>compression=zstd</code></li> <li><code>atime=off</code></li> <li><code>recordsize=128</code></li> </ul>"},{"location":"zfs/#brothaman-tools","title":"Brothaman Tools","text":"<ul> <li><code>bro-install-zfs</code> installs ZFS on Debian 12 (no pools).</li> <li><code>bro-test-zpool</code> creates a file-backed pool for development (default name <code>brotest</code>).</li> </ul> <p>Recommended dataset for per-user graphroots: * <code>POOL/containers/USER</code> mounted at <code>/var/lib/containers/unprivileged/USER</code> * Suggested props: <code>compression=zstd</code>, <code>atime=off</code></p>"},{"location":"diagrams/architecture/","title":"System Architecture Diagram","text":"<p>This diagram shows the overall system architecture for Brothaman's socket-activated container services.</p> <pre><code>graph LR\nClient((Client)) --&gt;|TCP| NAME_socket[NAME.socket]\nNAME_socket --&gt; NAME_proxy[NAME-proxy.service (systemd-socket-proxyd)]\nNAME_proxy --&gt;|forward to 127.0.0.1:INTERNAL_PORT| NAME_container[NAME.container (Quadlet)]\n</code></pre>"},{"location":"diagrams/architecture/#architecture-components","title":"Architecture Components","text":"<ul> <li>Client: External client connecting to the service</li> <li>NAME.socket: systemd socket unit that listens for incoming connections</li> <li>NAME-proxy.service: systemd-socket-proxyd service that forwards connections</li> <li>NAME.container: Podman Quadlet container running the actual service</li> </ul>"},{"location":"diagrams/architecture/#flow-description","title":"Flow Description","text":"<ol> <li>Client connects to the external port managed by the socket unit</li> <li>systemd activates the proxy service when a connection is received</li> <li>The proxy forwards the connection to the container's internal port</li> <li>The container handles the actual service logic</li> </ol> <p>This architecture enables: - Socket activation: Services start only when needed - Resource efficiency: Containers don't run when idle - Security: Internal ports are not directly exposed - Systemd integration: Full lifecycle management through systemd</p>"},{"location":"diagrams/per-service-pattern/","title":"Per-Service Pattern Diagram","text":"<p>This diagram illustrates the per-service pattern used in Brothaman for socket-activated container services.</p> <pre><code>flowchart TD\nA[NAME.socket] --&gt; B[NAME-proxy.service]\nB --&gt; C[NAME.container]\nC --&gt;|binds| D[127.0.0.1:INTERNAL_PORT]\n</code></pre>"},{"location":"diagrams/per-service-pattern/#service-pattern-components","title":"Service Pattern Components","text":"<ul> <li>NAME.socket: Socket unit that defines the listening address and port</li> <li>NAME-proxy.service: Proxy service using systemd-socket-proxyd</li> <li>NAME.container: Quadlet container descriptor</li> <li>127.0.0.1:INTERNAL_PORT: Container's internal service port</li> </ul>"},{"location":"diagrams/per-service-pattern/#pattern-benefits","title":"Pattern Benefits","text":"<p>This standardized pattern provides:</p>"},{"location":"diagrams/per-service-pattern/#socket-activation","title":"Socket Activation","text":"<ul> <li>Services start automatically when clients connect</li> <li>Zero resource usage when idle</li> <li>Fast startup times for lightweight containers</li> </ul>"},{"location":"diagrams/per-service-pattern/#security-isolation","title":"Security Isolation","text":"<ul> <li>Containers bind only to localhost</li> <li>External access controlled through systemd socket units</li> <li>No direct container port exposure</li> </ul>"},{"location":"diagrams/per-service-pattern/#systemd-integration","title":"Systemd Integration","text":"<ul> <li>Full lifecycle management</li> <li>Logging integration via journald</li> <li>Service dependencies and ordering</li> <li>Resource limits and controls</li> </ul>"},{"location":"diagrams/per-service-pattern/#scalability","title":"Scalability","text":"<ul> <li>Each service follows the same pattern</li> <li>Easy to replicate for new services</li> <li>Consistent configuration across services</li> </ul>"},{"location":"diagrams/per-service-pattern/#implementation-example","title":"Implementation Example","text":"<p>For a service named <code>myapp</code>: - <code>myapp.socket</code> - listens on external port (e.g., 8080) - <code>myapp-proxy.service</code> - forwards to localhost:3000 - <code>myapp.container</code> - runs container with internal port 3000</p>"},{"location":"labs/","title":"Brothaman Labs","text":"<p>Welcome to the Brothaman hands-on laboratory series! These labs provide step-by-step tutorials for learning rootless container management with ZFS, systemd, and Podman.</p>"},{"location":"labs/#lab-structure","title":"Lab Structure","text":"<p>The labs are organized into three main areas:</p>"},{"location":"labs/#foundation-labs","title":"Foundation Labs","text":"<p>Essential concepts and setup procedures: - Setup Environment - Configure your test environment with Vagrant - User Lingering - Understand systemd user services and lingering - Quadlet Basics - Learn container service descriptors</p>"},{"location":"labs/#storage-data-labs","title":"Storage &amp; Data Labs","text":"<p>Storage management and persistence: - Volumes &amp; ZFS - Bind mounts, persistent volumes, and ZFS snapshots - Debugging - Container troubleshooting and log analysis</p>"},{"location":"labs/#networking-services-labs","title":"Networking &amp; Services Labs","text":"<p>Advanced networking and service composition: - Network Configuration - Container networking and connectivity - Database Admin - PostgreSQL with pgAdmin setup - Pod Management - Multi-container pod orchestration - Proxy Testing - Load balancing and proxy configuration</p>"},{"location":"labs/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic familiarity with Linux command line</li> <li>Understanding of containers (Docker/Podman)</li> <li>Access to a Linux system with systemd</li> <li>Vagrant installed (for VM-based labs)</li> </ul>"},{"location":"labs/#getting-started","title":"Getting Started","text":"<p>Begin with the Setup Environment lab to configure your testing environment, then proceed through the Foundation labs before moving to more advanced topics.</p> <p>Each lab includes: - Clear learning objectives - Step-by-step instructions - Troubleshooting guidance - Cleanup procedures - VM snapshot management</p> <p>Ready to begin? Start with Setup Environment!</p>"},{"location":"labs/0.%20setup/","title":"Lab Setup","text":""},{"location":"labs/0.%20setup/#install-kvm-hypervisor","title":"Install KVM Hypervisor","text":"<p>You want to install vagrant and the libvirt provider on your Debian/Manjaro Linux then install the brothaman VM:</p> <pre><code>sudo apt-get install -y git vagrant vagrant-libvirt \\\n  libvirt-daemon-system libvirt-clients virtinst virt-manager \\\n  qemu-kvm qemu-utils\n</code></pre>"},{"location":"labs/0.%20setup/#clone-and-build-the-brothaman-z12-box","title":"Clone and build the brothaman-z12-box","text":"<p>A one time build of the brothaman/z12 vagrant box is required before first use:</p> <pre><code>git clone https://github.com/brothaman-org/brothaman-z12-box\ncd brothaman-z12-box\n./build.sh\n</code></pre>"},{"location":"labs/0.%20setup/#clone-and-fire-up-the-brothaman-lab-vm","title":"Clone and fire up the brothaman lab VM","text":"<p>Now we fire up the brothaman lab VM:</p> <pre><code>git clone https://github.com/brothaman-org/brothaman\ncd brothaman\nvagrant up &amp;&amp; vagrant snapshot save before-starting\n</code></pre>"},{"location":"labs/1.%20lingering/","title":"Lingering","text":"<p>In this lab we will prove that systemd lingering works when enabled for an unprivileged user and demonstrate its impact on user-scoped services that persist across reboots. You will:</p> <ol> <li>Create an unprivileged lingering user and enable systemd commands</li> <li>Create, install, and start test systemd user-scoped service</li> <li>Reboot with lingering</li> <li>Check if the test service runs after reboot (should be running)</li> <li>Disable lingering and reboot</li> <li>Check if the test service runs after reboot (should NOT be running)</li> <li>Re-enable lingering again and see what happens</li> <li>Lessons learned</li> </ol>"},{"location":"labs/1.%20lingering/#0-snapshot","title":"0. Snapshot","text":"<p>Since this test runs inside a Vagrant VM, capture a baseline snapshot before making any changes. From your host machine (in the same directory as your Vagrantfile):</p> <pre><code>if ! vagrant snapshot list 2&gt;&amp;1 | grep before-starting &gt;/dev/null; then\n    vagrant snapshot save before-starting;\nelse\n    echo before-starting snapshot already exists;\nfi\n</code></pre> <p>This allows you to roll the VM back to its exact pre-test state later.</p>"},{"location":"labs/1.%20lingering/#1-create-an-unprivileged-lingering-user-and-enable-systemd-commands","title":"1. Create an unprivileged lingering user and enable systemd commands","text":"<p>ATTENTION: Make sure to SSH into the vagrant vm with <code>vagrant ssh</code></p> <p>Create a dedicated unprivileged test user:</p> <pre><code># create and check user\nsudo useradd -m -s /bin/bash lingeruser\nid lingeruser\ngetent passwd lingeruser\nsudo -iu lingeruser ls -ld .\n</code></pre> <p>This ensures the user got created with home directory (<code>/home/lingeruser</code>) and we can execute commands as the user. Now let's make the user linger but notice the checks before and after for an instance of systemd running as the lingeruser:</p> <pre><code>echo \"checking if new user's systemd instance is running\"\nsudo ps -ux -U lingeruser | grep 'systemd --user' | grep -v grep\necho no systemd from the ps command \u261d\ufe0f\necho\n\n# make the user linger\nsudo loginctl enable-linger lingeruser\necho \"checking if new user's systemd instance is running\"\nsleep 3\nsudo ps -ux -U lingeruser | grep 'systemd --user' | grep -v grep\necho see systemd now from the ps command \u261d\ufe0f\necho\n\n# check if we can use systemctl\nsudo -iu lingeruser systemctl --user list-units\necho \"NOTICE: using systemctl \u261d\ufe0f fails \ud83d\ude1e\"\n\n</code></pre> <p>Lingering is enabled using the loginctl command. Unfortunately, this user cannot use systemd commands until some basic settings are set.</p> <p>For user-mode systemd commands to work, the <code>XDG_RUNTIME_DIR</code> and <code>DBUS_SESSION_BUS_ADDRESS</code> shell variables MUST BE correctly set. We can do that system wide for all unprivileged users by adding the following script in <code>/etc/profile.d/xdg_runtime.sh</code>:</p> <pre><code>cat | sudo tee /etc/profile.d/xdg_runtime.sh &lt;&lt;'EOF'\nexport XDG_RUNTIME_DIR=/run/user/$(id -u)\nexport DBUS_SESSION_BUS_ADDRESS=unix:path=${XDG_RUNTIME_DIR}/bus\nEOF\n\nsudo chmod +x /etc/profile.d/xdg_runtime.sh\n\nsudo -iu lingeruser systemctl --user list-units | tee /dev/null\necho systemctl \u261d\ufe0f now succeeds \ud83d\udc4f\n\n</code></pre>"},{"location":"labs/1.%20lingering/#whats-going-on-here","title":"What's going on here?","text":"<p>Systemd runs in system mode as root. When \"enabled to\", systemd can also run in user mode with the <code>--user</code> switch. The systemd process actually runs as a specific user. Here in this lab it runs as the lingeruser.</p> <p>If you noticed the lingeruser's systemd process was not present before the <code>loginctl enable-linger lingeruser</code> command was run. Running the command fired up the systemd infrastructure for the lingeruser and the ps command after it showed the user's systemd instance now running. Furthermore, the enable-linger command makes sure, the user's systemd instance is always running, even when the user is not logged in.</p> <p>Other things are also started. Namely a dbus broker for the user's system components bus. System components, including systemd, talk to one another using this messaging system. In fact, systemctl sends commands to the systemd user instance using this bus. However, to attach to the broker and send commands to the systemd user instance it needs to connect to the bus. These environment variables tell it where to find the bus.</p>"},{"location":"labs/1.%20lingering/#user-manager","title":"User Manager","text":"<p>The user manager is the per-user systemd instance that manages all user-scope units. On a system with systemd, there are two main layers of system management:</p> Layer Runs as Controls System manager <code>PID 1</code> (<code>/lib/systemd/systemd</code>) system-wide units (<code>/etc/systemd/system/*.service</code>) User manager <code>/lib/systemd/systemd --user</code> (one per logged-in or lingering user) per-user units (<code>~/.config/systemd/user/*.service</code>) <p>So each logged-in user (or each \u201clingering\u201d user) gets their own lightweight systemd instance that controls user-scoped services, sockets, timers, and targets. You can see it with <code>ps -u lingeruser | grep systemd</code>.</p>"},{"location":"labs/1.%20lingering/#two-ways-the-user-manager-starts","title":"Two ways the user manager starts","text":"<ol> <li> <p>Interactive login (default behavior)     When a user logs in (via <code>ssh</code>, TTY, desktop, etc.), PAM runs <code>systemd --user</code>, creating <code>/run/user/&lt;uid&gt;/</code> and starting user services like <code>dbus.socket</code>, <code>xdg-desktop-portal.service</code>, etc.</p> </li> <li> <p>Lingering (non-interactive)     When you enable lingering via <code>loginctl enable-linger &lt;user&gt;</code>. Systemd ensures that the per-user manager (<code>user@&lt;uid&gt;.service</code>) starts at boot, even if the user never logs in. That service runs as root's child process under the system manager and spawns <code>/lib/systemd/systemd --user</code> for that UID.</p> </li> </ol>"},{"location":"labs/1.%20lingering/#relationship-to-runuserbus","title":"Relationship to /run/user/\\/bus <p>The D-Bus session bus socket at <code>/run/user/&lt;uid&gt;/bus</code> is created and managed by the user manager.</p> <ul> <li>The user manager starts <code>dbus.socket</code> and <code>dbus.service</code> (or <code>dbus-broker.service</code>). </li> <li><code>dbus.socket</code> creates the <code>bus</code> socket.</li> <li>Any client that connects to <code>/run/user/&lt;uid&gt;/bus</code> triggers D-Bus activation of the broker/service.</li> </ul> <p>If the user manager isn't running, there's nothing to start <code>dbus.socket</code>, so <code>/run/user/&lt;uid&gt;/bus</code> doesn't exist \u2014 even though <code>/run/user/&lt;uid&gt;/</code> might. See it all running:</p> <pre><code>vagrant@debian12:~$ sudo loginctl user-status lingeruser \nlingeruser (1001)\n           Since: Fri 2025-10-17 19:29:45 UTC; 24min ago\n           State: lingering\n          Linger: yes\n            Unit: user-1001.slice\n                  \u2514\u2500user@1001.service\n                    \u2514\u2500init.scope\n                      \u251c\u25001801 /lib/systemd/systemd --user\n                      \u2514\u25001803 \"(sd-pam)\"\n\n...\nOct 17 ... systemd[1801]: Starting dbus.socket - D-Bus User Message Bus Socket...\nOct 17 ... systemd[1801]: Listening on dbus.socket - D-Bus User Message Bus Socket.\n...\nOct 17 19:29:46 debian12 systemd[1801]: Startup finished in 147ms.\n</code></pre>","text":""},{"location":"labs/1.%20lingering/#2-create-install-and-start-test-systemd-user-service","title":"2. Create, install, and start test systemd user service","text":"<p>NOTE: Sudo into the lingeruser with <code>sudo su - lingeruser</code></p> <p>Let's create the service directory and add lingeruser's new user-scoped systemd service. The service loops forever printing to a log file in /tmp then sleeping for 10 seconds. </p> <pre><code>mkdir -p ~/.config/systemd/user\n\ncat &gt; ~/.config/systemd/user/test-linger.service&lt;&lt;'EOF'\n[Unit]\nDescription=Lingering Proof Test Service\n\n[Service]\nExecStart=/bin/bash -c 'while true; do echo \"$(date -Is) test-linger running\" &gt;&gt; /tmp/test-linger.log; sleep 10; done'\nRestart=always\nRestartSec=5\n\n[Install]\nWantedBy=default.target\n\nEOF\n\n</code></pre> <p>Enable and start the service like any other service, but for user scope we use the <code>--user</code> switch. We also confirm the service has started and is logging to the file:</p> <pre><code>systemctl --user daemon-reload\nsystemctl --user enable --now test-linger.service\nsystemctl --user status test-linger.service\nsystemctl --user list-units --type=service | grep \"test-linger\"\ntail -f /tmp/test-linger.log\n\n</code></pre> <p>NOTE: you might need to type q to quit the systemd unit listing command</p> <p>Ctrl-C to exit the tail output.</p>"},{"location":"labs/1.%20lingering/#3-reboot-with-lingering","title":"3. Reboot with lingering","text":"<p>Back as vagrant user:</p> <p><code>sudo reboot</code></p>"},{"location":"labs/1.%20lingering/#4-check-if-the-test-service-runs-after-reboot","title":"4. Check if the test service runs after reboot","text":"<p>SSH back into the vagrant machine and run this as the vagrant user:</p> <p><code>tail -f /tmp/test-linger.log</code></p> <p>You might press return then wait for another line to be printed in 10s. Press Ctrl-C to exit the tailed output.</p>"},{"location":"labs/1.%20lingering/#5-disable-lingering-and-reboot","title":"5. Disable lingering and reboot","text":"<pre><code>sudo loginctl disable-linger lingeruser\nsudo reboot\n</code></pre>"},{"location":"labs/1.%20lingering/#6-check-if-the-test-service-runs-after-reboot-should-not-be-running","title":"6. Check if the test service runs after reboot (should NOT be running)","text":"<p>SSH back into the vagrant machine and run this as the vagrant user:</p> <pre><code>~/Local/brothaman-labs vagrant ssh                                              Last login: Fri Oct 17 20:11:38 2025 from 192.168.122.1\nvagrant@debian12:~$ tail -f /tmp/test-linger.log\ntail: cannot open '/tmp/test-linger.log' for reading: No such file or directory\ntail: no files remaining\nvagrant@debian12:~$ sleep 10; tail -f /tmp/test-linger.log\ntail: cannot open '/tmp/test-linger.log' for reading: No such file or directory\ntail: no files remaining\nvagrant@debian12:~$ sudo ps -u lingeruser\n    PID TTY          TIME CMD\n</code></pre> <p>It is clear nothing is running any longer.</p>"},{"location":"labs/1.%20lingering/#7-re-enable-lingering-and-see-what-happens","title":"7. Re-enable lingering and see what happens","text":"<pre><code>echo \"checking if new user's systemd instance is running\"\nsudo ps -ux -U lingeruser | grep 'systemd --user' | grep -v grep\necho no systemd from the ps command \u261d\ufe0f\necho\n\n# make the user linger\nsudo loginctl enable-linger lingeruser\necho \"checking if new user's systemd instance is running\"\nsleep 3\nsudo ps -ux -U lingeruser | grep 'systemd --user' | grep -v grep\necho see systemd now from the ps command \u261d\ufe0f\nsudo ps -ux -U lingeruser | grep 'test-linger' | grep -v grep\necho see test-linger shell from the ps command \u261d\ufe0f\necho\n\n</code></pre> <p>Reboot and check again, the service should still be up so long as we do not turn off user lingering.</p>"},{"location":"labs/1.%20lingering/#8-lessons-learned","title":"8. Lessons learned","text":"<p>Through this lab we verified how systemd's user-scoped manager behaves under different conditions and why lingering is such an important feature for background or headless service users.</p>"},{"location":"labs/1.%20lingering/#key-takeaways","title":"Key takeaways","text":"<ol> <li> <p>User-mode systemd is separate from system-mode.     Every user runs their own <code>systemd --user</code> instance, called the user manager.     This instance controls user-scoped units located under <code>~/.config/systemd/user/</code>.</p> </li> <li> <p>Lingering keeps the user manager alive across reboots and logouts.     Without lingering, <code>systemd --user</code> only starts when the user logs in through PAM.     With lingering enabled (<code>loginctl enable-linger &lt;user&gt;</code>), the user\u2019s <code>user@UID.service</code> starts at boot, even if they never log in.</p> </li> <li> <p>Environment variables are critical. <code>XDG_RUNTIME_DIR</code> and <code>DBUS_SESSION_BUS_ADDRESS</code> must point to <code>/run/user/&lt;uid&gt;</code> and its bus socket so that commands like <code>systemctl --user</code> can talk to the user manager.     Adding <code>/etc/profile.d/xdg_runtime.sh</code> ensures these variables exist for all unprivileged users.</p> </li> <li> <p>The D-Bus socket (<code>/run/user/&lt;uid&gt;/bus</code>) belongs to the user manager.     It only exists while that per-user <code>systemd --user</code> instance is running.     When lingering is disabled or the manager stops, the socket disappears and user-scoped commands fail.</p> </li> <li> <p>Services enabled under <code>default.target</code> survive reboots.     Our test service <code>test-linger.service</code> continued running after every reboot while lingering was enabled, and stopped existing once lingering was disabled.</p> </li> <li> <p>Lingering makes user accounts behave like lightweight system services.     This is ideal for rootless containers, background daemons (e.g., PostgreSQL, Gitea), or personal automation that must persist without an active session.</p> </li> <li> <p>Each lingered user consumes persistent resources.     Because their <code>user@UID.service</code> remains active, each lingering user adds a small amount of memory and process overhead\u2014acceptable for a few users, but not something to enable for hundreds indiscriminately.</p> </li> </ol>"},{"location":"labs/1.%20lingering/#practical-implications","title":"Practical implications","text":"<ul> <li>Use lingering for any headless service accounts that must auto-start at boot and not depend on an interactive login.</li> <li>Always ensure that <code>dbus-user-session</code> or <code>dbus-broker</code> is installed so the per-user D-Bus bus can start.</li> <li>Confirm lingering state with <code>loginctl show-user &lt;user&gt; | grep Linger</code></li> <li>Verify the user manager is alive with<code>systemctl status user@$(id -u &lt;user&gt;).service</code></li> </ul>"},{"location":"labs/1.%20lingering/#final-observation","title":"Final observation","text":"<p>Once lingering is enabled, a normal unprivileged account becomes a first-class citizen in systemd's dependency graph \u2014 its services are started, stopped, and monitored just like system-level units.</p> <p>Disabling lingering cleanly removes that persistence, returning the user to an ephemeral, login-only environment.</p>"},{"location":"labs/1.%20lingering/#9-rollback","title":"9. Rollback","text":"<p>When finished, restore the VM to the baseline snapshot:</p> <pre><code>vagrant snapshot restore before-starting --no-provision\n</code></pre>"},{"location":"labs/2.%20quadlet/","title":"Quadlet","text":"<p>In this lab we install the latest Podman with Quadlet support and use it to create, install, and test a simple container service under an unprivileged lingering user.</p> <p>This builds on 1. lingering \u2014 make sure you have completed that experiment and verified that lingering is enabled for your test user. Here's a quick summary of steps:</p> <ol> <li>Uninstall system podman if present</li> <li>Install the latest podman from alvistack</li> <li>Drop in the test-quadlet.container</li> <li>Reboot test making sure it is up after reboot</li> </ol>"},{"location":"labs/2.%20quadlet/#0-snapshot","title":"0. Snapshot","text":"<p>Before making changes, capture a baseline snapshot of the VM so we can roll back cleanly if anything goes wrong:</p> <pre><code>if ! vagrant snapshot list 2&gt;&amp;1 | grep before-quadlet &gt;/dev/null; then\n  vagrant snapshot save before-quadlet;\nelse\n  echo before-quadlet snapshot already exists;\nfi\n</code></pre> <p>This ensures that you can easily revert to a clean pre-Quadlet state.</p>"},{"location":"labs/2.%20quadlet/#1-uninstall-system-podman-if-present","title":"1. Uninstall System Podman (if present)","text":"<p>First, remove any system-level Podman installation that might conflict with the user-space version we'll install next.</p> <pre><code>sudo apt remove -y podman podman-rootless podman-plugins containers-common\nsudo apt autoremove -y\n</code></pre> <p>After removal, confirm that no residual binaries or systemd services remain:</p> <pre><code>which podman || echo \"Podman removed\"\nsudo systemctl disable --now podman.service podman.socket 2&gt;/dev/null || true\n</code></pre> <p>You should see \u201cPodman removed\u201d printed and no active Podman system services.</p>"},{"location":"labs/2.%20quadlet/#2-install-the-latest-podman-from-alvistack","title":"2. Install the Latest Podman from Alvistack","text":"<p>Back to the vagrant user</p> <p>We'll now install the latest version of Podman from the Alvistack repository, which typically ships newer Podman releases with full Quadlet support.</p>"},{"location":"labs/2.%20quadlet/#21-add-the-repository","title":"2.1 Add the Repository","text":"<pre><code>sudo apt update\nsudo apt install -y curl gnupg2\ncurl -fsSL https://downloadcontent.opensuse.org/repositories/home:/alvistack/Debian_12/Release.key | \\\n  sudo gpg --dearmor -o /usr/share/keyrings/alvistack.gpg\n\necho \"deb [signed-by=/usr/share/keyrings/alvistack.gpg] http://downloadcontent.opensuse.org/repositories/home:/alvistack/Debian_12 ./\" | \\\n  sudo tee /etc/apt/sources.list.d/alvistack.list\n</code></pre>"},{"location":"labs/2.%20quadlet/#22-install-podman","title":"2.2 Install Podman","text":"<pre><code>sudo apt update\nsudo apt install -y \\\n  podman conmon crun catatonit \\\n  netavark aardvark-dns \\\n  slirp4netns uidmap \\\n  fuse-overlayfs dbus-user-session \\\n  iptables nftables \\\n  passt\n\n</code></pre>"},{"location":"labs/2.%20quadlet/#23-verify-quadlet-support","title":"2.3 Verify Quadlet Support","text":"<p>Back to the lingeruser: <code>sudo su - lingeruser</code></p> <pre><code>podman quadlet --help\n</code></pre> <p>You should see subcommands like <code>install</code>, <code>list</code>, <code>rm</code>, and <code>print</code>.</p> <p>Next, check podman info:</p> <pre><code>podman system info\n</code></pre>"},{"location":"labs/2.%20quadlet/#24-kick-the-tires","title":"2.4 Kick the Tires","text":"<p>Run a quick smoke test to ensure Podman works for both root and unprivileged users:</p> <pre><code>podman run --rm quay.io/podman/hello\n</code></pre> <p>And print the version for reference:</p> <pre><code>podman version\n</code></pre> <p>Note the Client, Server, and Quadlet versions for your documentation.</p>"},{"location":"labs/2.%20quadlet/#3-drop-in-the-test-quadlet","title":"3. Drop in the Test Quadlet","text":"<p>In this section we'll create a simple Quadlet file for an unprivileged lingering user. This container will simply run a small web server to verify Quadlet automation.</p>"},{"location":"labs/2.%20quadlet/#31-switch-to-the-user","title":"3.1 Switch to the User","text":"<p>Back to the lingeruser: <code>sudo su - lingeruser</code></p>"},{"location":"labs/2.%20quadlet/#32-create-the-quadlet-file","title":"3.2 Create the Quadlet File","text":"<p>Create the Quadlet definition in the user's configuration directory:</p> <pre><code>mkdir -p ~/.config/containers/systemd/\ncat &gt; ~/.config/containers/systemd/test-quadlet.container &lt;&lt;'EOF'\n[Unit]\nDescription=Quadlet Test Container\n\n[Container]\nImage=nginx:alpine\nExec=nginx -g 'daemon off;'\nContainerName=test-quadlet\nPublishPort=8080:80\n\n[Service]\nRestart=always\nTimeoutStartSec=300\nStartLimitIntervalSec=60s\nStartLimitBurst=5\nRestartSec=10s\n\n[Install]\nWantedBy=default.target\nEOF\n</code></pre>"},{"location":"labs/2.%20quadlet/#33-reload-and-inspect-quadlet","title":"3.3 Reload and Inspect Quadlet","text":"<p>Tell systemd to reload the user unit files:</p> <pre><code>systemctl --user daemon-reload\n</code></pre> <p>Now check what Quadlet sees:</p> <pre><code>podman quadlet list\n</code></pre> <p>Start it:</p> <pre><code>systemctl --user start test-quadlet.service\n</code></pre> <p>And confirm that the generated <code>.service</code> file exists under:</p> <pre><code>cat /run/user/$UID/systemd/generator/test-quadlet.service\n</code></pre> <p>You should see the generated service file contents. Notice there was no service enable operation right? Then check the status:</p> <pre><code>systemctl --user status test-quadlet.service\n</code></pre> <p>You should see the Nginx container running with <code>Active: active (running)</code> status. Verify from Podman\u2019s perspective:</p> <pre><code>podman ps\n</code></pre> <p>Finally, test the container by curling the local port:</p> <pre><code>curl http://localhost:8080\n</code></pre> <p>You should see the Nginx welcome page HTML.</p>"},{"location":"labs/2.%20quadlet/#4-reboot-and-persistence-test","title":"4. Reboot and Persistence Test","text":"<p>Reboot the VM to ensure the Quadlet container auto-starts thanks to lingering:</p> <pre><code>sudo reboot\n</code></pre> <p>After reboot, log back in as the same unprivileged user and check:</p> <pre><code>systemctl --user status test-quadlet.service\n</code></pre> <p>You should see it running automatically.</p> <p>Confirm from Podman:</p> <pre><code>podman ps\n</code></pre> <p>And once more verify with:</p> <pre><code>curl http://localhost:8080\n</code></pre> <p>If all looks good, Quadlet persistence is working correctly under the lingering user.</p>"},{"location":"labs/2.%20quadlet/#5-rollback-optional","title":"5. Rollback (Optional)","text":"<p>If you want to revert to the pre-Quadlet state, roll back using your saved snapshot:</p> <pre><code>vagrant snapshot restore before-quadlet\n</code></pre>"},{"location":"labs/2.%20quadlet/#lessons-learned","title":"Lessons Learned","text":"<ol> <li>Quadlet integrates tightly with systemd, turning container definitions into first-class services.</li> <li>Lingering users are essential for persistent unprivileged containers across reboots.</li> <li>Alvistack\u2019s Podman builds offer up-to-date features missing from Debian's stock packages.</li> <li>Generated <code>.service</code> units under <code>/run/user/&lt;uid&gt;/containers/systemd/</code> are ephemeral but reflect the active container states.</li> <li>Quadlet simplifies container lifecycle management \u2014 no manual <code>podman run</code> required once configured.</li> <li>When combined with lingering, Quadlet gives you reliable, self-starting, user-scoped container services that survive reboots without root privileges.</li> </ol>"},{"location":"labs/3.%20volumes/","title":"Volumes","text":"<p>In this lab you will orchestrate storage using Podman volume quadlets: first wiring bind mounts into container units so nginx can serve the synced MkDocs site, then defining dedicated volume units whose lifecycle is decoupled from their consuming containers. Along the way you'll manage PostgreSQL data on ZFS, practice snapshotting and rollbacks, and see how volume descriptors integrate cleanly with container services and why a separate volume quadlet is beneficial in many scenarios.</p> <ol> <li>Serve Brothaman's docs with <code>Volume=</code> directives in test-quadlet.container's <code>[Container]</code> section</li> <li>Creating a volume quadlet</li> <li>Switching to the zfs-helper for snapshot management</li> <li>Connecting the volume to the PostgreSQL container quadlet</li> <li>Testing data persistence and snapshot rollbacks</li> </ol>"},{"location":"labs/3.%20volumes/#0-snapshot","title":"0. Snapshot","text":"<p>Since this test runs inside a Vagrant VM, capture a baseline snapshot before making any changes. From your host machine (in the same directory as your Vagrantfile):</p> <pre><code>if ! vagrant snapshot list 2&gt;&amp;1 | grep before-volumes &gt;/dev/null; then\n  vagrant snapshot save before-volumes;\nelse\n  echo before-volumes snapshot already exists;\nfi\n</code></pre> <p>This allows you to roll the VM back to its exact pre-test state later.</p>"},{"location":"labs/3.%20volumes/#1-serve-brothamans-docs-with-volume-directives-in-test-quadletcontainers-container-section","title":"1. Serve Brothaman's docs with <code>Volume=</code> directives in test-quadlet.container's <code>[Container]</code> section","text":"<p>In the last lab we created a simple nginx based container quadlet. By default without any configuration it serves a default test page. Let's go one step further and have it serve the brothaman website.</p> <p>If you look into the Vagrantfile an rsync command is used to copy the local <code>docs</code> directory into the VM at <code>/brothaman</code> with your lingeruser's uid:</p> <pre><code>  config.vm.synced_folder \"docs\", \"/brothaman\",\n    type: \"rsync\",\n    create: true,\n    owner: 1001,\n    group: 1001,\n    rsync__chown: true,\n    rsync__auto: false,\n    rsync__args: [\n      \"--verbose\",\n      \"--archive\",\n    ]\n</code></pre> <p>To have nginx serve this content, a virtual server configuration needs to be passed telling it how and from where to serve the content. The following nginx server configuration tells nginx to use /var/www/brothaman as its server root:</p> <pre><code>mkdir ~/nginx-conf.d/\ncat &gt; ~/nginx-conf.d/brothaman.org.conf &lt;&lt;'EOF'\nserver {\n    listen 80;\n    server_name brothaman.org www.brothaman.org;\n\n    # put your site files here (we just made it above)\n    root /var/www/brothaman;\n    index index.html index.htm;\n\n    # log to container stdout/stderr (best for containers)\n    access_log /dev/stdout;\n    error_log  /dev/stderr warn;\n\n    location / {\n        try_files $uri $uri/ =404;\n    }\n}\nEOF\n</code></pre> <p>The container author configured nginx to look for server configurations in <code>/etc/nginx/conf.d/</code>. Our configuration needs to reside there to get picked up at startup. Instead of copying the file into the container image, we can use a bind mount to map our configuration directory onto the container at <code>/etc/nginx/conf.d/</code> the quadlet descriptor, <code>Volume=/home/lingeruser/nginx-conf.d:/etc/nginx/conf.d</code>. Another <code>Volume=</code> mapping, <code>Volume=/brothaman:/var/www/brothaman</code>, also directly within the test-quadlet's <code>[Container]</code> section. Together nginx can then serve this directory as using the brothaman.org virtual server:</p> <pre><code>mkdir -p ~/.config/containers/systemd/\ncat &gt; ~/.config/containers/systemd/test-quadlet.container &lt;&lt;'EOF'\n[Unit]\nDescription=Quadlet Test Container\n\n[Container]\nImage=nginx:alpine\nExec=nginx -g 'daemon off;'\nContainerName=test-quadlet\nPublishPort=8080:80\nVolume=/brothaman:/var/www/brothaman\nVolume=/home/lingeruser/nginx-conf.d:/etc/nginx/conf.d\n\n[Service]\nRestart=always\nTimeoutStartSec=300\nStartLimitIntervalSec=60s\nStartLimitBurst=5\nRestartSec=10s\n\n[Install]\nWantedBy=default.target\nEOF\n</code></pre> <p>Before starting the container notice no separate volume quadlet was needed: meaning we did not create a <code>dot.volume</code> file. Both directives were put into the container quadlet's <code>[Container]</code> section directly. There's only one service, the test-quadlet container service, and it can now serve the mkdocs content. Reload systemd and start the container:</p> <pre><code>systemctl --user daemon-reload\nsystemctl --user restart test-quadlet.service\ncurl \"http://$(ip route | grep 'kernel scope link' | awk '{print $9}'):8080\"\n</code></pre> <p>You should see the bro site contents. You can point your browser to the VM's IP address at port 8080 to see it as well. Don't worry you don't have to set up DNS for brothaman.org, just accessing the IP:8080 will work fine. With one virtual server configuration nginx just uses it as the default site.</p>"},{"location":"labs/3.%20volumes/#2-creating-a-volume-quadlet","title":"2. Creating a volume quadlet","text":"<p>The example above is so simple, no separate volume quadlet was needed. However, for more complex scenarios where you:</p> <ul> <li>share the volume with multiple containers, or</li> <li>you want to manage the lifecycle of the volume separately from the container, or</li> <li>you want to use advanced volume features like ZFS snapshots</li> </ul> <p>It's beneficial to create a dedicated volume quadlet. In this lab section, the concept will be put to the test so we can see how it works and helps.</p>"},{"location":"labs/3.%20volumes/#21-create-a-zfs-backed-volume-quadlet-for-postgresql","title":"2.1 Create a ZFS backed volume quadlet for PostgreSQL","text":"<p>Let's consider a database container like PostgreSQL which needs persistent storage for its data. Before the container starts, we want to take a snapshot of the data. In case anything goes wrong during the container's operation, we can roll back to the previous snapshot. In such cases, a dedicated volume quadlet is the way to go to isolate the functionality.</p> <p>Let's use a ZFS-backed volume to take snapshots of the data volume between container restarts. We start by creating a ZFS dataset for PostgreSQL data storage and set it to mount at <code>/home/lingeruser/postgres</code>. Then we create a volume quadlet to manage this dataset.</p> <p>Make sure you're logged as the <code>vagrant</code> user to escalate privileges to root with <code>sudo</code>. Below a ZFS dataset, <code>testing/postgres</code>, is created then its mountpoint set to <code>/home/lingeruser/postgres</code>: it mounts automatically using the ZFS mount service. The mountpoint's user and group ownership permissions are also set to be owned by lingeruser so lineruser's unprivileged containers can access it.</p> <pre><code>sudo zfs create testing/postgres\nsudo zfs set mountpoint=/home/lingeruser/postgres testing/postgres\nsudo chown lingeruser:lingeruser /home/lingeruser/postgres\nsudo -iu lingeruser ls -ld postgres\ndf -h /home/lingeruser/postgres/\n</code></pre> <p>The dataset is now ready to be used as a persistent volume for our PostgreSQL container. Now switch back to the <code>lingeruser</code> again with <code>sudo su - lingeruser</code>:</p> <pre><code>mkdir -p ~/.config/containers/systemd/\ncat &gt; ~/.config/containers/systemd/postgres-data.volume &lt;&lt;'EOF'\n[Unit]\nDescription=PostgreSQL Volume\n\n[Volume]\nVolumeName=postgres-data\n\n[Service]\nExecStartPre=/usr/sbin/zfs snapshot testing/postgres@pre\nEOF\n\nsystemctl --user daemon-reload\nsystemctl --user start postgres-data-volume.service\n</code></pre> <p>NOTICE: The <code>postgres-data.volume</code> quadlet is started and stopped using a service name that combines the volume name, the <code>-volume</code> suffix, and the <code>.service</code> extension. The service name construction pattern is automatically handled by podman when you create a volume quadlet.</p>"},{"location":"labs/3.%20volumes/#22-troubleshooting-problems","title":"2.2 Troubleshooting Problems","text":"<p>How did it go for you? Using <code>journalctl --user -xe -u postgres-data-volume.service</code>, here are the failure errors in the service journal log:</p> <pre><code>Oct 26 14:13:16 debian12.localdomain systemd[904]: Starting postgres-data-volume.service - PostgreSQL Volume...\nOct 26 14:13:16 debian12.localdomain postgres-data-volume[45001]: cannot create snapshots : permission denied\nOct 26 14:13:16 debian12.localdomain systemd[904]: postgres-data-volume.service: Control process exited, code=exited, status=1/FAILURE\nOct 26 14:13:16 debian12.localdomain systemd[904]: postgres-data-volume.service: Failed with result 'exit-code'.\nOct 26 14:13:16 debian12.localdomain systemd[904]: Failed to start postgres-data-volume.service - PostgreSQL Volume.\n</code></pre> <p>The permission error is expected. The <code>lingeruser</code> does not have permission to create snapshots. Try this as <code>vagrant</code> to escalate to sudo and delegate snapshot permission to <code>lingeruser</code> on the dataset:</p> <pre><code>sudo zfs allow lingeruser snapshot testing/postgres\n</code></pre> <p>Now sudo back to <code>lingeruser</code> and start the volume service again:</p> <pre><code>systemctl --user start postgres-data-volume.service\nsystemctl --user status postgres-data-volume.service\nzfs list -t snapshot testing/postgres\n</code></pre> <p>You should see the volume service started successfully this time and a snapshot was created. Amazing! Now try stopping the volume service and starting it again now with:</p> <pre><code>systemctl --user status postgres-data-volume.service\nsystemctl --user start postgres-data-volume.service\n</code></pre> <p>This fails too because the snapshot with the same name already exists. Feel free to check the error message with <code>journalctl --user -xe -u postgres-data-volume.service</code>. Let's create snapshots with unique names using timestamps prefixed to the base name of the snapshot:</p> <pre><code>mkdir -p ~/.config/containers/systemd/\ncat &gt; ~/.config/containers/systemd/postgres-data.volume &lt;&lt;'EOF'\n[Unit]\nDescription=PostgreSQL Volume\n\n[Volume]\nVolumeName=postgres-data\n\n[Service]\nExecStartPre=/usr/sbin/zfs snapshot testing/postgres@pre_$(date +%Y%m%d%H%M%S)\nEOF\n\n</code></pre> <p>This too fails and the output looks a little cryptic in the logs:</p> <pre><code>Oct 28 08:28:45 debian12.localdomain systemd[904]: Starting postgres-data-volume.service - PostgreSQL Volume...\nOct 28 08:28:45 debian12.localdomain postgres-data-volume[78548]: usage:\nOct 28 08:28:45 debian12.localdomain postgres-data-volume[78548]:         snapshot [-r] [-o property=value] ... &lt;filesystem|volume&gt;@&lt;snap&gt; ...\nOct 28 08:28:45 debian12.localdomain postgres-data-volume[78548]: For the property list, run: zfs set|get\nOct 28 08:28:45 debian12.localdomain postgres-data-volume[78548]: For the delegated permission list, run: zfs allow|unallow\nOct 28 08:28:45 debian12.localdomain systemd[904]: postgres-data-volume.service: Control process exited, code=exited, status=2/INVALIDARGUMENT\nOct 28 08:28:45 debian12.localdomain systemd[904]: postgres-data-volume.service: Failed with result 'exit-code'.\n</code></pre> <p>What's going on here? The clue is in the <code>usage</code> complaint by the zfs CLI program. It's not able to parse the command line. The problem is that the <code>$(date +%Y%m%d%H%M%S)</code> command substitution is not being interpreted by systemd as expected. To fix this, we need to wrap the command substitution in a shell execution context. Update the <code>ExecStartPre</code> line as follows and try again:</p> <pre><code>mkdir -p ~/.config/containers/systemd/\ncat &gt; ~/.config/containers/systemd/postgres-data.volume &lt;&lt;'EOF'\n[Unit]\nDescription=PostgreSQL Volume\n\n[Volume]\nVolumeName=postgres-data\n\n[Service]\nExecStartPre=/bin/sh -c '/usr/sbin/zfs snapshot testing/postgres@pre_\"$(date +%Y%m%d%H%M%S)\"'\nEOF\n\nsystemctl --user daemon-reload\nsystemctl --user start postgres-data-volume.service\n</code></pre> <p>This too fails. Take a look at why in the journal, but there's a bunch of junk generated instead of the date string we wanted to append. That happens because systemd is performing specifier expansion. The '%' and '$' characters have special meanings in systemd unit files. They need to be escaped to be interpreted literally. Update the <code>ExecStartPre</code> line again as follows:</p> <pre><code>mkdir -p ~/.config/containers/systemd/\ncat &gt; ~/.config/containers/systemd/postgres-data.volume &lt;&lt;'EOF'\n[Unit]\nDescription=PostgreSQL Volume\n\n[Volume]\nVolumeName=postgres-data\n\n[Service]\nExecStartPre=/bin/sh -c '/usr/sbin/zfs snapshot testing/postgres@pre_\"$$(date +%%Y%%m%%d%%H%%M%%S)\"'\nEOF\n\nsystemctl --user daemon-reload\nsystemctl --user start postgres-data-volume.service\nzfs list -t snapshot testing/postgres\n</code></pre> <p>Now this works! Each time the volume service is started a new snapshot with a unique timestamped name is created. Great! Keep trying it and test by starting and restarting the service. Here let me help you:</p> <pre><code>for restart in $(seq 1 3); do\n  systemctl --user restart postgres-data-volume.service\n  sleep 2\ndone\nzfs list -t snapshot testing/postgres\n</code></pre>"},{"location":"labs/3.%20volumes/#23-lets-delete-the-extra-snapshots","title":"2.3 Let's delete the extra snapshots","text":"<p>That's looking good. Multiple snapshots were created with unique names. Perfect! But now we got that <code>pre</code> without the prefix let's try deleting that still logged in as the <code>lingeruser</code>:</p> <pre><code>zfs destroy testing/postgres@pre\n</code></pre> <p>In fact we cannot delete any snapshot. You should see they all fail due to a lack of permissions. That's because the <code>lingeruser</code> does not have permission to destroy snapshots. Let's switch back to <code>vagrant</code> and delegate the destroy permission on the dataset to <code>lingeruser</code>:</p> <pre><code>sudo zfs allow lingeruser destroy,snapshot testing/postgres\n</code></pre> <p>Now if you then try the destroy on the snapshot again it still does not work. What's going on here? The reason is ZFS supports the delegation of snapshot creation, but not the destruction of snapshots. Even the dataset owner can not destroy snapshots. But try destroy the pre snapshot with the <code>vagrant</code> user as root:</p> <pre><code>vagrant@debian12:~$ sudo zfs destroy testing/postgres@pre\nvagrant@debian12:~$ sudo zfs list -t snapshot testing/postgres\nNAME                                  USED  AVAIL     REFER  MOUNTPOINT\ntesting/postgres@pre-20251028085229     0B      -     40.0M  -\ntesting/postgres@pre-20251028085533     0B      -     40.0M  -\ntesting/postgres@pre-20251028085535     0B      -     40.0M  -\ntesting/postgres@pre-20251028085537     0B      -     40.0M  -\nvagrant@debian12:~$\n</code></pre>"},{"location":"labs/3.%20volumes/#3-switching-to-the-zfs-helper-for-snapshot-management","title":"3. Switching to the zfs-helper for snapshot management","text":"<p>So to manage snapshot deletion, we need to be perform the destroy operation with elevated privileges with the vagrant sudo enabled user. This is a limitation of OpenZFS's delegation model. The whole situation should be pretty irritating to you. Especially if you want to automate snapshot management in unprivileged containers and have to login and out of one user to sudo from another all the time right?</p> <p>WARNING: What many lazy admins (who you should never hire) often do in this case is make their unprivileged users sudoers without password prompt for zfs commands. Even adding having a sudo password should be forbidden on production systems. Unprivileged users for application sandboxing purposes should NEVER have the ability to escalate privileges with or without passwords. WTF is the point anyway right?</p> <p>Then what if you want out of this login, log out hell, and want some consistency. First off you should be automating the process with DevOps tools, but let me not go there right now. It's still pretty irritating even if you automate things. Plus doing so you still need two different users, one with sudo, and the target non sudoers user contexts to properly manage snapshots. Ugh.</p>"},{"location":"labs/3.%20volumes/#31-setup-lingeruser-to-use-zfs-helper","title":"3.1 Setup lingeruser to use zfs-helper","text":"<p>Although there are several ways to skin this cat, the best is to use a helper service that runs with elevated privileges and performs zfs management tasks on behalf of unprivileged user-scoped services. Such a service needs a solid policy framework to validate and authorize requests from unprivileged users. Its authorization controls should limit what operations can be performed on which datasets by which users. Luckily we did just that for brothaman and it is called <code>zfs-helper</code>. The stock package is already installed in the VM. All we need to do is configure it to allow our user's <code>postgres-data-volume.service</code> to manage snapshots on the <code>testing/postgres</code> dataset. See the edits added to the zfs-helper configuration files at <code>/etc/zfs-helper/policy.d/</code>:</p> <p>Perform these operations as the <code>vagrant</code> user.</p> <pre><code>sudo usermod -a -G zfshelper lingeruser\nsudo loginctl kill-user lingeruser\nsudo loginctl user-status lingeruser --no-pager\nsudo loginctl enable-linger lingeruser\nsudo loginctl user-status lingeruser --no-pager\nsudo -iu lingeruser id\n\npushd . || true\nsudo mkdir -p /etc/zfs-helper/policy.d/lingeruser\ncd /etc/zfs-helper/policy.d/lingeruser\ncat &lt;&lt;'EOF' | sudo tee -a units.list\npostgres-data-volume.service\nEOF\n\ncat &lt;&lt;'EOF' | sudo tee -a mount.list unmount.list snapshot.list rollback.list create.list destroy.list\nlingeruser testing/postgres\nEOF\npopd\n\n</code></pre> <p>Now let's use the service to manage snapshots for us. First, update the volume quadlet to use zfs-helper for snapshot creation instead of calling zfs directly. We could do this before directly since we were granted delegation, but let's do it now again to be consistent:</p> <p>Issue these commands as the <code>lingeruser</code>:</p> <pre><code>mkdir -p ~/.config/containers/systemd/\ncat &gt; ~/.config/containers/systemd/postgres-data.volume &lt;&lt;'EOF'\n[Unit]\nDescription=PostgreSQL Volume\n\n[Volume]\nVolumeName=postgres-data\n\n[Service]\nExecStartPre=/bin/sh -c '/usr/bin/zfs-helperctl snapshot testing/postgres@pre_\"$$(date +%%Y%%m%%d%%H%%M%%S)\"'\nEOF\n\nsystemctl --user daemon-reload\nsystemctl --user restart postgres-data-volume.service\nzfs list -t snapshot testing/postgres\n</code></pre>"},{"location":"labs/3.%20volumes/#32-delete-pre-start-snapshots-above-a-limit","title":"3.2 Delete pre-start snapshots above a limit","text":"<p>Now let's enhance the volume quadlet further to delete old snapshots above a certain limit each time the volume service starts. This way we can keep the number of snapshots manageable without manual intervention. Update the volume quadlet as follows to add a snapshot cleanup step before creating a new snapshot:</p> <pre><code>mkdir -p ~/.config/containers/systemd/\ncat &gt; ~/.config/containers/systemd/postgres-data.volume &lt;&lt;'EOF'\n[Unit]\nDescription=PostgreSQL Volume\nPartOf=test-postgresql.service\n\n[Volume]\nVolumeName=postgres-data\n\n[Service]\nEnvironment=KEEP_SNAPSHOTS=5\nExecStartPre=/bin/sh -c '/usr/bin/zfs-helperctl snapshot testing/postgres@pre_\"$$(date +%%Y%%m%%d%%H%%M%%S)\"'\nExecStartPre=/bin/sh -c 'zfs list -t snapshot -o name -s creation | grep -E \"^testing/postgres@pre_*\" | head -n -$${KEEP_SNAPSHOTS} | xargs -r -n1 zfs-helperctl destroy'\nEOF\nsystemctl --user daemon-reload\nsystemctl --user restart postgres-data-volume.service\nzfs list -t snapshot testing/postgres\n</code></pre> <p>Now each time the volume service starts, it will delete old pre-start snapshots beyond the specified limit after creating a new snapshot. This keeps the snapshot count manageable automatically. No fancy retention policies manager services or cron jobs needed. Just let the volume service handle it for you. You can verify this by restarting the volume service multiple times and checking the snapshot list:</p> <pre><code>for restart in $(seq 1 10); do\n  echo \"snapshot with $restart\"\n  systemctl --user restart postgres-data-volume.service\n  sleep 3\n  zfs list -t snapshot testing/postgres\ndone\n</code></pre>"},{"location":"labs/3.%20volumes/#4-connecting-the-volume-to-the-postgresql-container-quadlet","title":"4. Connecting the volume to the PostgreSQL container quadlet","text":"<p>Finally, let's connect the volume quadlet to the PostgreSQL container quadlet so the container can use the persistent storage with snapshotting and test it out. Update the PostgreSQL container quadlet to include the volume:</p> <pre><code>mkdir -p ~/.config/containers/systemd/\ncat &gt; ~/.config/containers/systemd/test-postgresql.container &lt;&lt;'EOF'\n[Unit]\nDescription=PostgreSQL Container\nPropagatesStopTo=postgres-data-volume.service\nBindsTo=postgres-data-volume.service\nAfter=postgres-data-volume.service\n\n[Container]\nImage=postgres:16\nContainerName=test-postgresql\nEnvironment=POSTGRES_PASSWORD=password\nVolume=/home/lingeruser/postgres:/var/lib/postgresql/data\nPublishPort=5432:5432\n\n[Service]\nRestart=always\nTimeoutStartSec=300\nStartLimitBurst=5\nRestartSec=10s\n\n[Install]\nWantedBy=default.target\nEOF\n\nsystemctl --user daemon-reload\nsystemctl --user start test-postgresql.service\n</code></pre>"},{"location":"labs/3.%20volumes/#notes-on-propagatesstopto-bindsto-partof-and-after","title":"Notes on PropagatesStopTo=, BindsTo=, PartOf=, and After=","text":"<p>These directives are used to manage the relationships between systemd services:</p> <ul> <li><code>PartOf=</code>: This directive indicates that the service is part of another service. If the parent service is stopped or restarted, the child service will also be affected. We made the volume quadlet part of the container quadlet so that stopping the container also stops the volume service. If it was shared we would not opt to do this.</li> <li><code>BindsTo=</code>: This directive creates a stronger link between services. If the service specified in <code>BindsTo=</code> is stopped, the current service will also be stopped. Make no sense to run the database without its volume right? Before the volume stops the database is shutdown allowing it to sync its buffers to prevent corruption.</li> <li><code>After=</code>: This directive specifies that the current service should be started only after the specified service has been started. Doh! The database needs its volume to be ready before it starts.</li> <li><code>PropagatesStopTo=</code>: This directive allows a service to propagate stop signals to its dependencies. When the container is stopped, it will also stop the volume service. This way stopping the container actually stops the volume service too. Without this directive a systemctl restart of the container service will not stop and start the volume service.</li> </ul> <p>Systemd is extremely powerful and an amazing init system. In fact it goes well beyond an init system. It is an entire service management platform. Learning to use its features effectively can greatly enhance your ability to manage services and their dependencies. Unfortunately, we snuck these in, with only slight mentions as describe here to limit the scope of this lab.</p> <p>With Quadlets, Podman has taken a revolutionary approach to container management by integrating deeply with systemd. This integration leverages systemd's capabilities to manage the container lifecycle and their dependencies effectively. It brings the power of systemd to container management, allowing for more robust and reliable containerized applications closer to the system. Its use of core system primitives instead of reinventing the wheel is a form of convergence. It's a game-changer for how we think about and manage containers in a Linux environment.</p>"},{"location":"labs/3.%20volumes/#5-testing-data-persistence-and-snapshot-rollbacks","title":"5. Testing data persistence and snapshot rollbacks","text":"<p>IMPORTANT: For this section you will need to keep two shells open, one logged into the VM as the <code>vagrant</code> user and one logged in as the <code>lingeruser</code>. This is because rolling back snapshots requires elevated privileges that only the <code>vagrant</code> user has via sudo. The <code>lingeruser</code> will be used to interact with the PostgreSQL container and perform database operations.</p> <p>You can now connect to the PostgreSQL container and verify that the data is being stored in the ZFS-backed volume in the <code>lingeruser</code> shell:</p> <pre><code>export PGPASSWORD=password\npsql -h \"$(ip route | grep 'kernel scope link' | awk '{print $9}')\" -p 5432 -U postgres -c '\\l'\n</code></pre> <p>You should see the default PostgreSQL databases listed. Let's create a new database, insert some data, and then restart the container to see that the data persists across restarts thanks to the ZFS-backed volume in the <code>lingeruser</code> shell:</p> <pre><code># Capture the latest snapshot before creating the test database\nbefore_testdb_snapshot=$(zfs list -t snapshot testing/postgres | grep -v NAME | awk '{print $1}' | tail -n 1)\necho \"Latest snapshot before the testdb: $before_testdb_snapshot\"\n\n# Let's create the database, the test table, and insert the test data\nexport PGPASSWORD=password\npsql -h \"$(ip route | grep 'kernel scope link' | awk '{print $9}')\" -p 5432 -U postgres -c 'CREATE DATABASE testdb;'\npsql -h \"$(ip route | grep 'kernel scope link' | awk '{print $9}')\" -p 5432 -U postgres -d testdb -c 'CREATE TABLE testtable (id SERIAL PRIMARY KEY, name VARCHAR(50));'\npsql -h \"$(ip route | grep 'kernel scope link' | awk '{print $9}')\" -p 5432 -U postgres -d testdb -c \"INSERT INTO testtable (name) VALUES ('testname');\"\npsql -h \"$(ip route | grep 'kernel scope link' | awk '{print $9}')\" -p 5432 -U postgres -d testdb -c 'SELECT * FROM testtable;'\n\n# Bounce the server to create another snapshot with the new db, table, and one testname record\nsystemctl --user restart test-postgresql.service\ntestname_snapshot=$(zfs list -t snapshot testing/postgres | grep -v NAME | awk '{print $1}' | tail -n 1)\necho \"Latest snapshot with only testable: $testname_snapshot\"\nsleep 5\n\n# Verify the data is still there and add another name, anthony\npsql -h \"$(ip route | grep 'kernel scope link' | awk '{print $9}')\" -p 5432 -U postgres -d testdb -c 'SELECT * FROM testtable;' \npsql -h \"$(ip route | grep 'kernel scope link' | awk '{print $9}')\" -p 5432 -U postgres -d testdb -c \"INSERT INTO testtable (name) VALUES ('anthony');\"\npsql -h \"$(ip route | grep 'kernel scope link' | awk '{print $9}')\" -p 5432 -U postgres -d testdb -c 'SELECT * FROM testtable;'\n\n# Bounce the server to create another snapshot with the new data\nsystemctl --user restart test-postgresql.service\nanthony_snapshot=$(zfs list -t snapshot testing/postgres | grep -v NAME | awk '{print $1}' | tail -n 1)\necho \"Latest snapshot with anthony: $anthony_snapshot\"\nsleep 5\n\npsql -h \"$(ip route | grep 'kernel scope link' | awk '{print $9}')\" -p 5432 -U postgres -d testdb -c 'SELECT * FROM testtable;'\n# Delete the testname entry\npsql -h \"$(ip route | grep 'kernel scope link' | awk '{print $9}')\" -p 5432 -U postgres -d testdb -c \"DELETE FROM testtable WHERE name = 'testname';\"\n# Verify the deletion\npsql -h \"$(ip route | grep 'kernel scope link' | awk '{print $9}')\" -p 5432 -U postgres -d testdb -c 'SELECT * FROM testtable;'\n\n# Shutdown the server to rollback\nsystemctl --user stop test-postgresql.service\nno_testname_snapshot=$(zfs list -t snapshot testing/postgres | grep -v NAME | awk '{print $1}' | tail -n 1)\necho \"Last snapshot without testname: $no_testname_snapshot\"\n</code></pre> <p>I know that's a long block of code but it creates a database, a table, then inserts two rows, and deletes one row. It creates snapshots at each step. You can see how the data changes over time with each snapshot. The code block also saves each snapshot name into variables for later use to experiment with rollbacks. Do not close the lingeruser shell yet so we can use those values to rollback.</p> <p>Let's try reverting to a previous snapshot to see the table revert to its earlier state. However, OpenZFS's delegation model has another shortcoming that bites us yet again: rollback delegation is not implemented. So to rollback we have to escalate privileges with the <code>vagrant</code> user via sudo. We already shut the server down at the end of the code block so we can rollback without corrupting the database.</p> <p>WARNING: Never roll back a dataset while it's mounted and in use. Always stop the container first to unmount the volume cleanly. This is especially important with databases that can be corrupted when the underlying disk changes leaving them in inconsistent states.</p> <p>Let's go back to the <code>vagrant</code> user's shell to elevate privileges for these rollback operations:</p> <pre><code>sudo zfs rollback -r \"[copy the anthony_snapshot variable's value from the lingeruser shell here]\"\n</code></pre> <p>Switching user shells we cannot just echo the $anthony_snapshot variable since it's not defined in the vagrant user's context. Just re-echo the value and copy it from within the lingeruser's shell. Now let's restart the PostgreSQL container and verify the data in the table:</p> <pre><code>systemctl --user start test-postgresql.service\nexport PGPASSWORD=password\npsql -h \"$(ip route | grep 'kernel scope link' | awk '{print $9}')\" -p 5432 -U postgres -d testdb -c 'SELECT * FROM testtable;'\n</code></pre> <p>Now you should see the previously deleted 'testname', and the 'anthony' entry present again since we rolled back to the snapshot taken before 'testname' was deleted towards the end of the code block. It was as if the delete of the <code>testname</code> entry never happened. Amazing!</p> <p>Now try rolling back successively further in time to snapshots taken before 'anthony' was added on your own. This will solidify your understanding of whats going on here. Just remember, we captured the past snapshot values in the <code>testname_snapshot</code> and <code>before_testdb_snapshot</code> variables in the <code>lingeruser</code> shell. Don't forget to shutdown the database in the vagrant user shell before each rollback.</p>"},{"location":"labs/3.%20volumes/#why-didnt-you-use-zfs-helper-for-rollbacks","title":"Why didn't you use zfs-helper for rollbacks?","text":"<p>The <code>zfs-helper</code> is explicitly designed to avoid delegated calls from the command line or elsewhere. It ONLY services requests from systemd unit services and ties authorization to systemd unit identity and kernel authenticated unix socket clients. These facilities work together to provide a secure and auditable mechanism for delegated management of ZFS datasets.</p>"},{"location":"labs/3.%20volumes/#how-does-it-work","title":"How does it work?","text":"<p>Systemd services have well-defined identities and can be authenticated and authorized based on their unit names with secure cgroup checks. These checks connects <code>zfs-helper</code> policies with systemd units to enforce strict access controls for zfs management operations on a per dataset (noun), per user (subject), and per operation (verb) basis.</p> <p>During each <code>zfs-helperctl</code> call it reads <code>/proc/self/cgroup</code>, maps that back to the calling unit name, and refuses requests unless that unit appears in the unit allow-list (the <code>units.list</code> you maintain under <code>/etc/zfs-helper/policy.d/</code>). That cgroup check is why you can list allowed datasets per operation and know the request actually came from the expected quadlet rather than an arbitrary process the user might spawn.</p> <p>A few tightly stitched mechanisms work together to provide additional security. Unix sockets, <code>AF_UNIX</code>, and its <code>SO_PEERCRED</code> options allow the <code>zfs-helper</code> to retrieve the UID/GID/PID credentials of the calling service. The kernel verifies the identity of the socket client. The server socket is created by systemd when the <code>zfs-helper</code> service starts and is under its control. Only users in the <code>zfshelper</code> system group can access this socket. Group access further ensures that ONLY authorized users can communicate with the <code>zfs-helper</code> service.</p> <p>In summary, when a service connects to this socket to make a request, the kernel provides the credentials of the calling process through the socket. An additional systemd user cgroup check ensures that the calling service is running within the correct user and systemd context. The allow lists control what operations can be performed on which datasets by which services. This multi-layered approach ensures that only authorized services can perform specific ZFS operations on designated datasets, providing a robust security model for delegated ZFS management.</p>"},{"location":"labs/3.%20volumes/#cant-a-compromised-unprivileged-account-create-a-service-with-the-same-name","title":"Can't a compromised unprivileged account create a service with the same name?","text":"<p>If an attacker only gets shell access as a user who happens to be in the <code>zfshelper</code> group, the cgroup gate keeps them from just running <code>zfs-helperctl</code> by hand\u2014their shell sits in a session cgroup, not the whitelisted service cgroup, so the helper denies the request. This blocks one of the easiest privilege-abuse paths and gives you auditability (\u201cwe know postgres-data-volume.service asked for that snapshot\u201d). Another reason why a separate volume quadlet as a separate service is a good security practice.</p> <p>A compromised account can still try to install a brand new user service, but it won't match anything in <code>units.list</code>, so the helper still rejects those calls. To succeed they'd have to hijack an already-authorized unit name; at that point they can trigger whatever operations you've granted to that unit (destroy snapshots, etc.), but only on the datasets/verbs you explicitly allowed. They cannot grant themselves access to other datasets or new verbs without compromising the root-maintained policy files.</p> <p>Bottom line: membership in the <code>zfshelper</code> group is powerful and should be restricted, yet the cgroup check sharply limits post-compromise blast radius to the exact dataset+operation pairs you've whitelisted. The helper doesn't stop a user from abusing the privileges you intentionally delegated, so continue to scope those policy entries tightly and monitor the corresponding services' behavior.</p>"},{"location":"labs/3.%20volumes/#6-rollback-optional","title":"6. Rollback (Optional)","text":"<p>When you're done testing, you can revert the VM to the snapshot created before starting this lab in step #0. This operation occurs on the host outside of the VM (not to be confused with ZFS snapshots):</p> <pre><code>vagrant snapshot restore before-volumes\n</code></pre>"},{"location":"labs/3.%20volumes/#lessons-learned","title":"Lessons Learned","text":"<p>We saw how adding a <code>Volume=</code> directive into container quadlets perform a bind mount of an external host directory into the container as a <code>source:destination</code> mapping. Containers like the nginx image implement a common pattern where they \"import\" configurations managed externally in host directories using bind mounts. BTW this was a consequence of how containers were originally used in development. The hosted site content to be served also was shared using such a bind mount.</p> <p>We then moved on to create an actual volume quadlet to manage a persistent ZFS-backed volume for a PostgreSQL container. We saw how ZFS snapshots can be created before starting the container to allow rollbacks in case of corruption. However, we also encountered the limitations of OpenZFS's delegation model that complicate snapshot management for unprivileged users. To solve this, we introduced the <code>zfs-helper</code> service that runs with elevated privileges and performs snapshot management tasks on behalf of unprivileged user-scoped container services based on a defined access policy with authentication.</p> <p>With <code>zfs-helper</code> in place, we updated the volume quadlet to use it for secure yet convenient snapshot creation and deletion. It removed a lot of the pain we experienced while manually managing snapshots. We also implemented automatic snapshot cleanup (retention policy) in the simplest way to maintain a manageable number of snapshots. Finally, we connected the volume quadlet to the PostgreSQL container quadlet and tested data persistence and snapshot rollbacks, demonstrating the effectiveness and power of ZFS snapshots and rollbacks for data protection and instant recovery.</p> <p>Hopefully by now you've started to understand why a separate volume quadlet is a good thing. It allows you to manage the lifecycle of persistent storage independently from the container, share volumes between multiple containers, and leverage advanced features like ZFS snapshots for data protection and recovery. It is also a good security practice. Although we could also do this inside the container quadlet directly, separating concerns into dedicated quadlets leads to cleaner, more secure, and more maintainable configurations.</p>"},{"location":"labs/4.%20networking/","title":"Networking","text":"<p>This lab focuses on setting up and managing container networking using Podman quadlets and systemd socket activation. You will learn how to create custom networks, connect containers, and expose services securely.</p> <ol> <li>Retrofit the test-postgresql quadlet with systemd socket activation</li> <li>Create an unprivileged pgadmin service and demonstrate cross-over dependency triggering</li> <li>Export network quadlets to connect multiple containers and compare connectivity methods</li> </ol>"},{"location":"labs/4.%20networking/#0-snapshot","title":"0. Snapshot","text":"<p>Before starting this lab, create a VM snapshot to allow easy rollback in case of issues.</p> <pre><code>vagrant snapshot save networking-start\n</code></pre>"},{"location":"labs/4.%20networking/#1-retrofit-postgresql-with-socket-activation","title":"1. Retrofit PostgreSQL with Socket Activation","text":"<p>Begin by modifying the existing <code>test-postgresql</code> quadlet to use systemd socket activation. This involves creating a <code>.socket</code> unit that listens on the desired port and a corresponding <code>.service</code> unit that starts the container when a connection is made.</p>"},{"location":"labs/4.%20networking/#background","title":"Background","text":"<p>Systemd socket activation allows services to start on-demand, reducing resource usage when the service is idle. A socket unit listens for incoming connections and activates the service unit when a connection is received. The mechanism is particularly useful for containers, as it allows them to remain stopped until needed.</p> <p>It works by having the socket unit listen on a specified port (e.g., 5432 for PostgreSQL). When a client connects, systemd starts the associated service unit, which in turn starts the container. The container can then handle the incoming connection.</p> <p>Handing off the listening socket to the container is achieved using <code>systemd-socket-proxyd</code>, which forwards connections from the host to the container's internal network namespace.</p>"},{"location":"labs/4.%20networking/#11-create-the-postgresql-activator-service-unit","title":"1.1 Create the PostgreSQL Activator Service Unit","text":"<p>In a <code>lingeruser</code> shell create a new file named <code>postgresql-activator.service</code> in the systemd user directory with the following content:</p> <pre><code>mkdir -p ~/.config/systemd/user/\ncat &lt;&lt;EOF &gt; ~/.config/systemd/user/postgresql-activator.service\n[Unit]\nDescription=Proxy host:5432 -&gt; 127.0.0.1:5432 inside After=default.target\nRequires=postgresql-activator.socket\nRequires=test-postgresql.service\nAfter=postgresql-activator.socket test-postgresql.service\n\n[Service]\nType=simple\n\n# Create the environment file for the proxy helper with the target container PID\nExecStartPre=/usr/bin/env sh -c \"/usr/bin/podman inspect -f 'TARGET_PID={{.State.Pid}}' test-postgresql &gt; %t/postgresql-proxyd.env\"\n\n# Environment variable file for the proxy helper does not have to exist with the dash\nEnvironmentFile=-%t/postgresql-proxyd.env\n\n# Demonstrates proper in namespace execution of bro-helper by gathering network info\nExecStartPre=/usr/bin/env sh -c \"/usr/local/bin/bro-helper --pid \\${TARGET_PID} -- ip -o addr\"\nExecStartPre=/usr/bin/env sh -c \"/usr/local/bin/bro-helper --pid \\${TARGET_PID} -- netstat -tlpn\"\n\n# Wait until Postgres is accepting connections inside its netns\nExecStartPre=/usr/bin/env sh -c \"i=0; while [ \\$i -lt 20 ]; do \\\n  /usr/bin/podman unshare nsenter -t \\\"\\$TARGET_PID\\\" -n \\\n    pg_isready -h 127.0.0.1 -p 5432 &amp;&amp; exit 0; \\\n  i=\\$((i+1)); sleep 1; \\\ndone; exit 1\"\n\n# Key is the use of exec to replace the shell with the proxyd process\n# Hands off the listening socket to systemd-socket-proxyd as desired\nExecStart=/usr/bin/env sh -c 'exec /usr/local/bin/bro-helper --pid \"\\${TARGET_PID}\" -- /lib/systemd/systemd-socket-proxyd 127.0.0.1:5432'\n\nNoNewPrivileges=no\nPrivateTmp=yes\nProtectHome=yes\nProtectSystem=full\n\nRestart=on-failure\nRestartSec=5s\n\n[Install]\nWantedBy=default.target\nEOF\n\nsystemctl --user daemon-reload\nsystemctl --user enable postgresql-activator.service\n</code></pre> <p>This service unit uses <code>systemd-socket-proxyd</code> to forward connections from the host socket to the PostgreSQL container's internal port.</p>"},{"location":"labs/4.%20networking/#12-create-the-socket-unit","title":"1.2 Create the Socket Unit","text":"<p>In a <code>lingeruser</code> shell create a new file named <code>postgresql-activator.socket</code> in the systemd user directory with the following content:</p> <pre><code>systemctl --user stop test-postgresql.service\nmkdir -p ~/.config/systemd/user/\ncat &lt;&lt;EOF &gt; ~/.config/systemd/user/postgresql-activator.socket\n[Unit]\nDescription=PostgreSQL Activator Socket\n\n[Socket]\nListenStream=5432\nNoDelay=true\nReusePort=true\nBacklog=128\n\n[Install]\nWantedBy=default.target\nEOF\n\nsystemctl --user daemon-reload\nsystemctl --user enable --now postgresql-activator.socket\n</code></pre> <p>The socket is now set up to listen on port 5432. When a connection is made, it will trigger the associated service unit with the same name. That services unit will need to be created next.</p> <p>The socket unit configuration includes:</p> <ul> <li><code>ListenStream=5432</code>: Listens on TCP port 5432 for PostgreSQL</li> <li><code>NoDelay=true</code>: Disables Nagle's algorithm for lower latency</li> <li><code>ReusePort=true</code>: Allows multiple sockets to bind to the same port</li> <li><code>Backlog=128</code>: Sets the maximum number of pending connections</li> </ul>"},{"location":"labs/4.%20networking/#13-update-the-test-postgresql-quadlet","title":"1.3 Update the test-postgresql quadlet","text":"<p>All that we've done below is remove the <code>PublishPort=</code> directive from the container quadlet, since the socket activation will handle incoming connections now. Also note we're not starting the service as usual after updating the quadlet and reloading. It will be started by the activator socket and its activator service. Update the <code>test-postgresql.container</code> quadlet as follows: </p> <pre><code>systemctl --user stop test-postgresql.service\nmkdir -p ~/.config/containers/systemd/\ncat &gt; ~/.config/containers/systemd/test-postgresql.container &lt;&lt;'EOF'\n[Unit]\nDescription=PostgreSQL Container\nPropagatesStopTo=postgres-data-volume.service\nBindsTo=postgres-data-volume.service\nAfter=postgres-data-volume.service\n\n[Container]\nImage=postgres:16\nContainerName=test-postgresql\nEnvironment=POSTGRES_PASSWORD=password\nVolume=/home/lingeruser/postgres:/var/lib/postgresql/data\nUserNS=keep-id\n\n[Service]\nRestart=always\nTimeoutStartSec=300\nStartLimitBurst=5\nRestartSec=10s\n\n[Install]\nWantedBy=default.target\nEOF\n\nsystemctl --user daemon-reload\nsystemctl --user start test-postgresql.service\nsystemctl --user stop test-postgresql.service\n\n</code></pre>"},{"location":"labs/4.%20networking/#junk","title":"Junk","text":"<p>Follow the lab steps sequentially: define the network quadlet, connect your containers, configure the socket/service pair that exports <code>TARGET_PID</code> and <code>TARGET_IP</code> for <code>bro-helper</code>, and finish by validating host and inter-container connectivity. <code>bro-helper</code> is the glue between socket activation and rootless Podman networking. In this lab the host-facing <code>.socket</code> unit accepts connections, then a companion <code>.service</code> uses <code>podman inspect -f '{{.State.Pid}}'</code> to grab the target container PID and (for named networks) the container IP. With those values exported (for example via <code>%t/web.env</code>), <code>ExecStart=/usr/local/bin/bro-helper --pid ${TARGET_PID} -- /lib/systemd/systemd-socket-proxyd ${TARGET_IP:-127.0.0.1}:8080</code> runs the proxy inside the container\u2019s network namespace. The proxy now sees whatever the container sees: loopback listeners for <code>--network=none</code>, pasta, or the address assigned on the custom Podman network you create below. The end result is single-hop traffic\u2014host socket \u2192 <code>systemd-socket-proxyd</code> inside the namespace \u2192 container service\u2014without any <code>PublishPort=</code> mapping, while keeping the container\u2019s network surface minimal.</p>"},{"location":"labs/5.%20debugging/","title":"Debugging","text":"<p>Buggy quadlet for debugging issues.</p>"},{"location":"labs/proxyd-testing/","title":"proxyd testing","text":"<p>Awesome, everything is working now with the simple straight forward service.</p> <p>NOTE: Need to stop the service when container stops?</p> <pre><code>lingeruser@debian12:~$ cat .config/systemd/user/nginx8080.service \n[Unit]\nDescription=Proxy host:8080 -&gt; 127.0.0.1:80 inside nginx-proxyd netns\nAfter=default.target\nRequires=nginx8080.socket\n\n[Service]\nType=simple\nEnvironmentFile=-%t/nginx-proxyd.env\nExecStartPre=/usr/bin/env sh -c \"/usr/bin/podman inspect -f 'TARGET_PID={{.State.Pid}}' nginx-proxyd &gt; %t/nginx-proxyd.env\"\n\nExecStartPre=/usr/local/bin/fd-setns-exec --pid ${TARGET_PID} -- ip -o addr\nExecStartPre=/usr/local/bin/fd-setns-exec --pid ${TARGET_PID} -- netstat -tlpn \nExecStart=/usr/local/bin/fd-setns-exec --pid ${TARGET_PID} -- /lib/systemd/systemd-socket-proxyd 127.0.0.1:80\n\nNoNewPrivileges=no\nPrivateTmp=yes\nProtectHome=yes\nProtectSystem=full\n\nRestart=on-failure\nRestartSec=0.5s\n\n[Install]\nWantedBy=default.target\n\n</code></pre> <pre><code>lingeruser@debian12:~$ cat .config/systemd/user/nginx8080.socket \n[Unit]\nDescription=Socket for nginx in container netns (host:8080)\n\n[Socket]\n# Listen on all interfaces; adjust as needed (0.0.0.0 or 127.0.0.1)\nListenStream=0.0.0.0:8080\nNoDelay=true\nReusePort=true\nBacklog=128\n\n[Install]\nWantedBy=default.target\n</code></pre>"}]}