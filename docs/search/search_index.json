{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Brothaman Overview","text":"<ul> <li>[ ] <code>bro-volume</code> \u2014 create/remove/manage rootless Podman volumes (ZFS-optional).</li> <li>[ ] <code>bro-network</code> \u2014 create/remove/manage rootless Podman networks (CNI + systemd-networkd).</li> <li>[v] <code>bro-user</code> \u2014 create/remove rootless users (lingering, subuid/gid, per-user storage).</li> <li>[ ] <code>bro-service</code> \u2014 create socket-activated services (Quadlet + systemd-socket-proxyd).</li> <li>[ ] <code>bro-compose</code> \u2014 convert docker-compose.yml into <code>bro-service</code> calls (no <code>-p</code> publishes).</li> <li>[ ] <code>bro-doctor</code> \u2014 optional diagnostics.</li> <li>[ ] <code>bro-install-podman</code> \u2014 installs or upgrades podman to latest from Alvistack.</li> </ul> <p>Principles: small scripts, idempotency, rootless-first, ZFS-optional, socket-activated composition, cross-user/host by on demand using sockets instead of orchestrating services.</p>"},{"location":"architecture/","title":"Architecture Overview","text":"<p>Administration best practices + systemd primitives + podman features</p> <ul> <li>secure container services and applications</li> <li>one minimal authoritative configuration</li> <li>fine control over noisy neighbors, renegades, and compromised containers</li> <li>elegantly managed cross-over container dependencies</li> <li>higher performance and more capabilities</li> </ul> <p>Brothaman uses a combination of system administration best practices with systemd primitives and Podman features to elegantly simplify, secure, and manage containerized services and compositions of dependent containers via docker-compose descriptor files.</p> <ul> <li>unprivileged podman</li> <li> <p>quadlet-patterns</p> </li> <li> <p>Quadlet based authoritative service configuration in systemd --user scopes</p> </li> <li>ZFS using fuse-overlayfs</li> <li>Systemd proxyd</li> <li>Systemd socket</li> <li>Fast networking with Pasta</li> </ul> <p>The best aspect differentiating Podman is its relentless pursuit to work harmoniously with systemd at the operating system level. Podman users easily hook containers into systemd as services using its <code>generate systemd</code> sub-command. Now there are even more powerful Quadlets.</p> <p>The plethora of systemd features makes it hard to notice glorious ways to make amazing things happen.</p> <p>Thankfully, systemd harmony is even better after this command was deprecated in favor of Quadlet. Unfortunately, many are reluctant to upgrade.</p> <p>Brothaman builds on systemd and Quadlet to run rootless containers that are activated by traffic.</p> <ol> <li>A <code>.socket</code> unit listens on a host port (or specific interfaces).</li> <li>When a client connects, <code>systemd-socket-proxyd</code> (<code>-Service</code>) forwards to a loopback internal port.</li> <li>The proxied connection activates the container via a Quadlet <code>.container</code> unit.</li> <li>No <code>-p</code> port mapping is used; the proxy handles external exposure.</li> </ol> <p>This pattern composes across users and hosts: a client of a service uses it and activation follows demand.</p>"},{"location":"bro-helper/","title":"Brothaman's Proxyd Helper","text":"<p>The <code>bro-helper</code> enables <code>systemd-socket-proxyd</code> instances to be exec'd inside the network namespace of containers. By doing so it allows it to proxy traffic from a systemd.socket's file descriptor to container ports that network services bind to on container interfaces.</p> <ol> <li>Allows for <code>--network=none</code> for toighter security</li> <li>Proxied network services can bind to loopback interfaces</li> <li>Single hop proxy (one less without podman's -p mappings)</li> <li>Enables traffic triggering and on-demand container start</li> <li>Primitive used to enable cross over dependencies between unprivileged containers</li> </ol>"},{"location":"bro-helper/#so-what-why-do-we-need-this","title":"So what? Why do we need this?","text":"<p>Unprivileged containers using <code>slirp4netns</code> or <code>pasta</code> (or <code>--network=none</code> for that matter) have no IP addresses exposed to the host, and hence no ports to proxy traffic to the container with <code>systemd-socket-proxyd</code>. So why do we even bother using <code>systemd-socket-proxyd</code> when the -p switch could be used to proxy traffic from the host to the container: i.e. <code>-p 8080:80</code>?</p> <p>ROOT CAUSE: Podman's proxy mechanism does not work with systemd.socket traffic triggering nor does it support server socket (listener) file descriptor passing while <code>systemd-socket-proxyd</code> does.</p>"},{"location":"bro-helper/#using-both-together-sucks","title":"Using both together sucks","text":"<p>You could make them both work together. Presume an NGINX container with the daemon running on its default port 80. Unprivileged containers cannot map container ports to host ports below 1024 so we make podman proxy host port 8080 to container port 80.</p> <p>Yet we still want to trigger containers to start when traffic appears using systemd sockets. Say we use a port for it at 8081. Then the <code>systemd-socket-proxyd</code> daemon proxies traffic to and from port 8080 to 8081.</p> <p>Using them together now doubles the proxied traffic overhead which adds latency and you have an extra bogus port sitting out there. With <code>bro-helper</code> one host port and one container port is needed. Most importantly, there's no additional stream copy and transfer across ports for absolutely no reason.</p>"},{"location":"bro-helper/#security-restrictions","title":"Security Restrictions","text":"<p>The <code>bro-helper</code> is a tiny c-program with slightly augmented capabilities allowing it to set the netns and spawns commands within it. It can exec <code>systemd-socket-proxyd</code>, <code>ip</code>, and <code>netstat</code> commands only. Furthermore, it only works on containers running as the invoking user.</p> <p>Least privilege is achieved by using a very narrow capability addition while using that capability in highly specific situations. Meanwhile we do not fork and honor the passed host file descriptors (listener socket) so are handed off to <code>systemd-socket-proxyd</code> running in the container.</p> <ul> <li>Find the netns you want (e.g., /proc/${PID}/ns/net),</li> <li>Join it with setns(fd, CLONE_NEWNET),</li> <li>Drop any capabilities it no longer needs,</li> <li>Exec the target process.</li> </ul> <p>That's it\u2014no daemons, no forking trees, just a short, auditable path.</p> <p>Two layers work together to maintain least privilege:</p>"},{"location":"bro-helper/#1-the-unit-file-constrains-what-the-process-can-ever-have","title":"1. The unit file constrains what the process can ever have","text":"<ul> <li><code>CapabilityBoundingSet=CAP_SYS_ADMIN CAP_NET_ADMIN</code>     \u2192 Even if the binary tried, it cannot gain caps outside this set.</li> <li><code>AmbientCapabilities=CAP_SYS_ADMIN CAP_NET_ADMIN</code> (or file caps on the binary)     \u2192 Ensures the helper starts with just the tiny set it needs.</li> <li><code>NoNewPrivileges=no</code>     \u2192 Required if you rely on ambient caps or file caps across <code>execve()</code>.</li> </ul>"},{"location":"bro-helper/#2-the-program-drops-capabilities-as-soon-as-its-done-with-setns","title":"2. The program drops capabilities as soon as it\u2019s done with <code>setns()</code>","text":"<p>Right after <code>setns()</code>, it clears its capability sets so the final <code>exec()</code>ed program runs unprivileged (or with only what you explicitly keep). Concretely:</p> <ul> <li>Use libcap to set effective+permitted+inheritable = empty before <code>execvp()</code>.</li> <li>Optionally lock things down further by dropping from the bounding set via <code>prctl(PR_CAPBSET_DROP, ...)</code> (irreversible for the lifetime of the process).</li> <li>Optionally set <code>PR_SET_NO_NEW_PRIVS</code> to 1 after dropping caps.</li> </ul> <p>Result: the helper temporarily uses <code>CAP_SYS_ADMIN</code> to perform one privileged kernel call, then throws away the keys.</p>"},{"location":"bro-helper/#what-it-does-conceptually","title":"What it does (conceptually)","text":"<ul> <li>Inputs: <code>--pid &lt;PID&gt;</code> (or sometimes <code>--netns-path &lt;path&gt;</code>), <code>-- &lt;cmd&gt; [args\u2026]</code></li> <li>Open the namespace: <code>fd = open(\"/proc/PID/ns/net\", O_RDONLY|O_CLOEXEC)</code></li> <li>Join it: <code>setns(fd, CLONE_NEWNET)</code></li> <li>Harden &amp; de-priv:</li> <li>Clear dangerous env (PATH, IFS if you're paranoid), set <code>umask(077)</code></li> <li>Drop capabilities (details below)</li> <li>Optionally set <code>prctl(PR_SET_NO_NEW_PRIVS, 1)</code> after dropping caps      </li> <li>Hand off: <code>execvp(\"systemd-socket-proxyd\", argv)</code>     From here, proxyd runs inside that netns, inheriting only the minimal privileges you allow.</li> </ul>"},{"location":"bro-helper/#why-it-needs-some-capabilities","title":"Why it needs (some) capabilities","text":"<ul> <li>To call <code>setns(CLONE_NEWNET)</code> into a netns you don't \u201cown\u201d, the kernel requires <code>CAP_SYS_ADMIN</code> in the owning user namespace of that netns. We own it but are still required to have the permission.</li> <li>You don\u2019t need <code>CAP_NET_ADMIN</code> to enter the netns, but tools you run inside (like <code>ip addr</code>) may need it to mutate interfaces. <code>systemd-socket-proxyd</code> typically doesn't need <code>CAP_NET_ADMIN</code>; it just opens sockets.</li> <li>So: minimum to enter is <code>CAP_SYS_ADMIN</code>. If you also run diagnostics like <code>ip</code> inside the netns, give <code>CAP_NET_ADMIN</code> too (and then drop it before you <code>exec proxyd</code> if proxyd doesn\u2019t need it).</li> </ul>"},{"location":"bro-helper/#more-security-options-for-the-next-iteration","title":"More security options for the next iteration","text":"<ul> <li>Validate the target: ensure the PID exists and (optionally) matches the expected user/UID for your trust model.</li> <li>Avoid TOCTOU: once you open the netns fd, you're safe; don't re-resolve paths again.</li> <li>Logging: on failure, log <code>errno</code> and the exact syscall (helps when seccomp/LSM interferes).</li> <li>Interface: support <code>--netns-path</code> as an alternative to <code>--pid</code>, so you can bind a stable symlink like <code>/run/netns/nginx-proxyd</code>.</li> </ul> <p>This is why the approach is safe and composable: short-lived privilege, immediate drop, tight unit caps, and the actual worker (<code>systemd-socket-proxyd</code>) runs with regular user privileges in the desired netns.</p>"},{"location":"compose-conversion/","title":"Compose Conversion","text":"<p>WIP: Need to look at how we will deal with all aspects of docker-compose files. We need to look at how various entities in the compose file are best converted into Quadlet files (i.e. volumes).</p> <p><code>bro-compose</code> reads <code>docker-compose.yml</code> and synthesizes per-service calls to <code>bro-service</code> and potentially other new Brothaman commands:</p> <ul> <li>Each published port becomes a <code>.socket</code> + proxyd pair</li> <li>Containers bind to container interface ports accessible to the host and proxyd</li> <li>Dependencies prefer usage (client connects to server socket) rather than orchestration</li> <li>Optional service\u2192user mapping allows separation of privileges</li> </ul>"},{"location":"development/","title":"Development Guide","text":"<p>This guide covers development practices, conventions, and contribution guidelines for the brothaman project.</p>"},{"location":"development/#naming-conventions","title":"Naming Conventions","text":"<p>Brothaman follows a consistent dual naming convention across all components:</p>"},{"location":"development/#package-names","title":"Package Names","text":"<p>All Debian packages use the full prefix <code>brothaman-*</code>:</p> <ul> <li><code>brothaman-helper</code> - Network namespace helper utility</li> <li><code>brothaman-compose</code> - Container compose management</li> <li><code>brothaman-service</code> - Service lifecycle management  </li> <li><code>brothaman-install-deps</code> - Dependency installation utilities</li> <li><code>brothaman-zfs</code> - ZFS management tools</li> </ul> <p>Rationale: Full package names provide clear identification in APT repositories and package listings.</p>"},{"location":"development/#command-names","title":"Command Names","text":"<p>All executable commands use the short prefix <code>bro-*</code>:</p> <ul> <li><code>bro-helper</code> - Network namespace helper</li> <li><code>bro-compose</code> - Container compose management</li> <li><code>bro-service</code> - Service lifecycle management</li> <li><code>bro-install-deps</code> - Install system dependencies</li> <li><code>bro-install-zfs</code> - Install ZFS utilities</li> </ul> <p>Rationale: Short command names are convenient for daily terminal usage and tab completion.</p>"},{"location":"development/#examples","title":"Examples","text":"<pre><code># Install the package\nsudo apt install brothaman-helper\n\n# Use the command\nbro-helper --netns /run/user/1000/netns/podman -- curl ifconfig.me\n\n# Search for all brothaman packages\napt search brothaman-*\n\n# Tab complete all bro commands  \nbro-&lt;TAB&gt;&lt;TAB&gt;\n</code></pre>"},{"location":"development/#package-structure","title":"Package Structure","text":"<p>Each brothaman component follows this standard Debian package structure:</p> <pre><code>pkgs/brothaman-&lt;name&gt;/\n\u251c\u2500\u2500 DEBIAN/\n\u2502   \u251c\u2500\u2500 control          # Package metadata\n\u2502   \u251c\u2500\u2500 postinst         # Post-installation script\n\u2502   \u2514\u2500\u2500 prerm            # Pre-removal script (if needed)\n\u251c\u2500\u2500 usr/local/bin/       # Executable binaries\n\u2502   \u2514\u2500\u2500 bro-&lt;name&gt;       # Main executable\n\u251c\u2500\u2500 src/                 # Source code (for compiled tools)\n\u2502   \u251c\u2500\u2500 Makefile         # Build configuration\n\u2502   \u2514\u2500\u2500 *.c/*.go/etc     # Source files\n\u2514\u2500\u2500 build.sh             # Build script (optional)\n</code></pre>"},{"location":"development/#build-process","title":"Build Process","text":"<p>Packages are built using the containerized build system:</p> <pre><code># Build all packages\n./scripts/build.sh\n\n# Build specific components\n./scripts/mkdebs.sh\n\n# Generate documentation\n./scripts/mkdocs.sh\n\n# Create repository\n./scripts/mkrepo.sh\n</code></pre>"},{"location":"development/#development-workflow","title":"Development Workflow","text":"<ol> <li>Create Package Structure: Add new package to <code>pkgs/brothaman-&lt;name&gt;/</code></li> <li>Add DEBIAN Control Files: Define package metadata and dependencies  </li> <li>Implement Functionality: Add source code and build configuration</li> <li>Update Documentation: Add or update relevant markdown files</li> <li>Test Build: Run <code>./scripts/build.sh</code> to verify package creation</li> <li>Update Navigation: Add new docs to <code>mkdocs.yml</code> navigation</li> </ol>"},{"location":"development/#code-standards","title":"Code Standards","text":"<ul> <li>Shell Scripts: Follow bash best practices with <code>set -euo pipefail</code></li> <li>C Code: Use standard GNU C with appropriate compiler warnings</li> <li>Go Code: Follow standard Go formatting and conventions</li> <li>Documentation: Use clear, concise markdown with examples</li> </ul>"},{"location":"development/#security-considerations","title":"Security Considerations","text":"<ul> <li>All network namespace operations validate ownership</li> <li>Capabilities are set via postinst scripts, not SUID binaries</li> <li>Commands use allowlists for permitted operations</li> <li>ZFS operations respect user permissions and quotas</li> </ul>"},{"location":"development/#testing","title":"Testing","text":"<ul> <li>Unit tests in <code>tests/</code> directory</li> <li>Integration tests via Vagrant environments  </li> <li>Package installation testing in clean containers</li> <li>Documentation validation via MkDocs build</li> </ul>"},{"location":"development/#contributing","title":"Contributing","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch: <code>git checkout -b feature/new-tool</code></li> <li>Follow naming conventions for packages and commands</li> <li>Add tests and documentation</li> <li>Submit a pull request</li> </ol>"},{"location":"development/#release-process","title":"Release Process","text":"<ol> <li>Update version numbers in package control files</li> <li>Run full build: <code>./scripts/build.sh</code> </li> <li>Sign repository: <code>./scripts/signrepo.sh</code></li> <li>Publish changes: <code>./scripts/publish.sh</code></li> <li>Create release: <code>./scripts/release.sh</code></li> </ol>"},{"location":"dictionary/","title":"Dictionary","text":"<ul> <li>Bimodal script: A wrapper that runs an Ansible play by default and a shell implementation with <code>--script</code>.</li> <li>Lingering: Systemd capability allowing user units to run without an interactive login.</li> <li>Quadlet: Systemd generator that converts <code>.container</code> / <code>.volume</code> files into native units for Podman.</li> <li>Socket activation: Starting services upon first connection to a <code>.socket</code> listener.</li> <li>systemd-socket-proxyd: Small proxy service that forwards accepted sockets to a target <code>IP:PORT</code>.</li> <li>Graphroot: Podman\u2019s content store path; per-user when running rootless.</li> <li>User Scope: Refers to systemd services running the same system account: these services have user scope and can be ordered with respect to one another using dependency directives</li> <li>Unprivileged account</li> <li>Unprivileged user</li> <li>Unprivileged podman</li> <li>Non-root account</li> <li>OS account</li> </ul>"},{"location":"directions/","title":"Directions","text":"<ol> <li> <p>Create a <code>${major}.${minor}.${micro}</code> component based version comparison function in the common library to compare version numbers. It takes two version arguments. If the first is higher and more recent it returns 1. If the second is higher and more recent it returns -1. If both are the same it returns 0. WARNING: make sure comparisons are not lexical but numeric value based on all components. This will be used to compare installed versions against what is available using the ALVISTACK_VERSION.</p> </li> <li> <p>Always favor of the <code>alvistack</code> to install from OpenSuse instead of the distributions packaged podman. The <code>bro-install</code> installer script (which also uninstalls) first checks if the <code>alvistack</code> package repository is setup with its keys. If not, any existing podman is removed and the repository is installed. If the repository is already setup, the installer script checks if podman is installed, and if so, the installer script checks the installed version to see if it is above or equal to ALVISTACK_VERSION. If the installed version is lower, it is removed and podman is reinstalled from the <code>alvistack</code> repository.</p> </li> <li> <p>The use of CoW filesystems for backing stores will be optional and an independent axis of operation. The script uses whatever it finds (ZFS, BTRFS, or none). Right now in the last <code>rootless-podman</code> script, if ZFS is not present, no backing store is used for user homes under the USER_HOME_BASE which is a good fallback behavior. Without CoW backing stores, users miss out on leveraging snapshotting features for various operations, but things still work.</p> </li> </ol> <p>NOTE: it turns out, the quadlet generated by chatgpt for the postgres container is faulty and not the podman installation. I tried a minimal quadlet and it worked. Let us incorporate that into the training and progression towards the final brothaman configuration.</p>"},{"location":"project-plan/","title":"Project Plan","text":""},{"location":"project-plan/#goals","title":"Goals","text":"<ul> <li>Modularize rootless Podman + ZFS workflow into small, idempotent tools.</li> <li>Prefer systemd primitives (socket activation + proxyd) over container port publishing.</li> <li>Ensure each script maps 1:1 to an Ansible role (default mode), with a fallback shell mode via <code>--script</code>.</li> </ul>"},{"location":"project-plan/#work-breakdown-structure","title":"Work Breakdown Structure","text":"<ol> <li>ZFS Installation \u2014 <code>bro-install-zfs</code></li> <li>Test Zpool \u2014 <code>bro-test-zpool</code></li> <li>Dependencies \u2014 <code>bro-install-deps</code></li> <li>User Management \u2014 <code>bro-user</code></li> <li>Service Generator \u2014 <code>bro-service</code></li> <li>Compose Converter \u2014 <code>bro-compose</code></li> <li>Doctor \u2014 <code>bro-doctor</code></li> <li>Documentation &amp; Diagrams \u2014 mkdocs site, mermaid diagrams</li> </ol>"},{"location":"project-plan/#deliverables","title":"Deliverables","text":"<ul> <li>Executable <code>bro-*</code> wrappers (bimodal: Ansible default, <code>--script</code> for shell fallback).</li> <li>Ansible collection skeleton: <code>akarasulu.brothaman</code> with roles matching each script.</li> <li>MkDocs site: this plan, architecture, how-tos, and a project dictionary.</li> <li>Example Vagrant environment for Debian 12 smoke testing.</li> </ul>"},{"location":"project-plan/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li>All scripts are idempotent and safe to re-run.</li> <li>Socket-activated services start on demand via <code>systemd-socket-proxyd</code>.</li> <li><code>bro-compose</code> generates deterministic plans and compatible service units.</li> <li>Documentation is sufficient to onboard a new operator using only this site.</li> </ul>"},{"location":"project-plan/#future-extensions","title":"Future Extensions","text":"<ul> <li>Cross-host activation cookbook.</li> <li>Secrets and TLS patterns.</li> <li>CI smoke tests for Vagrant-based runs.</li> </ul>"},{"location":"quadlet-patterns/","title":"Quadlet Patterns","text":"<p>Common fields we will generate: - <code>Image=</code> to select the container image - <code>EnvironmentFile=</code> and <code>Volume=</code> entries - <code>ContainerName=</code> for stable names - <code>Exec=</code> or <code>Args=</code> for explicit command overrides - <code>After=</code>/<code>Requires=</code> for optional intra-user ordering</p>"},{"location":"socket-activation/","title":"Socket Activation","text":"<ul> <li><code>NAME.socket</code>: Listens on <code>IP:PORT</code> in the user systemd scope.</li> <li><code>NAME-proxy.service</code>: Runs <code>systemd-socket-proxyd</code>, forwarding accepted fds to <code>127.0.0.1:INTERNAL_PORT</code>.</li> <li><code>NAME.container</code>: Quadlet container that binds only to loopback on the internal port.</li> </ul> <p>Flow: Client \u2192 <code>NAME.socket</code> \u2192 <code>NAME-proxy.service</code> \u2192 container <code>127.0.0.1:INTERNAL_PORT</code>.</p>"},{"location":"unprivileged-podman/","title":"Unprivileged Podman","text":"<p>Unprivileged Podman refers to the use of Podman by restricted non-root OS accounts rather than the privileged root user. Podman commands and containers run as non-root system account processes while using bounded resources with quota restricted limits specifically assigned to the unprivileged account; i.e. cpu, memory and disk quotas.</p> <p>Brothaman creates Podman containers and composable container applications on isolated unprivileged OS accounts.</p>"},{"location":"unprivileged-podman/#security-pattern","title":"Security Pattern","text":"<p>Running applications and services pulled from repositories on the Internet in restricted jails is a sensible security pattern right? You don't want to run other people's shit on your shit as root. Each container adds to the overall attack surface in their own [un]predictable ways.</p> <p>Running containers (or applications composed of containers) in restricted environments minimizes the blast radius on compromise. Even without compromise, noisy neighbors or renegade containers MUST BE limited and throttled to functionally protect system resources. With compromise, the account, its processes, and resources need to be frozen and quarantined. Perhaps snapshotted and archived before being immediately destroyed for post-mortem forensic analysis.</p> <p>See zfs</p>"},{"location":"unprivileged-podman/#storage-concerns","title":"Storage Concerns","text":"<p>Unprivileged means much more than just jailed service and application processes with their CPU and memory quotas. It includes all resources with storage being the most critical of them all. After all, the most constraining resources on most systems is almost always storage. Even older CPU's with slower clocks and memory buses can almost always saturate storage I/O even with storage technologies 10-years into the future.</p> <p>Storage is the key resource needing the most protection, yet we often protect it least of all. Brothaman forces the use of ZFS and cgroup v2 blkio limits to cover your back. ZFS as a copy-on-write (CoW) filesystem with capacity quotas makes snapshotting, capacity limiting, and quarantining of containerized services and applications a cake walk.</p>"},{"location":"user-scope/","title":"User Scope","text":""},{"location":"users-and-lingering/","title":"Users &amp; Lingering","text":"<p>Rootless services run in user systemd scope. To run without an active login: - Enable lingering: <code>loginctl enable-linger USER</code></p> <p>Per-user containers config (<code>~/.config/containers/storage.conf</code>):</p> <pre><code>[storage]\ndriver = \"overlay\"\nrunroot = \"/run/user/UID\"\ngraphroot = \"/var/lib/containers/unprivileged/USER\"\n\n[storage.options.overlay]\nmount_program = \"/usr/bin/fuse-overlayfs\"\n</code></pre> <p>Subuid/subgid mappings must be present for rootless Podman.</p>"},{"location":"zfs/","title":"ZFS","text":"<p>Brothaman uses ZFS to implement storage I/O throttling of unprivileged container services. As the premier copy-on-write (CoW) file system with rapid low cost snapshotting and rollback ZFS is also ideal for snapshotting, quarantining and archiving compromised containers for forensic analysis.</p>"},{"location":"zfs/#unprivileged-with-zfs","title":"Unprivileged with ZFS","text":"<p>Podman's ZFS driver uses ZFS snapshots and mountpoints for container layering, but it is not stable and only supports privileged Podman. ZFS delegation just does not provide unprivileged users with the key mounting privileges needed. Once it does Brothaman can reconsider using it as the main driver. Don't hold your breath on it though; Linux might never let ZFS do this as long as it keeps one global mount space.</p>"},{"location":"zfs/#fuse-overlayfs-on-zfs","title":"FUSE Overlayfs on ZFS","text":"<p>Brothaman DOES NOT use ZFS for image layer management but it does use it as the underlying backing store for unprivileged users. Brothaman uses the Overlayfs driver instead of the default vfs driver to greatly improve performance and reduce the overhead of layer storage. To do this in unprivileged environments requires the use of the <code>fuse-overlayfs</code> program as the <code>mount_program</code> (and <code>mountopt = 'nodev'</code>) within the unprivileged account's storage configuration file at <code>${USER_HOME}/.config/containers/storage.conf</code>.</p>"},{"location":"zfs/#zfs-dataset-settings","title":"ZFS Dataset Settings","text":"<p>Each unprivileged user's home directory is mounted using a new dedicated ZFS data set. That dataset is configured with the following attributed values:</p> <ul> <li><code>xattr=sa</code></li> <li><code>acltype=posixacl</code></li> <li><code>aclinherit=passthrough</code></li> <li><code>aclmode=passthrough</code></li> <li><code>mountpoint=\"${USER_HOME}\"</code></li> <li><code>compression=zstd</code></li> <li><code>atime=off</code></li> <li><code>recordsize=128</code></li> </ul>"},{"location":"zfs/#brothaman-tools","title":"Brothaman Tools","text":"<ul> <li><code>bro-install-zfs</code> installs ZFS on Debian 12 (no pools).</li> <li><code>bro-test-zpool</code> creates a file-backed pool for development (default name <code>brotest</code>).</li> </ul> <p>Recommended dataset for per-user graphroots: * <code>POOL/containers/USER</code> mounted at <code>/var/lib/containers/unprivileged/USER</code> * Suggested props: <code>compression=zstd</code>, <code>atime=off</code></p>"},{"location":"labs/","title":"Brothaman Labs","text":"<p>Welcome to the Brothaman hands-on laboratory series! These labs provide step-by-step tutorials for learning rootless container management with ZFS, systemd, and Podman.</p>"},{"location":"labs/#lab-structure","title":"Lab Structure","text":"<p>The labs are organized into three main areas:</p>"},{"location":"labs/#foundation-labs","title":"Foundation Labs","text":"<p>Essential concepts and setup procedures: - Setup Environment - Configure your test environment with Vagrant - User Lingering - Understand systemd user services and lingering - Quadlet Basics - Learn container service descriptors</p>"},{"location":"labs/#storage-data-labs","title":"Storage &amp; Data Labs","text":"<p>Storage management and persistence: - Volumes &amp; ZFS - Bind mounts, persistent volumes, and ZFS snapshots - Debugging - Container troubleshooting and log analysis</p>"},{"location":"labs/#networking-services-labs","title":"Networking &amp; Services Labs","text":"<p>Advanced networking and service composition: - Network Configuration - Container networking and connectivity - Database Admin - PostgreSQL with pgAdmin setup - Pod Management - Multi-container pod orchestration - Proxy Testing - Load balancing and proxy configuration</p>"},{"location":"labs/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic familiarity with Linux command line</li> <li>Understanding of containers (Docker/Podman)</li> <li>Access to a Linux system with systemd</li> <li>Vagrant installed (for VM-based labs)</li> </ul>"},{"location":"labs/#getting-started","title":"Getting Started","text":"<p>Begin with the Setup Environment lab to configure your testing environment, then proceed through the Foundation labs before moving to more advanced topics.</p> <p>Each lab includes: - Clear learning objectives - Step-by-step instructions - Troubleshooting guidance - Cleanup procedures - VM snapshot management</p> <p>Ready to begin? Start with Setup Environment!</p>"},{"location":"labs/0.%20setup/","title":"Lab Setup","text":""},{"location":"labs/0.%20setup/#install-kvm-hypervisor","title":"Install KVM Hypervisor","text":"<p>You want to install vagrant and the libvirt provider on your Debian/Manjaro Linux then install the brothaman VM:</p> <pre><code>sudo apt-get install -y git vagrant vagrant-libvirt \\\n  libvirt-daemon-system libvirt-clients virtinst virt-manager \\\n  qemu-kvm qemu-utils\n</code></pre>"},{"location":"labs/0.%20setup/#clone-and-build-the-brothaman-z12-box","title":"Clone and build the brothaman-z12-box","text":"<p>A one time build of the brothaman/z12 vagrant box is required before first use:</p> <pre><code>git clone https://github.com/brothaman-org/brothaman-z12-box\ncd brothaman-z12-box\n./build.sh\n</code></pre>"},{"location":"labs/0.%20setup/#clone-and-fire-up-the-brothaman-lab-vm","title":"Clone and fire up the brothaman lab VM","text":"<p>Now we fire up the brothaman lab VM:</p> <pre><code>git clone https://github.com/brothaman-org/brothaman\ncd brothaman\nvagrant up &amp;&amp; vagrant snapshot save before-starting\n</code></pre>"},{"location":"labs/1.%20lingering/","title":"Lingering","text":"<p>In this lab we will prove that systemd lingering works when enabled for an unprivileged user and demonstrate its impact on user-scoped services that persist across reboots. You will:</p> <ol> <li>Create an unprivileged lingering user and enable systemd commands</li> <li>Create, install, and start test systemd user-scoped service</li> <li>Reboot with lingering</li> <li>Check if the test service runs after reboot (should be running)</li> <li>Disable lingering and reboot</li> <li>Check if the test service runs after reboot (should NOT be running)</li> <li>Re-enable lingering again and see what happens</li> <li>Lessons learned</li> </ol>"},{"location":"labs/1.%20lingering/#0-snapshot","title":"0. Snapshot","text":"<p>Since this test runs inside a Vagrant VM, capture a baseline snapshot before making any changes. From your host machine (in the same directory as your Vagrantfile):</p> <pre><code>if ! vagrant snapshot list 2&gt;&amp;1 | grep before-starting &gt;/dev/null; then\n    vagrant snapshot save before-starting;\nelse\n    echo before-starting snapshot already exists;\nfi\n</code></pre> <p>This allows you to roll the VM back to its exact pre-test state later.</p>"},{"location":"labs/1.%20lingering/#1-create-an-unprivileged-lingering-user-and-enable-systemd-commands","title":"1. Create an unprivileged lingering user and enable systemd commands","text":"<p>ATTENTION: Make sure to SSH into the vagrant vm with <code>vagrant ssh</code></p> <p>Create a dedicated unprivileged test user:</p> <pre><code># create and check user\nsudo useradd -m -s /bin/bash lingeruser\nid lingeruser\ngetent passwd lingeruser\nsudo -iu lingeruser ls -ld .\n</code></pre> <p>This ensures the user got created with home directory (<code>/home/lingeruser</code>) and we can execute commands as the user. Now let's make the user linger but notice the checks before and after for an instance of systemd running as the lingeruser:</p> <pre><code>echo \"checking if new user's systemd instance is running\"\nsudo ps -ux -U lingeruser | grep 'systemd --user' | grep -v grep\necho no systemd from the ps command \u261d\ufe0f\necho\n\n# make the user linger\nsudo loginctl enable-linger lingeruser\necho \"checking if new user's systemd instance is running\"\nsleep 3\nsudo ps -ux -U lingeruser | grep 'systemd --user' | grep -v grep\necho see systemd now from the ps command \u261d\ufe0f\necho\n\n# check if we can use systemctl\nsudo -iu lingeruser systemctl --user list-units\necho \"NOTICE: using systemctl \u261d\ufe0f fails \ud83d\ude1e\"\n\n</code></pre> <p>Lingering is enabled using the loginctl command. Unfortunately, this user cannot use systemd commands until some basic settings are set.</p> <p>For user-mode systemd commands to work, the <code>XDG_RUNTIME_DIR</code> and <code>DBUS_SESSION_BUS_ADDRESS</code> shell variables MUST BE correctly set. We can do that system wide for all unprivileged users by adding the following script in <code>/etc/profile.d/xdg_runtime.sh</code>:</p> <pre><code>cat | sudo tee /etc/profile.d/xdg_runtime.sh &lt;&lt;'EOF'\nexport XDG_RUNTIME_DIR=/run/user/$(id -u)\nexport DBUS_SESSION_BUS_ADDRESS=unix:path=${XDG_RUNTIME_DIR}/bus\nEOF\n\nsudo chmod +x /etc/profile.d/xdg_runtime.sh\n\nsudo -iu lingeruser systemctl --user list-units | tee /dev/null\necho systemctl \u261d\ufe0f now succeeds \ud83d\udc4f\n\n</code></pre>"},{"location":"labs/1.%20lingering/#whats-going-on-here","title":"What's going on here?","text":"<p>Systemd runs in system mode as root. When \"enabled to\", systemd can also run in user mode with the <code>--user</code> switch. The systemd process actually runs as a specific user. Here in this lab it runs as the lingeruser.</p> <p>If you noticed the lingeruser's systemd process was not present before the <code>loginctl enable-linger lingeruser</code> command was run. Running the command fired up the systemd infrastructure for the lingeruser and the ps command after it showed the user's systemd instance now running. Furthermore, the enable-linger command makes sure, the user's systemd instance is always running, even when the user is not logged in.</p> <p>Other things are also started. Namely a dbus broker for the user's system components bus. System components, including systemd, talk to one another using this messaging system. In fact, systemctl sends commands to the systemd user instance using this bus. However, to attach to the broker and send commands to the systemd user instance it needs to connect to the bus. These environment variables tell it where to find the bus.</p>"},{"location":"labs/1.%20lingering/#user-manager","title":"User Manager","text":"<p>The user manager is the per-user systemd instance that manages all user-scope units. On a system with systemd, there are two main layers of system management:</p> Layer Runs as Controls System manager <code>PID 1</code> (<code>/lib/systemd/systemd</code>) system-wide units (<code>/etc/systemd/system/*.service</code>) User manager <code>/lib/systemd/systemd --user</code> (one per logged-in or lingering user) per-user units (<code>~/.config/systemd/user/*.service</code>) <p>So each logged-in user (or each \u201clingering\u201d user) gets their own lightweight systemd instance that controls user-scoped services, sockets, timers, and targets. You can see it with <code>ps -u lingeruser | grep systemd</code>.</p>"},{"location":"labs/1.%20lingering/#two-ways-the-user-manager-starts","title":"Two ways the user manager starts","text":"<ol> <li> <p>Interactive login (default behavior)     When a user logs in (via <code>ssh</code>, TTY, desktop, etc.), PAM runs <code>systemd --user</code>, creating <code>/run/user/&lt;uid&gt;/</code> and starting user services like <code>dbus.socket</code>, <code>xdg-desktop-portal.service</code>, etc.</p> </li> <li> <p>Lingering (non-interactive)     When you enable lingering via <code>loginctl enable-linger &lt;user&gt;</code>. Systemd ensures that the per-user manager (<code>user@&lt;uid&gt;.service</code>) starts at boot, even if the user never logs in. That service runs as root's child process under the system manager and spawns <code>/lib/systemd/systemd --user</code> for that UID.</p> </li> </ol>"},{"location":"labs/1.%20lingering/#relationship-to-runuserbus","title":"Relationship to /run/user/\\/bus <p>The D-Bus session bus socket at <code>/run/user/&lt;uid&gt;/bus</code> is created and managed by the user manager.</p> <ul> <li>The user manager starts <code>dbus.socket</code> and <code>dbus.service</code> (or <code>dbus-broker.service</code>). </li> <li><code>dbus.socket</code> creates the <code>bus</code> socket.</li> <li>Any client that connects to <code>/run/user/&lt;uid&gt;/bus</code> triggers D-Bus activation of the broker/service.</li> </ul> <p>If the user manager isn't running, there's nothing to start <code>dbus.socket</code>, so <code>/run/user/&lt;uid&gt;/bus</code> doesn't exist \u2014 even though <code>/run/user/&lt;uid&gt;/</code> might. See it all running:</p> <pre><code>vagrant@debian12:~$ sudo loginctl user-status lingeruser \nlingeruser (1001)\n           Since: Fri 2025-10-17 19:29:45 UTC; 24min ago\n           State: lingering\n          Linger: yes\n            Unit: user-1001.slice\n                  \u2514\u2500user@1001.service\n                    \u2514\u2500init.scope\n                      \u251c\u25001801 /lib/systemd/systemd --user\n                      \u2514\u25001803 \"(sd-pam)\"\n\n...\nOct 17 ... systemd[1801]: Starting dbus.socket - D-Bus User Message Bus Socket...\nOct 17 ... systemd[1801]: Listening on dbus.socket - D-Bus User Message Bus Socket.\n...\nOct 17 19:29:46 debian12 systemd[1801]: Startup finished in 147ms.\n</code></pre>","text":""},{"location":"labs/1.%20lingering/#2-create-install-and-start-test-systemd-user-service","title":"2. Create, install, and start test systemd user service","text":"<p>NOTE: Sudo into the lingeruser with <code>sudo su - lingeruser</code></p> <p>Let's create the service directory and add lingeruser's new user-scoped systemd service. The service loops forever printing to a log file in /tmp then sleeping for 10 seconds. </p> <pre><code>mkdir -p ~/.config/systemd/user\n\ncat &gt; ~/.config/systemd/user/test-linger.service&lt;&lt;'EOF'\n[Unit]\nDescription=Lingering Proof Test Service\n\n[Service]\nExecStart=/bin/bash -c 'while true; do echo \"$(date -Is) test-linger running\" &gt;&gt; /tmp/test-linger.log; sleep 10; done'\nRestart=always\nRestartSec=5\n\n[Install]\nWantedBy=default.target\n\nEOF\n\n</code></pre> <p>Enable and start the service like any other service, but for user scope we use the <code>--user</code> switch. We also confirm the service has started and is logging to the file:</p> <pre><code>systemctl --user daemon-reload\nsystemctl --user enable --now test-linger.service\nsystemctl --user status test-linger.service\nsystemctl --user list-units --type=service | grep \"test-linger\"\ntail -f /tmp/test-linger.log\n\n</code></pre> <p>NOTE: you might need to type q to quit the systemd unit listing command</p> <p>Ctrl-C to exit the tail output.</p>"},{"location":"labs/1.%20lingering/#3-reboot-with-lingering","title":"3. Reboot with lingering","text":"<p>Back as vagrant user:</p> <p><code>sudo reboot</code></p>"},{"location":"labs/1.%20lingering/#4-check-if-the-test-service-runs-after-reboot","title":"4. Check if the test service runs after reboot","text":"<p>SSH back into the vagrant machine and run this as the vagrant user:</p> <p><code>tail -f /tmp/test-linger.log</code></p> <p>You might press return then wait for another line to be printed in 10s. Press Ctrl-C to exit the tailed output.</p>"},{"location":"labs/1.%20lingering/#5-disable-lingering-and-reboot","title":"5. Disable lingering and reboot","text":"<pre><code>sudo loginctl disable-linger lingeruser\nsudo reboot\n</code></pre>"},{"location":"labs/1.%20lingering/#6-check-if-the-test-service-runs-after-reboot-should-not-be-running","title":"6. Check if the test service runs after reboot (should NOT be running)","text":"<p>SSH back into the vagrant machine and run this as the vagrant user:</p> <pre><code>~/Local/brothaman-labs vagrant ssh                                              Last login: Fri Oct 17 20:11:38 2025 from 192.168.122.1\nvagrant@debian12:~$ tail -f /tmp/test-linger.log\ntail: cannot open '/tmp/test-linger.log' for reading: No such file or directory\ntail: no files remaining\nvagrant@debian12:~$ sleep 10; tail -f /tmp/test-linger.log\ntail: cannot open '/tmp/test-linger.log' for reading: No such file or directory\ntail: no files remaining\nvagrant@debian12:~$ sudo ps -u lingeruser\n    PID TTY          TIME CMD\n</code></pre> <p>It is clear nothing is running any longer.</p>"},{"location":"labs/1.%20lingering/#7-re-enable-lingering-and-see-what-happens","title":"7. Re-enable lingering and see what happens","text":"<pre><code>echo \"checking if new user's systemd instance is running\"\nsudo ps -ux -U lingeruser | grep 'systemd --user' | grep -v grep\necho no systemd from the ps command \u261d\ufe0f\necho\n\n# make the user linger\nsudo loginctl enable-linger lingeruser\necho \"checking if new user's systemd instance is running\"\nsleep 3\nsudo ps -ux -U lingeruser | grep 'systemd --user' | grep -v grep\necho see systemd now from the ps command \u261d\ufe0f\nsudo ps -ux -U lingeruser | grep 'test-linger' | grep -v grep\necho see test-linger shell from the ps command \u261d\ufe0f\necho\n\n</code></pre> <p>Reboot and check again, the service should still be up so long as we do not turn off user lingering.</p>"},{"location":"labs/1.%20lingering/#8-lessons-learned","title":"8. Lessons learned","text":"<p>Through this lab we verified how systemd's user-scoped manager behaves under different conditions and why lingering is such an important feature for background or headless service users.</p>"},{"location":"labs/1.%20lingering/#key-takeaways","title":"Key takeaways","text":"<ol> <li> <p>User-mode systemd is separate from system-mode.     Every user runs their own <code>systemd --user</code> instance, called the user manager.     This instance controls user-scoped units located under <code>~/.config/systemd/user/</code>.</p> </li> <li> <p>Lingering keeps the user manager alive across reboots and logouts.     Without lingering, <code>systemd --user</code> only starts when the user logs in through PAM.     With lingering enabled (<code>loginctl enable-linger &lt;user&gt;</code>), the user\u2019s <code>user@UID.service</code> starts at boot, even if they never log in.</p> </li> <li> <p>Environment variables are critical. <code>XDG_RUNTIME_DIR</code> and <code>DBUS_SESSION_BUS_ADDRESS</code> must point to <code>/run/user/&lt;uid&gt;</code> and its bus socket so that commands like <code>systemctl --user</code> can talk to the user manager.     Adding <code>/etc/profile.d/xdg_runtime.sh</code> ensures these variables exist for all unprivileged users.</p> </li> <li> <p>The D-Bus socket (<code>/run/user/&lt;uid&gt;/bus</code>) belongs to the user manager.     It only exists while that per-user <code>systemd --user</code> instance is running.     When lingering is disabled or the manager stops, the socket disappears and user-scoped commands fail.</p> </li> <li> <p>Services enabled under <code>default.target</code> survive reboots.     Our test service <code>test-linger.service</code> continued running after every reboot while lingering was enabled, and stopped existing once lingering was disabled.</p> </li> <li> <p>Lingering makes user accounts behave like lightweight system services.     This is ideal for rootless containers, background daemons (e.g., PostgreSQL, Gitea), or personal automation that must persist without an active session.</p> </li> <li> <p>Each lingered user consumes persistent resources.     Because their <code>user@UID.service</code> remains active, each lingering user adds a small amount of memory and process overhead\u2014acceptable for a few users, but not something to enable for hundreds indiscriminately.</p> </li> </ol>"},{"location":"labs/1.%20lingering/#practical-implications","title":"Practical implications","text":"<ul> <li>Use lingering for any headless service accounts that must auto-start at boot and not depend on an interactive login.</li> <li>Always ensure that <code>dbus-user-session</code> or <code>dbus-broker</code> is installed so the per-user D-Bus bus can start.</li> <li>Confirm lingering state with <code>loginctl show-user &lt;user&gt; | grep Linger</code></li> <li>Verify the user manager is alive with<code>systemctl status user@$(id -u &lt;user&gt;).service</code></li> </ul>"},{"location":"labs/1.%20lingering/#final-observation","title":"Final observation","text":"<p>Once lingering is enabled, a normal unprivileged account becomes a first-class citizen in systemd's dependency graph \u2014 its services are started, stopped, and monitored just like system-level units.</p> <p>Disabling lingering cleanly removes that persistence, returning the user to an ephemeral, login-only environment.</p>"},{"location":"labs/1.%20lingering/#9-rollback","title":"9. Rollback","text":"<p>When finished, restore the VM to the baseline snapshot:</p> <pre><code>vagrant snapshot restore before-starting --no-provision\n</code></pre>"},{"location":"labs/2.%20quadlet/","title":"Quadlet","text":"<p>In this lab we install the latest Podman with Quadlet support and use it to create, install, and test a simple container service under an unprivileged lingering user.</p> <p>This builds on 1. lingering \u2014 make sure you have completed that experiment and verified that lingering is enabled for your test user. Here's a quick summary of steps:</p> <ol> <li>Uninstall system podman if present</li> <li>Install the latest podman from alvistack</li> <li>Drop in the test-quadlet.container</li> <li>Reboot test making sure it is up after reboot</li> </ol>"},{"location":"labs/2.%20quadlet/#0-snapshot","title":"0. Snapshot","text":"<p>Before making changes, capture a baseline snapshot of the VM so we can roll back cleanly if anything goes wrong:</p> <pre><code>if ! vagrant snapshot list 2&gt;&amp;1 | grep before-quadlet &gt;/dev/null; then\n  vagrant snapshot save before-quadlet;\nelse\n  echo before-quadlet snapshot already exists;\nfi\n</code></pre> <p>This ensures that you can easily revert to a clean pre-Quadlet state.</p>"},{"location":"labs/2.%20quadlet/#1-uninstall-system-podman-if-present","title":"1. Uninstall System Podman (if present)","text":"<p>First, remove any system-level Podman installation that might conflict with the user-space version we'll install next.</p> <pre><code>sudo apt remove -y podman podman-rootless podman-plugins containers-common\nsudo apt autoremove -y\n</code></pre> <p>After removal, confirm that no residual binaries or systemd services remain:</p> <pre><code>which podman || echo \"Podman removed\"\nsudo systemctl disable --now podman.service podman.socket 2&gt;/dev/null || true\n</code></pre> <p>You should see \u201cPodman removed\u201d printed and no active Podman system services.</p>"},{"location":"labs/2.%20quadlet/#2-install-the-latest-podman-from-alvistack","title":"2. Install the Latest Podman from Alvistack","text":"<p>Back to the vagrant user</p> <p>We'll now install the latest version of Podman from the Alvistack repository, which typically ships newer Podman releases with full Quadlet support.</p>"},{"location":"labs/2.%20quadlet/#21-add-the-repository","title":"2.1 Add the Repository","text":"<pre><code>sudo apt update\nsudo apt install -y curl gnupg2\ncurl -fsSL https://downloadcontent.opensuse.org/repositories/home:/alvistack/Debian_12/Release.key | \\\n  sudo gpg --dearmor -o /usr/share/keyrings/alvistack.gpg\n\necho \"deb [signed-by=/usr/share/keyrings/alvistack.gpg] http://downloadcontent.opensuse.org/repositories/home:/alvistack/Debian_12 ./\" | \\\n  sudo tee /etc/apt/sources.list.d/alvistack.list\n</code></pre>"},{"location":"labs/2.%20quadlet/#22-install-podman","title":"2.2 Install Podman","text":"<pre><code>sudo apt update\nsudo apt install -y \\\n  podman conmon crun catatonit \\\n  netavark aardvark-dns \\\n  slirp4netns uidmap \\\n  fuse-overlayfs dbus-user-session \\\n  iptables nftables \\\n  passt\n\n</code></pre>"},{"location":"labs/2.%20quadlet/#23-verify-quadlet-support","title":"2.3 Verify Quadlet Support","text":"<p>Back to the lingeruser: <code>sudo su - lingeruser</code></p> <pre><code>podman quadlet --help\n</code></pre> <p>You should see subcommands like <code>install</code>, <code>list</code>, <code>rm</code>, and <code>print</code>.</p> <p>Next, check podman info:</p> <pre><code>podman system info\n</code></pre>"},{"location":"labs/2.%20quadlet/#24-kick-the-tires","title":"2.4 Kick the Tires","text":"<p>Run a quick smoke test to ensure Podman works for both root and unprivileged users:</p> <pre><code>podman run --rm quay.io/podman/hello\n</code></pre> <p>And print the version for reference:</p> <pre><code>podman version\n</code></pre> <p>Note the Client, Server, and Quadlet versions for your documentation.</p>"},{"location":"labs/2.%20quadlet/#3-drop-in-the-test-quadlet","title":"3. Drop in the Test Quadlet","text":"<p>In this section we'll create a simple Quadlet file for an unprivileged lingering user. This container will simply run a small web server to verify Quadlet automation.</p>"},{"location":"labs/2.%20quadlet/#31-switch-to-the-user","title":"3.1 Switch to the User","text":"<p>Back to the lingeruser: <code>sudo su - lingeruser</code></p>"},{"location":"labs/2.%20quadlet/#32-create-the-quadlet-file","title":"3.2 Create the Quadlet File","text":"<p>Create the Quadlet definition in the user's configuration directory:</p> <pre><code>mkdir -p ~/.config/containers/systemd/\ncat &gt; ~/.config/containers/systemd/test-quadlet.container &lt;&lt;'EOF'\n[Unit]\nDescription=Quadlet Test Container\n\n[Container]\nImage=nginx:alpine\nExec=nginx -g 'daemon off;'\nContainerName=test-quadlet\nPublishPort=8080:80\n\n[Service]\nRestart=always\nTimeoutStartSec=120\n\n[Install]\nWantedBy=default.target\nEOF\n</code></pre>"},{"location":"labs/2.%20quadlet/#33-reload-and-inspect-quadlet","title":"3.3 Reload and Inspect Quadlet","text":"<p>Tell systemd to reload the user unit files:</p> <pre><code>systemctl --user daemon-reload\n</code></pre> <p>Now check what Quadlet sees:</p> <pre><code>podman quadlet list\n</code></pre> <p>Start it:</p> <pre><code>systemctl --user start test-quadlet.service\n</code></pre> <p>And confirm that the generated <code>.service</code> file exists under:</p> <pre><code>cat /run/user/$UID/systemd/generator/test-quadlet.service\n</code></pre> <p>You should see the generated service file contents. Notice there was no service enable operation right? Then check the status:</p> <pre><code>systemctl --user status test-quadlet.service\n</code></pre> <p>You should see the Nginx container running with <code>Active: active (running)</code> status. Verify from Podman\u2019s perspective:</p> <pre><code>podman ps\n</code></pre> <p>Finally, test the container by curling the local port:</p> <pre><code>curl http://localhost:8080\n</code></pre> <p>You should see the Nginx welcome page HTML.</p>"},{"location":"labs/2.%20quadlet/#4-reboot-and-persistence-test","title":"4. Reboot and Persistence Test","text":"<p>Reboot the VM to ensure the Quadlet container auto-starts thanks to lingering:</p> <pre><code>sudo reboot\n</code></pre> <p>After reboot, log back in as the same unprivileged user and check:</p> <pre><code>systemctl --user status test-quadlet.service\n</code></pre> <p>You should see it running automatically.</p> <p>Confirm from Podman:</p> <pre><code>podman ps\n</code></pre> <p>And once more verify with:</p> <pre><code>curl http://localhost:8080\n</code></pre> <p>If all looks good, Quadlet persistence is working correctly under the lingering user.</p>"},{"location":"labs/2.%20quadlet/#5-rollback-optional","title":"5. Rollback (Optional)","text":"<p>If you want to revert to the pre-Quadlet state, roll back using your saved snapshot:</p> <pre><code>vagrant snapshot restore before-quadlet\n</code></pre>"},{"location":"labs/2.%20quadlet/#lessons-learned","title":"Lessons Learned","text":"<ol> <li>Quadlet integrates tightly with systemd, turning container definitions into first-class services.</li> <li>Lingering users are essential for persistent unprivileged containers across reboots.</li> <li>Alvistack\u2019s Podman builds offer up-to-date features missing from Debian's stock packages.</li> <li>Generated <code>.service</code> units under <code>/run/user/&lt;uid&gt;/containers/systemd/</code> are ephemeral but reflect the active container states.</li> <li>Quadlet simplifies container lifecycle management \u2014 no manual <code>podman run</code> required once configured.</li> <li>When combined with lingering, Quadlet gives you reliable, self-starting, user-scoped container services that survive reboots without root privileges.</li> </ol>"},{"location":"labs/3.%20volumes/","title":"Volumes","text":"<p>In this lab we create bind mounts of existing directories in dot.container quadlet descriptors so they're available in the container. We also create persistent volumes and access them in dot.volume descriptors tying them into container descriptors. We also use ZFS and/or BTRFS to demonstrate how they can be used to archive / rollback changes.</p> <ol> <li>Serve existing mkdocs directory with an nginx container</li> <li>Install ZFS</li> <li>Create a test zpool</li> <li>Create a dot.volume on a ZFS mount for the gitea container</li> <li>Automate snapshots with restarts</li> <li>Create a volume snapshot policy manager service</li> </ol>"},{"location":"labs/3.%20volumes/#0-snapshot","title":"0. Snapshot","text":"<p>Since this test runs inside a Vagrant VM, capture a baseline snapshot before making any changes. From your host machine (in the same directory as your Vagrantfile):</p> <pre><code>if ! vagrant snapshot list 2&gt;&amp;1 | grep before-volumes &gt;/dev/null; then\n  vagrant snapshot save before-volumes;\nelse\n  echo before-volumes snapshot already exists;\nfi\n</code></pre> <p>This allows you to roll the VM back to its exact pre-test state later.</p>"},{"location":"labs/3.%20volumes/#1-serve-existing-mkdocs-directory-with-a-nginx-container","title":"1. Serve existing mkdocs directory with a nginx container","text":""},{"location":"labs/4.%20debugging/","title":"Debugging","text":"<p>Buggy quadlet for debugging issues.</p>"},{"location":"labs/proxyd-testing/","title":"proxyd testing","text":"<p>Awesome, everything is working now with the simple straight forward service.</p> <p>NOTE: Need to stop the service when container stops?</p> <pre><code>lingeruser@debian12:~$ cat .config/systemd/user/nginx8080.service \n[Unit]\nDescription=Proxy host:8080 -&gt; 127.0.0.1:80 inside nginx-proxyd netns\nAfter=default.target\nRequires=nginx8080.socket\n\n[Service]\nType=simple\nEnvironmentFile=-%t/nginx-proxyd.env\nExecStartPre=/usr/bin/env sh -c \"/usr/bin/podman inspect -f 'TARGET_PID={{.State.Pid}}' nginx-proxyd &gt; %t/nginx-proxyd.env\"\n\nExecStartPre=/usr/local/bin/fd-setns-exec --pid ${TARGET_PID} -- ip -o addr\nExecStartPre=/usr/local/bin/fd-setns-exec --pid ${TARGET_PID} -- netstat -tlpn \nExecStart=/usr/local/bin/fd-setns-exec --pid ${TARGET_PID} -- /lib/systemd/systemd-socket-proxyd 127.0.0.1:80\n\nNoNewPrivileges=no\nPrivateTmp=yes\nProtectHome=yes\nProtectSystem=full\n\nRestart=on-failure\nRestartSec=0.5s\n\n[Install]\nWantedBy=default.target\n\n</code></pre> <pre><code>lingeruser@debian12:~$ cat .config/systemd/user/nginx8080.socket \n[Unit]\nDescription=Socket for nginx in container netns (host:8080)\n\n[Socket]\n# Listen on all interfaces; adjust as needed (0.0.0.0 or 127.0.0.1)\nListenStream=0.0.0.0:8080\nNoDelay=true\nReusePort=true\nBacklog=128\n\n[Install]\nWantedBy=default.target\n</code></pre>"},{"location":"labs/proxyd-testing/#1-set-up-the-container","title":"1. Set up the container","text":"<pre><code>sudo su - lingeruser\n</code></pre> <p>Fire up nginx to listen on default port 80 inside the non-networked container on the localhost / loopback and confirm it is working.</p> <pre><code>podman rm -f nginx-proxyd 2&gt;/dev/null || true\npodman run -d --name nginx-proxyd --network=none --restart=no docker.io/library/nginx:alpine \npodman exec nginx-proxyd netstat -tlnp | grep \":80\"\npodman exec nginx-proxyd curl localhost\n</code></pre> <p>Capture the netns path for the container process</p> <pre><code>NETNS_PATH=\"$(podman inspect -f '{{ .NetworkSettings.SandboxKey }}' nginx-proxyd)\"\necho \"NETNS_PATH=${NETNS_PATH}\"\n# Example: \n#   for pasta /run/user/1001/netns/netns-0f518fa4-fc9a-b4e3-5f0c-cd7286004f80\n#   for others /proc/20173/ns/net\n\n# Confirm it\u2019s a file we (the same user) can open\nls -l \"${NETNS_PATH}\"\n</code></pre>"},{"location":"labs/proxyd-testing/#2-create-user-systemd-units-socket-service","title":"2 Create user systemd units (socket + service)","text":"<p>We'll bind host:8080 in a user socket unit (no root needed since &gt;1024), and on first connection systemd will spawn <code>systemd-socket-proxyd</code> inside the container\u2019s netns, dialing <code>127.0.0.1:80</code>.</p>"},{"location":"labs/proxyd-testing/#21-the-socket-unit","title":"2.1 The socket unit","text":"<pre><code>mkdir -p ~/.config/systemd/user\ncat &gt; ~/.config/systemd/user/nginx8080.socket &lt;&lt;'EOF'\n[Unit]\nDescription=Socket for nginx in container netns (host:8080)\n\n[Socket]\n# Listen on all interfaces; adjust as needed (0.0.0.0 or 127.0.0.1)\nListenStream=0.0.0.0:8080\nNoDelay=true\nReusePort=true\nBacklog=128\n\n[Install]\nWantedBy=default.target\nEOF\n</code></pre>"},{"location":"labs/proxyd-testing/#22-the-service-unit","title":"2.2 The service unit","text":"<p>Paste the exact netns path you captured into <code>NetworkNamespacePath=...</code> below.</p> <pre><code>cat &gt; ~/.config/systemd/user/nginx8080.service &lt;&lt;EOF\n[Unit]\nDescription=Proxy host:8080 -&gt; 127.0.0.1:80 inside nginx-proxyd netns\n\n# Make sure container exists before we try to connect (best-effort)\nAfter=default.target\n\n[Service]\n# CRITICAL: join the container's network namespace so 127.0.0.1 is *inside* the container\nNetworkNamespacePath=/run/user/1001/netns/netns-0f518fa4-fc9a-b4e3-5f0c-cd7286004f80\n\n# Socket-activated; systemd passes the accepted client socket(s) via LISTEN_FDS\nExecStart=/usr/lib/systemd/systemd-socket-proxyd 127.0.0.1:80\n\n# Reasonable hardening/defaults for a tiny proxy hop\nDynamicUser=no\nProtectSystem=strict\nProtectHome=yes\nPrivateTmp=yes\nNoNewPrivileges=true\nLockPersonality=true\nRestrictRealtime=true\nRestrictSUIDSGID=true\nCapabilityBoundingSet=\nSystemCallArchitectures=native\n\n# Restart on occasional failures\nRestart=on-failure\nRestartSec=0.5s\n# Inherit our user's env\nEnvironment=LANG=C.UTF-8\n\n[Install]\nWantedBy=default.target\nEOF\n</code></pre> <p><code>NetworkNamespacePath=</code> is the magic: it makes the proxyd\u2019s client side (the \u201cdial to 127.0.0.1:80\u201d) happen inside the container\u2019s netns.</p>"}]}